<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Antonio Caliò</title>
    <link>/</link>
      <atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    <description>Antonio Caliò</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Fri, 01 Jan 2021 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/images/icon_hu28aae1bbe67b568759c86bc74a5bbfe0_150966_512x512_fill_lanczos_center_2.png</url>
      <title>Antonio Caliò</title>
      <link>/</link>
    </image>
    
    <item>
      <title>Example Project</title>
      <link>/courses/bdanalytics/exampleproject/</link>
      <pubDate>Sun, 05 May 2019 00:00:00 +0100</pubDate>
      <guid>/courses/bdanalytics/exampleproject/</guid>
      <description>&lt;p&gt;The goal of this project is to build an interaction graph starting from the
comments associated to a collection of YouTube videos.&lt;/p&gt;
&lt;p&gt;The entire pipeline is organized as follows:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;A user provides a certain query &lt;em&gt;q&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;q&lt;/em&gt; is executed via the YouTube API.&lt;/li&gt;
&lt;li&gt;The query returns a collection of related videos (V).
&lt;ul&gt;
&lt;li&gt;For each video (v \in V), extract the collection of comment threads underlying (v)&lt;/li&gt;
&lt;li&gt;Each comment thread (t \in T) has a number of comments. Among them, there is the &lt;em&gt;top-level&lt;/em&gt;
comment, which represents the comment that started the discussion.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Create an edge from a user u to another user v, if u is the author of a &lt;em&gt;top-level&lt;/em&gt; comment of a thread where v has participated in.&lt;/li&gt;
&lt;li&gt;Associate a score to each edge that matches the sentiment score of the comment.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The following image shows the process behind the extraction of the interaction graph.








  











&lt;figure id=&#34;figure-interaction-graph&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;/media/img/ytExample.jpg&#34; data-caption=&#34;Interaction Graph&#34;&gt;


  &lt;img src=&#34;/media/img/ytExample.jpg&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Interaction Graph
  &lt;/figcaption&gt;


&lt;/figure&gt;
&lt;/p&gt;
&lt;h1 id=&#34;architecture&#34;&gt;Architecture&lt;/h1&gt;
&lt;p&gt;The project has the following components:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;DataAPI&lt;/em&gt;. It executes and manages the queries to the YouTube API&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Kafka-Producer&lt;/em&gt;. It is in charge of the data download and the data transmission to Kafka&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Spark-Consumer&lt;/em&gt;. It extracts the interaction graph directly from Kafka.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;dependencies&#34;&gt;Dependencies&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Apache Avro for data serialization&lt;/li&gt;
&lt;li&gt;CoreNLP for sentiment analysis - download the following &lt;a href=&#34;http://nlp.stanford.edu/software/stanford-corenlp-latest.zip&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;jar&lt;/a&gt; and put it in the lib of the Spark-Consumer&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;how-to-run-the-project&#34;&gt;How to run the project&lt;/h1&gt;
&lt;p&gt;You need a working docker distribution on your machine, with docker-compose.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Go to the Docker directory and run&lt;/p&gt;
&lt;p&gt;docker-compose up -d&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Then go to the &lt;em&gt;DataAPI&lt;/em&gt; compile your project by running &lt;code&gt;mvn package&lt;/code&gt;, then move the jar file into
the &lt;code&gt;lib&lt;/code&gt; inside the &lt;em&gt;Kafka-Producer&lt;/em&gt; root folder.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Run the producer with &lt;code&gt;sbt run&lt;/code&gt; providing all the necessary arguments&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Run the consumer with &lt;code&gt;sbt run&lt;/code&gt; providing all the necessary arguments&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Introduction to Apache Kafka</title>
      <link>/courses/bdanalytics/kafka/</link>
      <pubDate>Sun, 05 May 2019 00:00:00 +0100</pubDate>
      <guid>/courses/bdanalytics/kafka/</guid>
      <description>&lt;p&gt;Apache kafka is a distributed streaming platform.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;What is Event Streaming?&lt;/p&gt;
&lt;p&gt;Event streaming is the practice of capturing data in real-time from
event sources like databases, sensors or mobile devices.&lt;/p&gt;
&lt;p&gt;With event-streaming you need to design your application as an event-driven
application. This means that events your application has constantly to reach
to what is happening in its surrounding environment.&lt;/p&gt;
&lt;p&gt;Event streaming nsures a continuous flow and interpretation of data so that the right information is at the right
time.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;What can I use event streaming for?&lt;/p&gt;
&lt;p&gt;There is number of different contexts where real-time processing is a real thing.
For instance:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;To process payments and financial transactions&lt;/li&gt;
&lt;li&gt;To track and monitor cars, trucks, fleets and shipments&lt;/li&gt;
&lt;li&gt;To continuously capture and analyze sensor data coming from and IoT ecosystem&lt;/li&gt;
&lt;li&gt;To collect and immediately react to customer interactions and orders&lt;/li&gt;
&lt;li&gt;To monitor patients in a hospital to ensures timely treatment in emergencies&lt;/li&gt;
&lt;li&gt;To connect, store and make available data produced by different division of a company&lt;/li&gt;
&lt;li&gt;To serve as the foundation for data platforms and event-driven architectures&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Apache Kafka in the real world&lt;/p&gt;
&lt;p&gt;Here is a list of company that are using Kafka:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;The New York Times&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Pinterest&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;zalando&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;line&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;adidas&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;airbnb&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;cisco&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;What does it mean to be an event streaming platform?&lt;/p&gt;
&lt;p&gt;Kafka combines together three key capabilities:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;publish&lt;/strong&gt; (write) and &lt;strong&gt;subscribe to&lt;/strong&gt; (read) to stream of events&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;store&lt;/strong&gt; streams of events in a durable and reliable fashion&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;process&lt;/strong&gt; streams of event as they occur or retrospectively&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;All this functionality is provided in a distributed, highly scalable, elastic, fault-tolerant and
secure manner.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;How does Kafka work in a nutshell?&lt;/p&gt;
&lt;p&gt;The Kafka system consists of &lt;strong&gt;servers&lt;/strong&gt; and &lt;strong&gt;clients&lt;/strong&gt; communicating with each over via high-performance
TCP network protocol.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;SERVERS&lt;/strong&gt;. Kafka runs on a cluster comprising several servers which can be distributed across different
datacenters or cloud regions.
One of  the advantage of using Kafka as a cluster is that if a server goes down there is always
another server that  will cover for the failed server – this means being fault-tolerant.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;CLIENTS&lt;/strong&gt;. In the client-side of a Kafka application are to one in charge of the process either
writing or process the stream of events.
Kafka has several client libraries that allows you to develop kafka clients in a variety of
different languages.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Main Concepts and Terminology&lt;/p&gt;
&lt;p&gt;Kafka is built around the following concepts:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Event&lt;/strong&gt;. Generally speaking, an event records the fact that &lt;em&gt;something happened&lt;/em&gt; in your business world.
An event is often referred as &lt;strong&gt;message&lt;/strong&gt;.
An event is associated with a key, and it has a value and a timestamp (plus, optional metadata).
An example is the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Event Key: &amp;ldquo;Alice&amp;rdquo;&lt;/li&gt;
&lt;li&gt;Event Value: &amp;ldquo;Made a payment to Bob&amp;rdquo;&lt;/li&gt;
&lt;li&gt;Event timestamp: &amp;ldquo;Sept 29, 2020 at 11:00&amp;rdquo;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Producers&lt;/strong&gt; and &lt;strong&gt;Consumers&lt;/strong&gt;.  The producers are all the client applications that write the events
on Kafka, while the consumers are those application that &lt;em&gt;subscribe&lt;/em&gt; to these events, thus they are
the one consuming the data produced by the producers.
Kafka provides several guarantees, for instance with Kafka you have the guarantee that each event
will be processed exactly once&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Topics&lt;/strong&gt;. Event are organized in topics. They resemble the concept of folder within a file system.
For instance, you can have a topic called &amp;ldquo;payments&amp;rdquo; that records all the transactions that happened
in your business.&lt;/p&gt;
&lt;p&gt;A topic can have zero-to-multiple producers or receivers.&lt;/p&gt;
&lt;p&gt;Events can be consumed any time, in fact they can reside with the Kafka storage system for
an arbitrarily long time, without affecting the performance of the application. In fact,
one cool feature about Kafka is that its operations are constant with respect to the data size.
Therefore, storing data for a long time is perfectly fine.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Partitions&lt;/strong&gt;. Usually, a topic is spread across different &amp;ldquo;buckets&amp;rdquo; located on different Kafka
brokers.
When an event occurs, it is actually appended to one of the topic partitions,
Kafka is able to guarantee:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Events with same event key are always written in the same partition&lt;/li&gt;
&lt;li&gt;Consumers  read the event within the same partition in the exact order of arrival&lt;/li&gt;
&lt;/ul&gt;








  











&lt;figure id=&#34;figure-a-topic-with-four-partitions&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;/media/img/kafka-partititons.png&#34; data-caption=&#34;A topic with four partitions&#34;&gt;


  &lt;img src=&#34;/media/img/kafka-partititons.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    A topic with four partitions
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Replica&lt;/strong&gt;. In order to achieve the fault-tolerance and high-availability each topic is
replicated across different datacenters or geographical regions.
Therefore there are always multiple brokers that have a copy of the data, just in case something
goes wrong.
A commonly used setting is to use a replication factor of 3 – these each data is copied three times.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The code for this lesson is available &lt;a href=&#34;https://github.com/acalio/BDAnalytics/tree/main/08-introduction-to-kafka&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h1 id=&#34;quickstart&#34;&gt;QuickStart&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Step-1: Download Kafka from: &lt;a href=&#34;https://downloads.apache.org/kafka/2.6.0/kafka_2.13-2.6.0.tgz&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://downloads.apache.org/kafka/2.6.0/kafka&lt;sub&gt;2.13&lt;/sub&gt;-2.6.0.tgz&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;wget https://downloads.apache.org/kafka/2.6.0/kafka_2.13-2.6.0.tgz
tar -xzf kafka_2.13-2.6.0.tgz
cd kafka_2.13-2.6.0
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Step-2: Start the Kafka Environment&lt;/p&gt;
&lt;p&gt;First you need to activate the Zookeeper service.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;zookeper-server-start.sh config/zookeper.properties
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In another terminal session run:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kafka-server-start.sh config/server.properties
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Step-3: Create a topic to store your events&lt;/p&gt;
&lt;p&gt;Before you can write your first event you have to create a topic.&lt;/p&gt;
&lt;p&gt;In order to create a topic, you must issue the following command:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kafka-topics.sh --create --topic quickstart-events --bootstrap-server localhost:9092
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you want to obtain some information on the topic you have just created, you need to issue the
following command:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kafka-topics.sh --describe --topic quickstart-events --bootstrap-server localhost:9092
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Step-4: Setup a producer and a consumer from the command line&lt;/p&gt;
&lt;p&gt;First you need to setup a &lt;em&gt;producer&lt;/em&gt;. There is a built-in command line tool.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ kafka-console-producer.sh --topic quickstart-events --bootstrap-server localhost:9092
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then you can produce messages directly from the console:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;gt; my first event
&amp;gt; my second event
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There is also a built-in command line tool for the consumer.
You can run the following command:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kafka-console-consumer.sh --topic quickstart-events --from-beginning --bootstrap-server localhost:9092
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h1 id=&#34;kafka-architecture&#34;&gt;Kafka Architecture&lt;/h1&gt;
&lt;p&gt;Kafka is essentially a commit with a very simple data structure that is able
to be extremely fault tolerant and (horizontally) scalable.&lt;/p&gt;
&lt;p&gt;The Kafka commit log is basically ad queue where the records cannot be directly
deleted or modified, but they can only be appended at the end of the data structure.&lt;/p&gt;
&lt;p&gt;The order of items stored in the queue reflects the order of arrival to the system.
In particular, for all the messages stored in the same partition kafka guarantees its order.&lt;/p&gt;
&lt;p&gt;Because of this strong guarantee, records are immutable, once they are in the queue
they cannot be modified.&lt;/p&gt;
&lt;p&gt;Moreover, Kafka automatically assigns each record with a unique sequential ID, known
as &amp;ldquo;offset&amp;rdquo;.&lt;/p&gt;








  











&lt;figure id=&#34;figure-kafka-offset&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;/media/img/kafka-offset.png&#34; data-caption=&#34;Kafka Offset&#34;&gt;


  &lt;img src=&#34;/media/img/kafka-offset.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Kafka Offset
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;h1 id=&#34;component-overview&#34;&gt;Component Overview&lt;/h1&gt;
&lt;p&gt;In the Kafka architecture the following entities are involved:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Topics&lt;/li&gt;
&lt;li&gt;Producers&lt;/li&gt;
&lt;li&gt;Consumers&lt;/li&gt;
&lt;li&gt;Consumers groups&lt;/li&gt;
&lt;li&gt;Clusters&lt;/li&gt;
&lt;li&gt;Brokers&lt;/li&gt;
&lt;li&gt;Partitions&lt;/li&gt;
&lt;li&gt;Replicas&lt;/li&gt;
&lt;li&gt;Leaders&lt;/li&gt;
&lt;li&gt;Followers&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The following diagram provides a high lever overview of how
these components interact with each others








  











&lt;figure id=&#34;figure-kafka-components-overview&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;/media/img/kafka-component.png&#34; data-caption=&#34;Kafka components overview&#34;&gt;


  &lt;img src=&#34;/media/img/kafka-component.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Kafka components overview
  &lt;/figcaption&gt;


&lt;/figure&gt;
&lt;/p&gt;
&lt;h1 id=&#34;kafka-api-architecture&#34;&gt;Kafka API Architecture&lt;/h1&gt;
&lt;p&gt;There are four different APIs:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Producer API&lt;/strong&gt;. It enables an application to publish a stream of records to one
or more Kafka topics.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Consumer API&lt;/strong&gt;. It enables an application to subscribe to one or more Kafka topics.
It also enables an application to process the stream of data deriving from the topic it
is subscribed to.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Streams API&lt;/strong&gt;. It enables an application to process  input streams and to produce
other streams as output. It is very useful when you need to design stream-oriented applications&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Connect API&lt;/strong&gt;. It connects application or data systems to Kafka topics.
For instance, with the Connect API, a connector could capture all the updates
to a database and ensure that all the updates are available within a Kafka topic&lt;/li&gt;
&lt;/ol&gt;
&lt;h1 id=&#34;kafka-cluster-architecture&#34;&gt;Kafka Cluster Architecture&lt;/h1&gt;
&lt;p&gt;Here is a list of the main Kafka components within a Kafka cluster.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Broker&lt;/p&gt;
&lt;p&gt;Any server within a Kafka cluster is considered a Kafka Broker.
Brokers achieves the requirement of fault-tolerance, reliability and
availability at the heart of the Kafka ecosystem.&lt;/p&gt;
&lt;p&gt;Each broker is able to handle, thus to read and write, hundreds of messages every second.&lt;/p&gt;
&lt;p&gt;A broker has a unique ID and it responsible for a number of partitions belonging to one of
more topics.&lt;/p&gt;
&lt;p&gt;The brokers leverage ZooKeper to elect a leader among them. A leader is in charge
for dealing with client requests directed towards a particular partition.&lt;/p&gt;
&lt;p&gt;Clearly, the more brokers are in the cluster, the higher the reliability of the entire
system will be.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Apache Zookeeper Architecture&lt;/p&gt;
&lt;p&gt;ZooKeper is the manager of the entire cluster.
It knows, every time, who are the nodes involved in the cluster and
it notifies the other brokers when something changes in terms of
topics added or removed.&lt;/p&gt;
&lt;p&gt;For instance, when a new Broker joins the cluster, ZooKeper is responsible for
informing the entire cluster. It does the same thing also when a Borker fails.&lt;/p&gt;
&lt;p&gt;Furthermore, the ZooKeeper coordinates the  elections of the leader of each partition.&lt;/p&gt;
&lt;p&gt;The election is run every time something changes from the broker perspective.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Producer&lt;/p&gt;
&lt;p&gt;It represents a data source. The process that publishes messages to
one or more Kafka topics.&lt;/p&gt;
&lt;p&gt;A Kafka producer is also able to balance the data among  different brokers&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Consumer&lt;/p&gt;
&lt;p&gt;It reads data by reading messages directly from the topics it is subscribed to.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h1 id=&#34;kafka-architecture-1&#34;&gt;Kafka Architecture&lt;/h1&gt;
&lt;p&gt;The following concepts are the main concepts required to understand the entire
framework.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Kafka Topic&lt;/p&gt;
&lt;p&gt;It is basically a channel through which data is streamed.&lt;/p&gt;
&lt;p&gt;Producers publish their messages to topics while consumers read messages from
the topic thy are subscribed to.&lt;/p&gt;
&lt;p&gt;Topics are useful because they provide the ability of organizing messages within
groups. A topic is identified with a unique names within the same Kafka cluster.&lt;/p&gt;
&lt;p&gt;There is no limit to the number of topics that can be created on the same cluster.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Kafka Partitions&lt;/p&gt;
&lt;p&gt;Topics are divided into partitions.
The partitions are also replicated across the different brokers partecipating to
a Kafka cluster.&lt;/p&gt;
&lt;p&gt;From each partition, multiple consumers and producers can read and write in parallel.&lt;/p&gt;
&lt;p&gt;All the messages  that have the same keys will be written on the same partition.&lt;/p&gt;
&lt;p&gt;As regards messages without a key, they will be written among different partitions in
a round robin fashion.&lt;/p&gt;
&lt;p&gt;Messages with the same key will be processed respecting the order of their arrival.&lt;/p&gt;
&lt;p&gt;There is no limit on the number of different partitions that can be created on
topic.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Topic Replication Factor&lt;/p&gt;
&lt;p&gt;Replicating data is essential in order to achieve both availability and reliability.
The replication factor determines how many replicas are there for every topic.&lt;/p&gt;
&lt;p&gt;It is defined on the topic level and it applies on the partition level.&lt;/p&gt;
&lt;p&gt;For instance, a topic replication factor of 3 means that each partition, belonging to
that topic, will be replicated 3 times.&lt;/p&gt;
&lt;p&gt;Given a topic, for each partition a broker is elected as leaders while the other ones
are in charge of keeping the replicas.&lt;/p&gt;
&lt;p&gt;Clearly, the replication factor cannot be greater than the total number of brokers.&lt;/p&gt;
&lt;p&gt;When a replica is up to date with the leader is said to be an In-Sync Replica (ISR)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Consumer Group&lt;/p&gt;
&lt;p&gt;A group of consumers includes all the consumers that share a common task.&lt;/p&gt;
&lt;p&gt;When Kafka sends messages to a group of consumers, each individual consumer is assigned
to a particular partition, then only that particular consumer will read from that partition.
This means that there is a one-to-one relationship between partitions and consumers within
the group. As a consequence, if the number of consumer within group exceeds the number
partitions, some consumer will be inactive.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h1 id=&#34;understanding-the-entire-architecture&#34;&gt;Understanding the entire Architecture&lt;/h1&gt;
&lt;p&gt;Here we focus on understanding how the components
presented above cooperate with each others.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Example 1: One Consumer, One Producer&lt;/li&gt;
&lt;/ol&gt;








  











&lt;figure id=&#34;figure-one-producer-one-consumer&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;/media/img/kafka-example1.png&#34; data-caption=&#34;One Producer, One Consumer&#34;&gt;


  &lt;img src=&#34;/media/img/kafka-example1.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    One Producer, One Consumer
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;
&lt;p&gt;Example 2: One Producer, Multiple Consumers








  











&lt;figure id=&#34;figure-one-producers-multiple-consumers&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;/media/img/kafka-example2.png&#34; data-caption=&#34;One Producers, Multiple Consumers&#34;&gt;


  &lt;img src=&#34;/media/img/kafka-example2.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    One Producers, Multiple Consumers
  &lt;/figcaption&gt;


&lt;/figure&gt;
&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Example 3: Consumer Groups








  











&lt;figure id=&#34;figure-consumer-groups&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;/media/img/kafka-example3.png&#34; data-caption=&#34;Consumer Groups&#34;&gt;


  &lt;img src=&#34;/media/img/kafka-example3.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Consumer Groups
  &lt;/figcaption&gt;


&lt;/figure&gt;
&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Introduction to Scala and its Build System</title>
      <link>/courses/bdanalytics/scalaintro/</link>
      <pubDate>Sun, 05 May 2019 00:00:00 +0100</pubDate>
      <guid>/courses/bdanalytics/scalaintro/</guid>
      <description>&lt;p&gt;This lesson provides a quick introduction to the Scala ecosystem.&lt;/p&gt;
&lt;p&gt;What you will learn:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;How to write a Scala program&lt;/li&gt;
&lt;li&gt;How to compile Scala source code via the SBT shell&lt;/li&gt;
&lt;li&gt;Ho to work with loops and data structures&lt;/li&gt;
&lt;li&gt;How to use the the SBT build system&lt;/li&gt;
&lt;li&gt;How to structure a real-world project&lt;/li&gt;
&lt;li&gt;How to  manage dependencies in your project&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The material for this lesson is available  &lt;a href=&#34;https://github.com/acalio/BDAnalytics/tree/main/01-Introduction-to-Scala&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Introduction to Spark SQL</title>
      <link>/courses/bdanalytics/sparksql/</link>
      <pubDate>Sun, 05 May 2019 00:00:00 +0100</pubDate>
      <guid>/courses/bdanalytics/sparksql/</guid>
      <description>&lt;p&gt;SparkSQL is a library for structured data processing.
It provides an abstraction mechanism – the main one is called &lt;code&gt;DataFrame&lt;/code&gt; – which can
serve as  a distributed SQL query engine.&lt;/p&gt;
&lt;p&gt;Spark SQL offers the following features:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Integrated&lt;/strong&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Seamlessly mix SQL queries with Spark programs.&lt;/li&gt;
&lt;li&gt;Spark SQL lets you query structured data as a distributed dataset (RDD) in Spark&lt;/li&gt;
&lt;li&gt;This tight integration makes it easy to run SQL queries alongside complex analytic algorithms&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Unified Data Access&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Load and query data from a variety of different sources like: Apache Hive tables, parquet files, JSON files, etc.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Scalability&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Use the same engine for both interactive and long queries&lt;/li&gt;
&lt;li&gt;SparkSQL leverages on the RDD model to provide fault tolerance an scalability&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The code for this lesson is available &lt;a href=&#34;https://github.com/acalio/BDAnalytics/tree/main/04-Spark-SQL&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h1 id=&#34;architecture&#34;&gt;Architecture&lt;/h1&gt;








  











&lt;figure id=&#34;figure-sparksql-architecture&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;/media/img/arch.png&#34; data-caption=&#34;SparkSQL architecture&#34;&gt;


  &lt;img src=&#34;/media/img/arch.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    SparkSQL architecture
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;The architecture contains three layers namely:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Language API − Spark is compatible with different languages and Spark SQL. It is also, supported by these languages- API (python, scala, java, HiveQL).&lt;/li&gt;
&lt;li&gt;Schema RDD − Spark Core is designed with special data structure called RDD. Generally, Spark SQL works on schemas, tables, and records. Therefore, we can use the Schema RDD as temporary table. We can call this Schema RDD as Data Frame.&lt;/li&gt;
&lt;li&gt;Data Sources − Usually the Data source for spark-core is a text file, Avro file, etc. However, the Data Sources for Spark SQL is different. Those are Parquet file, JSON document, HIVE tables, and Cassandra database.&lt;/li&gt;
&lt;/ol&gt;
&lt;h1 id=&#34;what-is-a-dataframe&#34;&gt;What is a DataFrame&lt;/h1&gt;
&lt;p&gt;A DataFrame is a distributed collection of data,
which is organized into named columns.
You can think of a DataFrame as a relational table.&lt;/p&gt;
&lt;p&gt;DataFrames can be constructed from a variety of different sources such as Hive tables, Structured Data files,
external database, or also an existing RDD.&lt;/p&gt;
&lt;h2 id=&#34;features-of-a-dataframe&#34;&gt;Features of a DataFrame&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Ability to process the data in the size of Kilobytes to Petabytes on a single node cluster to large cluster&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Supports different data formats (Avro, csv, elastic search, and Cassandra) and storage systems (HDFS, HIVE tables, mysql, etc).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;State of art optimization and code generation through the Spark SQL Catalyst optimizer (tree transformation framework).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Can be easily integrated with all Big Data tools and frameworks via Spark-Core.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Optimized application of udf function over the entire dataframe.
The following instructions will create a new column whose values are given by the column value to the power of two&lt;/p&gt;
&lt;p&gt;import org.apache.spark.sql.functions.udf
val square = (x=&amp;gt; x*x)
val squaredDF = df.withColumn(&amp;ldquo;square&amp;rdquo;, square(&amp;ldquo;value&amp;rdquo;))&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Example&lt;/p&gt;
&lt;p&gt;Imagine you have the following data, formatted as a JSON file:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;{
   {&amp;quot;id&amp;quot; : &amp;quot;1201&amp;quot;, &amp;quot;name&amp;quot; : &amp;quot;satish&amp;quot;, &amp;quot;age&amp;quot; : &amp;quot;25&amp;quot;}
   {&amp;quot;id&amp;quot; : &amp;quot;1202&amp;quot;, &amp;quot;name&amp;quot; : &amp;quot;krishna&amp;quot;, &amp;quot;age&amp;quot; : &amp;quot;28&amp;quot;}
   {&amp;quot;id&amp;quot; : &amp;quot;1203&amp;quot;, &amp;quot;name&amp;quot; : &amp;quot;amith&amp;quot;, &amp;quot;age&amp;quot; : &amp;quot;39&amp;quot;}
   {&amp;quot;id&amp;quot; : &amp;quot;1204&amp;quot;, &amp;quot;name&amp;quot; : &amp;quot;javed&amp;quot;, &amp;quot;age&amp;quot; : &amp;quot;23&amp;quot;}
   {&amp;quot;id&amp;quot; : &amp;quot;1205&amp;quot;, &amp;quot;name&amp;quot; : &amp;quot;prudvi&amp;quot;, &amp;quot;age&amp;quot; : &amp;quot;23&amp;quot;}
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can read this file and create a dataframe as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;...
val spark = SparkSession
      .builder
      .appName(&amp;quot;SparkSQL&amp;quot;)
      .master(&amp;quot;local[*]&amp;quot;)
      .getOrCreate()
val df =  spark.sqlContext.read.json(&amp;quot;example.json&amp;quot;)
df.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The last instruction returns the following result:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;+----+------+--------+
|age | id   |  name  |
+----+------+--------+
| 25 | 1201 | satish |
| 28 | 1202 | krishna|
| 39 | 1203 | amith  |
| 23 | 1204 | javed  |
| 23 | 1205 | prudvi |
+----+------+--------+
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can access to the structure underlying a dataframe as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;df.printSchema
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In this case it returns the following:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;root
   |-- age: string (nullable = true)
   |-- id: string (nullable = true)
   |-- name: string (nullable = true)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You handle a DataFrame in a very similar fashion to a Pandas dataframe.&lt;/p&gt;
&lt;p&gt;For instance:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;df.select(&amp;quot;name&amp;quot;).show 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Or also&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;// This import is needed to use the $-notation
import spark.sqlContext.implicts._ 
df.select($&amp;quot;name&amp;quot;).show 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It returns:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;+--------+
|  name  |
+--------+
| satish |
| krishna|
| amith  |
| javed  |
| prudvi |
+--------+
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can use filter:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;dfs.filter(dfs(&amp;quot;age&amp;quot;) &amp;gt; 23).show()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It returns:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;+----+------+--------+
|age | id   | name   |
+----+------+--------+
| 25 | 1201 | satish |
| 28 | 1202 | krishna|
| 39 | 1203 | amith  |
+----+------+--------+
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can group and apply aggregate functions to your data as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;dfs.groupBy(&amp;quot;age&amp;quot;).count().show()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It returns:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;+----+-----+
|age |count|
+----+-----+
| 23 |  2  |
| 25 |  1  |
| 28 |  1  |
| 39 |  1  |
+----+-----+
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;running-sql-queries&#34;&gt;Running SQL Queries&lt;/h2&gt;
&lt;p&gt;An SQLContext enables applications to run SQL queries programmatically while running SQL functions and returns the result as a DataFrame.&lt;/p&gt;
&lt;p&gt;Generally, in the background, SparkSQL supports two different methods for converting existing RDDs into DataFrames.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Inferring the schema via reflection&lt;/p&gt;
&lt;p&gt;This method uses reflection to generate the schema of an RDD that contains specific types
of objects. The Scala interface for Spark SQL supports automatically converting an RDD containing case classes to a DataFrame.
The &lt;strong&gt;case class&lt;/strong&gt; defines the schema of the table. The names
of the arguments to the case class are read using reflection and they become the names of the columns.&lt;/p&gt;
&lt;p&gt;Case classes can also be nested or contain complex
types such as Sequences or Arrays. This RDD can be implicitly be
converted to a DataFrame and then registered as a table.
Tables can be used in subsequent SQL statements.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Example&lt;/p&gt;
&lt;p&gt;Imagine you are given with the following data:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;1201, satish, 25
1202, krishna, 28
1203, amith, 39
1204, javed, 23
1205, prudvi, 23
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;First you need to define a case class – which is  class that only define its contructor – to provide your data with a fixed structure:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;case class Employee(id: Int, name: String, age: Int)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, you create an RDD mapping each line to the above case class and then convert it to a DataFrame&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;val spark = SparkSession
      .builder
      .appName(&amp;quot;SparkSQL&amp;quot;)
      .master(&amp;quot;local[*]&amp;quot;)
      .getOrCreate()
        
val df =  spark.sparkContext.textFile(&amp;quot;employee.txt&amp;quot;)
   .map(_.split(&amp;quot;,&amp;quot;)).map(e=&amp;gt;Employee(e(0).trim.toInt, e(1), e(2).trim.toInt)).toDF
df.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now you have a fully functional data frame. If you want to use the SQL engine your first need to register
the dataframe as a table:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;empl.registerTempTable(&amp;quot;employee&amp;quot;) //set the name of the table associated with the dataset
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then your can perform regular SQL query as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;spark.sqlContext.sql(&amp;quot;Select * from employee*&amp;quot;).show
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It returns:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;+------+---------+----+
|  id  |  name   |age |
+------+---------+----+
| 1201 | satish  | 25 |
| 1202 | krishna | 28 |
| 1203 | amith   | 39 |
| 1204 | javed   | 23 |
| 1205 | prudvi  | 23 |
+------+---------+----+
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;SparkSQL understands any sql query – so if you know SQL you are good to go.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Specify the schema programmatically&lt;/p&gt;
&lt;p&gt;The second method for creating DataFrame is through programmatic interface that allows you to construct a schema and then apply it to an existing RDD. We can create a DataFrame programmatically using the following three steps.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Create an RDD of Rows from an Original RDD.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Create the schema represented by a StructType matching the structure of Rows in the RDD created in Step 1.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Apply the schema to the RDD of Rows via createDataFrame method provided by SQLContext.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Example&lt;/p&gt;
&lt;p&gt;Imagine you are given with the following data:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;1201, satish, 25
1202, krishna, 28
1203, amith, 39
1204, javed, 23
1205, prudvi, 23
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, you create an RDD from the text file.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;//remember to include these imports
import org.apache.spark.sql.Row
import org.apache.spark.sql.types.{StructType, StructField, StringType}
        
val spark = SparkSession
      .builder
      .appName(&amp;quot;SparkSQL&amp;quot;)
      .master(&amp;quot;local[*]&amp;quot;)
      .getOrCreate()
        
val df =  spark.sparkContext.textFile(&amp;quot;employee.txt&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, instead of defining a case class as we did earlier, we define schema with a String.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;val schemaString = &amp;quot;id name age&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The above string is then used to generate a schema as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;val schema = StructType(schemaString.split(&amp;quot; &amp;quot;).map(fieldName =&amp;gt; =StructField(fieldName, StringType, true)))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Use the following command to convert an RDD (employee) to Rows. It means, here we are specifying the logic for reading the RDD data and store it into rowRDD. Here we are using two map functions: one is a delimiter for splitting the record string (.map(&lt;sub&gt;.split&lt;/sub&gt;(&amp;quot;,&amp;quot;))) and the second
map function for defining a Row with the field index
value (.map(e ⇒ Row(e(0).trim.toInt, e(1), e(2).trim.toInt))).&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;val rowRDD = employee.map(_.split(&amp;quot;,&amp;quot;)).map(e ⇒ Row(e(0).trim.toInt, e(1), e(2).trim.toInt))
val employeeDF = sqlContext.createDataFrame(rowRDD, schema)
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;comparative-evaluation&#34;&gt;Comparative evaluation&lt;/h1&gt;
&lt;h2 id=&#34;dataframe-vs-dataset&#34;&gt;DataFrame vs DataSet&lt;/h2&gt;
&lt;p&gt;They are basically the same object, namely a collection of structured data
– for instance DataSet[Person], DataSet[(String, Double)].&lt;/p&gt;
&lt;p&gt;Actually, a &lt;code&gt;DataFrame&lt;/code&gt; is an alias for &lt;code&gt;Dataset[Row]&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The major difference between the two is that the structure of the data contained within
a  &lt;code&gt;DataFrame&lt;/code&gt;  is inferred at runtime, while for a &lt;code&gt;DataSet&lt;/code&gt; object Scala is able to infer the
actual type of the objects at compile time.
Clearly, this second mechanism is beneficial in terms of performance and it is also less prone
to potential errors.&lt;/p&gt;
&lt;h2 id=&#34;dataset-vs-rdd&#34;&gt;DataSet vs RDD&lt;/h2&gt;
&lt;p&gt;An RDD can be converted to a &lt;code&gt;DataSet&lt;/code&gt; object with the method &lt;code&gt;toDS&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;DataSets are more convenient than RDD for the following reasons:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;better efficiency&lt;/li&gt;
&lt;li&gt;better interoperability with other libraries:
&lt;ul&gt;
&lt;li&gt;MLlib relies on Datasets&lt;/li&gt;
&lt;li&gt;Spark streaming is moving towards structured streaming&lt;/li&gt;
&lt;li&gt;Everything that use apache Avro&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;p&gt;The material for this lesson is available  &lt;a href=&#34;https://github.com/acalio/BDAnalytics/tree/main/04-Spark-SQL/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Machine Learning with Spark</title>
      <link>/courses/bdanalytics/sparkmllib/</link>
      <pubDate>Sun, 05 May 2019 00:00:00 +0100</pubDate>
      <guid>/courses/bdanalytics/sparkmllib/</guid>
      <description>&lt;p&gt;MLlib is the Spark is the primary library for addressing machine learning problem within the Spark ecosystem.&lt;/p&gt;
&lt;p&gt;MLlib provides the following features:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A collection of ML Algorithms for solving a variety of ML problem, such as classification, regression,
clustering and collaborative filtering&lt;/li&gt;
&lt;li&gt;A collection of techniques for featurization, i.e., extracting and transforming the features of your dataset&lt;/li&gt;
&lt;li&gt;The ability to design e construct Pipeline of execution&lt;/li&gt;
&lt;li&gt;Persistence: models can be saved on file so that you will not be required to train a model every time you need to
use it&lt;/li&gt;
&lt;li&gt;Utility tools, mostly for linear algebra e statistics&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Spark MLlib is available in the version 3.0. It now uses dataset instead
of regular RDDs.&lt;/p&gt;
&lt;p&gt;In the following we will discuss some of the main tools provided by MLlib.&lt;/p&gt;
&lt;p&gt;The code for this lesson is available &lt;a href=&#34;https://github.com/acalio/BDAnalytics/tree/main/07-Spark-MLlib&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h1 id=&#34;basic-statistics&#34;&gt;Basic Statistics&lt;/h1&gt;
&lt;h2 id=&#34;correlation&#34;&gt;Correlation&lt;/h2&gt;
&lt;p&gt;Correlation measures the strength of a linear relationship between two variables.
More specifically, given two variables X e Y, they are considered:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;positively correlated – if when X increases Y increases and vice versa&lt;/li&gt;
&lt;li&gt;negatively correlated – if when X increases Y increases and vice versa&lt;/li&gt;
&lt;li&gt;no correlated – when their trends are not dependent from one another&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The quantity compute by MLLlib is the Pearson&amp;rsquo;s coefficient, which is defined in [-1,1].
A coefficient equal to 1 (-1) defines a &lt;em&gt;strong&lt;/em&gt; positive (negative) correlation.&lt;/p&gt;
&lt;p&gt;The following snippet compute the correlation matrix between each pair of variables.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;import org.apache.spark.ml.linalg.{Matrix, Vectors}
import org.apache.spark.ml.stat.Correlation
import org.apache.spark.sql.Row

val data = Seq(
  Vectors.sparse(4, Seq((0, 1.0), (3, -2.0))),
  Vectors.dense(4.0, 5.0, 0.0, 3.0),
  Vectors.dense(6.0, 7.0, 0.0, 8.0),
  Vectors.sparse(4, Seq((0, 9.0), (3, 1.0)))
)

val df = data.map(Tuple1.apply).toDF(&amp;quot;features&amp;quot;)
val Row(coeff1: Matrix) = Correlation.corr(df, &amp;quot;features&amp;quot;).head
println(s&amp;quot;Pearson correlation matrix:\n $coeff1&amp;quot;)

val Row(coeff2: Matrix) = Correlation.corr(df, &amp;quot;features&amp;quot;, &amp;quot;spearman&amp;quot;).head
println(s&amp;quot;Spearman correlation matrix:\n $coeff2&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;summarizer&#34;&gt;Summarizer&lt;/h2&gt;
&lt;p&gt;It provides a summary of the main statistics for each column of a given &lt;em&gt;DataFrame&lt;/em&gt;. The metrics computed
by this object are the column-wise:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;max&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;min&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;average&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;sum&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;variance/std&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;number of non-zero elements&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;total count&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;import org.apache.spark.ml.linalg.{Vector, Vectors}
import org.apache.spark.ml.stat.Summarizer
	
val data = Seq(
    (Vectors.dense(2.0, 3.0, 5.0), 1.0),
    (Vectors.dense(4.0, 6.0, 7.0), 2.0)
    )
	
val df = data.toDF(&amp;quot;features&amp;quot;, &amp;quot;weight&amp;quot;)
	
val (meanVal, varianceVal) = df.select(metrics(&amp;quot;mean&amp;quot;, &amp;quot;variance&amp;quot;)
    .summary($&amp;quot;features&amp;quot;, $&amp;quot;weight&amp;quot;).as(&amp;quot;summary&amp;quot;))
    .select(&amp;quot;summary.mean&amp;quot;, &amp;quot;summary.variance&amp;quot;)
    n.as[(Vector, Vector)].first()
	
println(s&amp;quot;with weight: mean = ${meanVal}, variance = ${varianceVal}&amp;quot;)
	
val (meanVal2, varianceVal2) = df.select(mean($&amp;quot;features&amp;quot;), variance($&amp;quot;features&amp;quot;))
    .as[(Vector, Vector)].first()
	
println(s&amp;quot;without weight: mean = ${meanVal2}, sum = ${varianceVal2}&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;data-sources&#34;&gt;Data Sources&lt;/h1&gt;
&lt;p&gt;MLlib is able to  work with a variety of &amp;ldquo;standard&amp;rdquo; formats as Parquet, CSV, JSON. In addition to that the
library provides mechanisms to read some other special data format.&lt;/p&gt;
&lt;h2 id=&#34;image-data-source&#34;&gt;Image data source&lt;/h2&gt;
&lt;p&gt;This data is used to load image files directly from a folder. Images can be in any format (e.g., jpg, png, …).
When you load the DataFrame, it has a column of type StryctType, which represents the actual image.
Each image has an image schema which comprises the following information:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;origin: StringType, the path of the file containing the image&lt;/li&gt;
&lt;li&gt;height: IntegerType, the height of the image&lt;/li&gt;
&lt;li&gt;width: IntegerType, the width of the image&lt;/li&gt;
&lt;li&gt;nChannels: Integertype, number of image channels&lt;/li&gt;
&lt;li&gt;mode: IntegerType, OpenCV compatible type&lt;/li&gt;
&lt;li&gt;data: BinaryType, the image bytes in OpenCV-compatible format&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In order to read the images you need to use the &amp;ldquo;image&amp;rdquo; format. You also need to provide the path of the folder
containing the images you want to read.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;val df = spark.read.format(&amp;quot;image&amp;quot;).option(&amp;quot;dropInvalid&amp;quot;, true).load(&amp;quot;{your path}&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;libsvm-data-source&#34;&gt;LIBSVM data source&lt;/h2&gt;
&lt;p&gt;This source is used to read data in the libsvm format from a given directory.
When you load the file, MLlib creates a data frame with two columns:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;label: DoubleType, represents the labels of the dataset&lt;/li&gt;
&lt;li&gt;features: VectorUDT, represents the feature-vectors of each data point&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In order to read this type of data you need to specify the &amp;ldquo;libsvm&amp;rdquo; format.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;val df = spark.read.format(&amp;quot;libsvm&amp;quot;).option(&amp;quot;numFeatures&amp;quot;, &amp;quot;780&amp;quot;).load(&amp;quot;{your path}&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;pipelines&#34;&gt;Pipelines&lt;/h1&gt;
&lt;p&gt;Pipelines are a very convenient to define e organize all the single procedures and step that contribute to you entire
machine learning approach.&lt;/p&gt;
&lt;p&gt;They are very similar to &lt;em&gt;scikit-learn&lt;/em&gt; pipelines. The Pipelines API are designed around the following concepts:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;DataFrame&lt;/li&gt;
&lt;li&gt;Transformer. It is an abstraction that includes both feature transformers and ML models.
Any transformer has a method &lt;code&gt;transform()&lt;/code&gt; which takes a DataFrame as input and returns another DataFrame
obtained by applying some function, whose results are retrieved within a new column appended to the original structure.&lt;/li&gt;
&lt;li&gt;Estimators. It is an abstraction for denoting any algorithm that has the method &lt;code&gt;fit&lt;/code&gt;  in its own interface.
The &lt;code&gt;fit()&lt;/code&gt; method takes as input a &lt;code&gt;DataFrame&lt;/code&gt;  and returns an object of type &lt;code&gt;Model&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Pipelines. Usually, when you deal with ML problems, the entire algorithm representing your approach for solving the problem
can be narrowed down to a set of subsequent different steps.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;how-pipelines-work&#34;&gt;How Pipelines Work&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s imagine you are dealing with some
classification problems over text documents.&lt;/p&gt;
&lt;p&gt;Chances are that your workflow will include the following stages:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Split each document into words&lt;/li&gt;
&lt;li&gt;Convert each word int a vector – you may want to apply some representation model such as Word2Vec&lt;/li&gt;
&lt;li&gt;Train and compute the prediction of your model&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Step 1 requires  a Transformer, while Step 2 and Step 3 require to use some Estimators. Through a Pipeline you
will be able to aggregate all these steps and to represent them as a pipeline.&lt;/p&gt;
&lt;p&gt;A Pipeline is specified as a sequence of stages, and each stage is either a Transformer or an Estimator.
These stages are run in order, and the input DataFrame is transformed as it passes through each stage.&lt;/p&gt;
&lt;p&gt;For Transformer stages, the transform() method is called on the DataFrame.
For Estimator stages, the fit() method is called to produce a Transformer
(which becomes part of the PipelineModel, or fitted Pipeline),
and that Transformer’s transform() method is called on the DataFrame.&lt;/p&gt;
&lt;p&gt;We illustrate this for the simple text document workflow. The figure below is for the training time usage of a Pipeline.&lt;/p&gt;
&lt;p&gt;The following image represents the pipeline described above.








  











&lt;figure id=&#34;figure-pipeline&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;/media/img/pip.png&#34; data-caption=&#34;Pipeline&#34;&gt;


  &lt;img src=&#34;/media/img/pip.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Pipeline
  &lt;/figcaption&gt;


&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;A Pipeline works as an Estimator, it means that it has the method &lt;code&gt;fit()&lt;/code&gt;. When you call this method
on the pipeline, all the stages are executed. More specifically, for any stage S&lt;sub&gt;i&lt;/sub&gt; we have:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;if S&lt;sub&gt;i&lt;/sub&gt; is a transformer, the pipeline will call the method transform.&lt;/li&gt;
&lt;li&gt;if S&lt;sub&gt;i&lt;/sub&gt; is an estimator, the pipeline will first call the &lt;code&gt;fit&lt;/code&gt; method and then the &lt;code&gt;transform&lt;/code&gt; method&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The method &lt;code&gt;fit&lt;/code&gt; called on a Pipeline yields a Pipeline model, which is also a Transformer. This means that you can
use the this model to call the &lt;code&gt;transform&lt;/code&gt; method. As a result, the pipeline will go through all the stages by calling
the &lt;code&gt;transform&lt;/code&gt; method over each on of them. Be careful that, as opposed to when you called the &lt;code&gt;fit&lt;/code&gt; method
over the entire pipeline, when you call &lt;code&gt;transform&lt;/code&gt; on the Pipeline, all the estimators work as Transformers.&lt;/p&gt;
&lt;h2 id=&#34;parameters&#34;&gt;Parameters&lt;/h2&gt;
&lt;p&gt;You can specify the parameters for either a Transformer of an Estimators in one of the following
two ways:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Both Estimators and Transformers share a common API to specify their parameters.
A parameter is specified via a &lt;code&gt;Param&lt;/code&gt; object, which is a named parameter with a self-contained documentation.
A &lt;code&gt;ParamMap&lt;/code&gt; is instead a set of parameter-value pairs. You can define a &lt;code&gt;ParamMap&lt;/code&gt; and pass it to the
&lt;code&gt;fit()&lt;/code&gt; or the &lt;code&gt;transform()&lt;/code&gt; method. A &lt;code&gt;ParamMap&lt;/code&gt; is created with the following notation:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;ParamMap(l1.maxIter -&amp;gt; 10, l2.maxIter -&amp;gt; 20)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Where l1 and l2 are two instance included in the Pipeline.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Set the parameter directly on the instance that you include within the your Pipeline. Let &lt;code&gt;algo&lt;/code&gt; be an instance
of any ML algorithm available in MLlib. You can use this instance to set directly the parameters for that particular algoirthm
Usually, you have method like &lt;code&gt;set{NameOfTheParameter}&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;examples&#34;&gt;Examples&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Estimator, Transformer and Param&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;import org.apache.spark.ml.classification.LogisticRegression
import org.apache.spark.ml.linalg.{Vector, Vectors}
import org.apache.spark.ml.param.ParamMap
import org.apache.spark.sql.Row
    
// Prepare training data from a list of (label, features) tuples.
val training = spark.createDataFrame(Seq(
  (1.0, Vectors.dense(0.0, 1.1, 0.1)),
  (0.0, Vectors.dense(2.0, 1.0, -1.0)),
  (0.0, Vectors.dense(2.0, 1.3, 1.0)),
  (1.0, Vectors.dense(0.0, 1.2, -0.5))
)).toDF(&amp;quot;label&amp;quot;, &amp;quot;features&amp;quot;)
    
// Create a LogisticRegression instance. This instance is an Estimator.
val lr = new LogisticRegression()
// Print out the parameters, documentation, and any default values.
println(s&amp;quot;LogisticRegression parameters:\n ${lr.explainParams()}\n&amp;quot;)
    
// We may set parameters using setter methods.
lr.setMaxIter(10)
  .setRegParam(0.01)
    
// Learn a LogisticRegression model. This uses the parameters stored in lr.
val model1 = lr.fit(training)
// Since model1 is a Model (i.e., a Transformer produced by an Estimator),
// we can view the parameters it used during fit().
// This prints the parameter (name: value) pairs, where names are unique IDs for this
// LogisticRegression instance.
println(s&amp;quot;Model 1 was fit using parameters: ${model1.parent.extractParamMap}&amp;quot;)
    
// We may alternatively specify parameters using a ParamMap,
// which supports several methods for specifying parameters.
val paramMap = ParamMap(lr.maxIter -&amp;gt; 20)
  .put(lr.maxIter, 30)  // Specify 1 Param. This overwrites the original maxIter.
  .put(lr.regParam -&amp;gt; 0.1, lr.threshold -&amp;gt; 0.55)  // Specify multiple Params.
    
// One can also combine ParamMaps.
val paramMap2 = ParamMap(lr.probabilityCol -&amp;gt; &amp;quot;myProbability&amp;quot;)  // Change output column name.
val paramMapCombined = paramMap ++ paramMap2
    
// Now learn a new model using the paramMapCombined parameters.
// paramMapCombined overrides all parameters set earlier via lr.set* methods.
val model2 = lr.fit(training, paramMapCombined)
println(s&amp;quot;Model 2 was fit using parameters: ${model2.parent.extractParamMap}&amp;quot;)
    
// Prepare test data.
val test = spark.createDataFrame(Seq(
  (1.0, Vectors.dense(-1.0, 1.5, 1.3)),
  (0.0, Vectors.dense(3.0, 2.0, -0.1)),
  (1.0, Vectors.dense(0.0, 2.2, -1.5))
)).toDF(&amp;quot;label&amp;quot;, &amp;quot;features&amp;quot;)
    
// Make predictions on test data using the Transformer.transform() method.
// LogisticRegression.transform will only use the &#39;features&#39; column.
// Note that model2.transform() outputs a &#39;myProbability&#39; column instead of the usual
// &#39;probability&#39; column since we renamed the lr.probabilityCol parameter previously.
model2.transform(test)
  .select(&amp;quot;features&amp;quot;, &amp;quot;label&amp;quot;, &amp;quot;myProbability&amp;quot;, &amp;quot;prediction&amp;quot;)
  .collect()
  .foreach { case Row(features: Vector, label: Double, prob: Vector, prediction: Double) =&amp;gt;
    println(s&amp;quot;($features, $label) -&amp;gt; prob=$prob, prediction=$prediction&amp;quot;)
  }
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Pipeline&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;import org.apache.spark.ml.{Pipeline, PipelineModel}
import org.apache.spark.ml.classification.LogisticRegression
import org.apache.spark.ml.feature.{HashingTF, Tokenizer}
import org.apache.spark.ml.linalg.Vector
import org.apache.spark.sql.Row
    
// Prepare training documents from a list of (id, text, label) tuples.
val training = spark.createDataFrame(Seq(
  (0L, &amp;quot;a b c d e spark&amp;quot;, 1.0),
  (1L, &amp;quot;b d&amp;quot;, 0.0),
  (2L, &amp;quot;spark f g h&amp;quot;, 1.0),
  (3L, &amp;quot;hadoop mapreduce&amp;quot;, 0.0)
)).toDF(&amp;quot;id&amp;quot;, &amp;quot;text&amp;quot;, &amp;quot;label&amp;quot;)
    
// Configure an ML pipeline, which consists of three stages: tokenizer, hashingTF, and lr.
val tokenizer = new Tokenizer()
  .setInputCol(&amp;quot;text&amp;quot;)
  .setOutputCol(&amp;quot;words&amp;quot;)
val hashingTF = new HashingTF()
  .setNumFeatures(1000)
  .setInputCol(tokenizer.getOutputCol)
  .setOutputCol(&amp;quot;features&amp;quot;)
val lr = new LogisticRegression()
  .setMaxIter(10)
  .setRegParam(0.001)
val pipeline = new Pipeline()
  .setStages(Array(tokenizer, hashingTF, lr))
    
// Fit the pipeline to training documents.
val model = pipeline.fit(training)
    
// Now we can optionally save the fitted pipeline to disk
model.write.overwrite().save(&amp;quot;/tmp/spark-logistic-regression-model&amp;quot;)
    
// We can also save this unfit pipeline to disk
pipeline.write.overwrite().save(&amp;quot;/tmp/unfit-lr-model&amp;quot;)
    
// And load it back in during production
val sameModel = PipelineModel.load(&amp;quot;/tmp/spark-logistic-regression-model&amp;quot;)
    
// Prepare test documents, which are unlabeled (id, text) tuples.
val test = spark.createDataFrame(Seq(
  (4L, &amp;quot;spark i j k&amp;quot;),
  (5L, &amp;quot;l m n&amp;quot;),
  (6L, &amp;quot;spark hadoop spark&amp;quot;),
  (7L, &amp;quot;apache hadoop&amp;quot;)
)).toDF(&amp;quot;id&amp;quot;, &amp;quot;text&amp;quot;)
    
// Make predictions on test documents.
model.transform(test)
  .select(&amp;quot;id&amp;quot;, &amp;quot;text&amp;quot;, &amp;quot;probability&amp;quot;, &amp;quot;prediction&amp;quot;)
  .collect()
  .foreach { case Row(id: Long, text: String, prob: Vector, prediction: Double) =&amp;gt;
    println(s&amp;quot;($id, $text) --&amp;gt; prob=$prob, prediction=$prediction&amp;quot;)
  }
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;extracting-transforming-and-selecting&#34;&gt;Extracting, transforming and Selecting&lt;/h1&gt;
&lt;p&gt;This section is organized into three different subsections:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Feature Extractors. This section introduces some algorithm for extracting features from your dataset&lt;/li&gt;
&lt;li&gt;Feature Transformers. This section introduces some of the algorithm that execute commonly used transformations
the input dataset&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;feature-extractors&#34;&gt;Feature Extractors&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Word2Vec&lt;/p&gt;
&lt;p&gt;Word2Vec is an Estimator which takes as input a sequence of words representing a document and then
it trains a Word2VecModel over these words. The model maps each word to a unique fixed-size vector.
A document is then represented by simply taking the average of all the vectors associated with any word
contained within the original text.
Once you have a vector, you can apply all sorts of ML algorithms.&lt;/p&gt;
&lt;p&gt;The following snippet shows how to use this model.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;import org.apache.spark.ml.feature.Word2Vec
import org.apache.spark.ml.linalg.Vector
import org.apache.spark.sql.Row
    
// Input data: Each row is a bag of words from a sentence or document.
val documentDF = spark.createDataFrame(Seq(
  &amp;quot;Hi I heard about Spark&amp;quot;.split(&amp;quot; &amp;quot;),
  &amp;quot;I wish Java could use case classes&amp;quot;.split(&amp;quot; &amp;quot;),
  &amp;quot;Logistic regression models are neat&amp;quot;.split(&amp;quot; &amp;quot;)
).map(Tuple1.apply)).toDF(&amp;quot;text&amp;quot;)
    
// Learn a mapping from words to Vectors.
val word2Vec = new Word2Vec()
  .setInputCol(&amp;quot;text&amp;quot;)
  .setOutputCol(&amp;quot;result&amp;quot;)
  .setVectorSize(3)
  .setMinCount(0)
val model = word2Vec.fit(documentDF)
    
val result = model.transform(documentDF)
result.collect().foreach { case Row(text: Seq[_], features: Vector) =&amp;gt;
  println(s&amp;quot;Text: [${text.mkString(&amp;quot;, &amp;quot;)}] =&amp;gt; \nVector: $features\n&amp;quot;) }
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;feature-transformers&#34;&gt;Feature Transformers&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Binarizer&lt;/p&gt;
&lt;p&gt;Binarization is the process of thresholding numerical features to binary (0/1) features.&lt;/p&gt;
&lt;p&gt;Binarizer takes the common parameters inputCol and outputCol, as
well as the threshold for binarization. Feature values greater than the threshold
are binarized to 1.0; values equal to or less
than the threshold are binarized to 0.0.
Both Vector and Double types are supported for inputCol.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;import org.apache.spark.ml.feature.Binarizer
    
val data = Array((0, 0.1), (1, 0.8), (2, 0.2))
val dataFrame = spark.createDataFrame(data).toDF(&amp;quot;id&amp;quot;, &amp;quot;feature&amp;quot;)
    
val binarizer: Binarizer = new Binarizer()
  .setInputCol(&amp;quot;feature&amp;quot;)
  .setOutputCol(&amp;quot;binarized_feature&amp;quot;)
  .setThreshold(0.5)
    
val binarizedDataFrame = binarizer.transform(dataFrame)
    
println(s&amp;quot;Binarizer output with Threshold = ${binarizer.getThreshold}&amp;quot;)
binarizedDataFrame.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;PCA&lt;/p&gt;
&lt;p&gt;PCA is a statistical procedure that uses
an orthogonal transformation to convert a set of observations of possibly correlated
variables into a set of values of linearly uncorrelated variables
called principal components. A PCA class trains a model to project
vectors to a low-dimensional space using PCA. The example below shows how to
project 5-dimensional feature vectors into 3-dimensional principal components.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;import org.apache.spark.ml.feature.PCA
import org.apache.spark.ml.linalg.Vectors
    
val data = Array(
  Vectors.sparse(5, Seq((1, 1.0), (3, 7.0))),
  Vectors.dense(2.0, 0.0, 3.0, 4.0, 5.0),
  Vectors.dense(4.0, 0.0, 0.0, 6.0, 7.0)
)
val df = spark.createDataFrame(data.map(Tuple1.apply)).toDF(&amp;quot;features&amp;quot;)
    
val pca = new PCA()
  .setInputCol(&amp;quot;features&amp;quot;)
  .setOutputCol(&amp;quot;pcaFeatures&amp;quot;)
  .setK(3)
  .fit(df)
    
val result = pca.transform(df).select(&amp;quot;pcaFeatures&amp;quot;)
result.show(false)
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;OneHotEncoder&lt;/p&gt;
&lt;p&gt;One-hot-encoding maps a categorical feature to a binary vector with at most a single one-value.
For instance, imagine you have a categorical feature that allows 5 possible categorical values:
A,B,C,D,E. This feature is then represented as a 5-sized vector where each value is mapped to
a particular position. Therefore the value A - becomes -&amp;gt; [1,0,0,0,0], the value B - becomes -&amp;gt; [0,1,0,0,0]
ans so on.&lt;/p&gt;
&lt;p&gt;The following snippet shows how to use this transformer:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;import org.apache.spark.ml.feature.OneHotEncoder
    
val df = spark.createDataFrame(Seq(
  (0.0, 1.0),
  (1.0, 0.0),
  (2.0, 1.0),
  (0.0, 2.0),
  (0.0, 1.0),
  (2.0, 0.0)
)).toDF(&amp;quot;categoryIndex1&amp;quot;, &amp;quot;categoryIndex2&amp;quot;)
    
val encoder = new OneHotEncoder()
  .setInputCols(Array(&amp;quot;categoryIndex1&amp;quot;, &amp;quot;categoryIndex2&amp;quot;))
  .setOutputCols(Array(&amp;quot;categoryVec1&amp;quot;, &amp;quot;categoryVec2&amp;quot;))
val model = encoder.fit(df)
    
val encoded = model.transform(df)
encoded.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;standardscaler&#34;&gt;StandardScaler&lt;/h2&gt;
&lt;p&gt;Many ML algorithm are very sensitive to the scale of the input dataset. These algorithms work best when
all the feature have the same scale. For this reason it is often required to normalize your data.
This scaler normalize  the data so that each numerical feature has unit standard deviation and zero mean.&lt;/p&gt;
&lt;p&gt;The StandardScaler is actually an Estimator, thus it has the method &lt;code&gt;fit&lt;/code&gt; which returns a &lt;code&gt;StandardScalerModel&lt;/code&gt;
object, which is a Transformer. Therefore, by calling &lt;code&gt;transform&lt;/code&gt; on the StandardScaler model you can scale
the input features as desired.&lt;/p&gt;
&lt;p&gt;The following snippet shows how to use this scaler.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;import org.apache.spark.ml.feature.StandardScaler

val dataFrame = spark.read.format(&amp;quot;libsvm&amp;quot;).load(&amp;quot;data/mllib/sample_libsvm_data.txt&amp;quot;)

val scaler = new StandardScaler()
  .setInputCol(&amp;quot;features&amp;quot;)
  .setOutputCol(&amp;quot;scaledFeatures&amp;quot;)
  .setWithStd(true)
  .setWithMean(false)

// Compute summary statistics by fitting the StandardScaler.
val scalerModel = scaler.fit(dataFrame)

// Normalize each feature to have unit standard deviation.
val scaledData = scalerModel.transform(dataFrame)
scaledData.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is not the only option when it comes to scaling your data.
There are other techniques that work the same way as this scaler, the only
thing that change of course is the algorithm used for scale the data.&lt;/p&gt;
&lt;h2 id=&#34;bucketizer&#34;&gt;Bucketizer&lt;/h2&gt;
&lt;p&gt;A Bucketizer transforms a real-valued feature into a column where values are divided into buckets.&lt;/p&gt;
&lt;p&gt;When you use this type of transformer you need to specify the buckets into which you wish to divided your
feature. Therefore you must specify an ordered vector containing the ranges according to which buckets
have to been defined.&lt;/p&gt;
&lt;p&gt;The following snippet shows how to use this transformer.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;import org.apache.spark.ml.feature.Bucketizer

val splits = Array(Double.NegativeInfinity, -0.5, 0.0, 0.5, Double.PositiveInfinity)

val data = Array(-999.9, -0.5, -0.3, 0.0, 0.2, 999.9)
val dataFrame = spark.createDataFrame(data.map(Tuple1.apply)).toDF(&amp;quot;features&amp;quot;)

val bucketizer = new Bucketizer()
  .setInputCol(&amp;quot;features&amp;quot;)
  .setOutputCol(&amp;quot;bucketedFeatures&amp;quot;)
  .setSplits(splits)

// Transform original data into its bucket index.
val bucketedData = bucketizer.transform(dataFrame)

println(s&amp;quot;Bucketizer output with ${bucketizer.getSplits.length-1} buckets&amp;quot;)
bucketedData.show()

val splitsArray = Array(
  Array(Double.NegativeInfinity, -0.5, 0.0, 0.5, Double.PositiveInfinity),
  Array(Double.NegativeInfinity, -0.3, 0.0, 0.3, Double.PositiveInfinity))

val data2 = Array(
  (-999.9, -999.9),
  (-0.5, -0.2),
  (-0.3, -0.1),
  (0.0, 0.0),
  (0.2, 0.4),
  (999.9, 999.9))
val dataFrame2 = spark.createDataFrame(data2).toDF(&amp;quot;features1&amp;quot;, &amp;quot;features2&amp;quot;)

val bucketizer2 = new Bucketizer()
  .setInputCols(Array(&amp;quot;features1&amp;quot;, &amp;quot;features2&amp;quot;))
  .setOutputCols(Array(&amp;quot;bucketedFeatures1&amp;quot;, &amp;quot;bucketedFeatures2&amp;quot;))
  .setSplitsArray(splitsArray)

// Transform original data into its bucket index.
val bucketedData2 = bucketizer2.transform(dataFrame2)

println(s&amp;quot;Bucketizer output with [&amp;quot; +
  s&amp;quot;${bucketizer2.getSplitsArray(0).length-1}, &amp;quot; +
  s&amp;quot;${bucketizer2.getSplitsArray(1).length-1}] buckets for each input column&amp;quot;)
bucketedData2.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;vectorassembler&#34;&gt;VectorAssembler&lt;/h2&gt;
&lt;p&gt;A VectorAssembler is a transformer that allows you to combine several columns
into a single vector column.&lt;/p&gt;
&lt;p&gt;It is able to deal with the following column types: numeric, boolean and vector.
In each row, the vector is obtained by concatenating the value of each column in the
order specified by the user.&lt;/p&gt;
&lt;p&gt;The following snippet shows how to use this transformer&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;import org.apache.spark.ml.feature.VectorAssembler
import org.apache.spark.ml.linalg.Vectors

val dataset = spark.createDataFrame(
    Seq((0, 18, 1.0, Vectors.dense(0.0, 10.0, 0.5), 1.0))
).toDF(&amp;quot;id&amp;quot;, &amp;quot;hour&amp;quot;, &amp;quot;mobile&amp;quot;, &amp;quot;userFeatures&amp;quot;, &amp;quot;clicked&amp;quot;)

val assembler = new VectorAssembler()
    .setInputCols(Array(&amp;quot;hour&amp;quot;, &amp;quot;mobile&amp;quot;, &amp;quot;userFeatures&amp;quot;))
    .setOutputCol(&amp;quot;features&amp;quot;)
  val output = assembler.transform(dataset)
  println(&amp;quot;Assembled columns &#39;hour&#39;, &#39;mobile&#39;, &#39;userFeatures&#39; to vector column &#39;features&#39;&amp;quot;)
  output.select(&amp;quot;features&amp;quot;, &amp;quot;clicked&amp;quot;).show(false)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Starting from the following situation:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;| id | hour | mobile | userFeatures     | clicked |
|:--:|:----:|:------:|:----------------:|:-------:|
| 0  | 18   | 1.0    | [0.0, 10.0, 0.5] | 1.0     |
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The above snippet will tranform the dataset as follows:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;id&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;hour&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;mobile&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;userFeatures&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;clicked&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;features&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;0&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;18&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;1.0&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;[0.0, 10.0, 0.5]&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;1.05&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;[18.0, 1.0, 0.0, 10.0,0.5]&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;imputer&#34;&gt;Imputer&lt;/h2&gt;
&lt;p&gt;The Imputer is an estimator that is useful for replacing missing/null/NaN values accordingly
to a predefined strategy. For instance, you might use an Imputer to replace the missing values
in a feature of the dataset by replacing them with the average value computed with respect
to that feature.
An Imputer can only work with numerical features – it is not able to understand
categorical values. It also provides the ability to configure which value has to be considered
as  &amp;ldquo;missing&amp;rdquo;. For instance, if we set &lt;code&gt;.setMissingValue(0)&lt;/code&gt; then all the occurrences of 0
will be replaced by the Imputer.&lt;/p&gt;
&lt;p&gt;The following snippet shows how to use the Imputer.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;import org.apache.spark.ml.feature.Imputer

val df = spark.createDataFrame(Seq(
  (1.0, Double.NaN),
  (2.0, Double.NaN),
  (Double.NaN, 3.0),
  (4.0, 4.0),
  (5.0, 5.0)
)).toDF(&amp;quot;a&amp;quot;, &amp;quot;b&amp;quot;)

val imputer = new Imputer()
  .setInputCols(Array(&amp;quot;a&amp;quot;, &amp;quot;b&amp;quot;))
  .setOutputCols(Array(&amp;quot;out_a&amp;quot;, &amp;quot;out_b&amp;quot;))

val model = imputer.fit(df)
model.transform(df).show()
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;classification--regression&#34;&gt;Classification &amp;amp; Regression&lt;/h1&gt;
&lt;p&gt;These are two different types of supervised learning problem.&lt;/p&gt;
&lt;p&gt;In any supervised learning problem you are provided with a set of items, each item
has a set of features, and more importantly it is assigned with a label.&lt;/p&gt;
&lt;p&gt;The type of the label tells you if you are dealing with a regression problem rather
than a classification one. More specifically, if the label is a real value then you have
a regression problem, while if it is a categorical value then you are dealing with
a classification problem.&lt;/p&gt;
&lt;p&gt;MLlib offers a lot of algorithms for addressing these kinds of supervised problems.
This is the list of algorithms provided by the library:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Classification
&lt;ul&gt;
&lt;li&gt;Logistic regression
&lt;ul&gt;
&lt;li&gt;Binomial logistic regression&lt;/li&gt;
&lt;li&gt;Multinomial logistic regression&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Decision tree classifier&lt;/li&gt;
&lt;li&gt;Random forest classifier&lt;/li&gt;
&lt;li&gt;Gradient-boosted tree classifier&lt;/li&gt;
&lt;li&gt;Multilayer perceptron classifier&lt;/li&gt;
&lt;li&gt;Linear Support Vector Machine&lt;/li&gt;
&lt;li&gt;One-vs-Rest classifier (a.k.a. One-vs-All)&lt;/li&gt;
&lt;li&gt;Naive Bayes&lt;/li&gt;
&lt;li&gt;Factorization machines classifier&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Regression
&lt;ul&gt;
&lt;li&gt;Linear regression&lt;/li&gt;
&lt;li&gt;Generalized linear regression
&lt;ul&gt;
&lt;li&gt;Available families&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Decision tree regression&lt;/li&gt;
&lt;li&gt;Random forest regression&lt;/li&gt;
&lt;li&gt;Gradient-boosted tree regression&lt;/li&gt;
&lt;li&gt;Survival regression&lt;/li&gt;
&lt;li&gt;Isotonic regression&lt;/li&gt;
&lt;li&gt;Factorization machines regressor&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Linear methods&lt;/li&gt;
&lt;li&gt;Factorization Machines&lt;/li&gt;
&lt;li&gt;Decision trees
&lt;ul&gt;
&lt;li&gt;Inputs and Outputs
&lt;ul&gt;
&lt;li&gt;Input Columns&lt;/li&gt;
&lt;li&gt;Output Columns&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Tree Ensembles
&lt;ul&gt;
&lt;li&gt;Random Forests
&lt;ul&gt;
&lt;li&gt;Inputs and Outputs
&lt;ul&gt;
&lt;li&gt;Input Columns&lt;/li&gt;
&lt;li&gt;Output Columns (Predictions)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Gradient-Boosted Trees (GBTs)
&lt;ul&gt;
&lt;li&gt;Inputs and Outputs
&lt;ul&gt;
&lt;li&gt;Input Columns&lt;/li&gt;
&lt;li&gt;Output Columns (Predictions)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We will not discuss any of the previous algorithms. Since this is beyond the scope
of this lesson. A useful resource to learn (quickly) is the scikit-learn documentation,
available &lt;a href=&#34;https://scikit-learn.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h1 id=&#34;model-selection-and-tuning&#34;&gt;Model Selection and Tuning&lt;/h1&gt;
&lt;p&gt;This section describes how to use MLlib’s tooling
for tuning ML algorithms and Pipelines. Built-in Cross-Validation
and other tooling allow users to
optimize hyperparameters in algorithms and Pipelines.&lt;/p&gt;
&lt;h2 id=&#34;parameter-tuning&#34;&gt;Parameter Tuning&lt;/h2&gt;
&lt;p&gt;Parameter Tuning is the task of trying different configuration in order to
improve performance.&lt;/p&gt;
&lt;p&gt;You can tune a single estimator as well as an entire pipeline.
When you want to improve the performance of your model you need an object
of type Evaluator. An Evaluator allows you to evaluate how well your model is able to
fit the data. There is a number of different evaluators, the right one depends on the
type of problem you are dealing with. For instance if you are dealing with a regression problem
you should use the RegressionEvaluator, while for classification problems you should use
the BinaryClassificationEvaluator or the MulticlassificationEvaluator.&lt;/p&gt;
&lt;p&gt;A common way tune an ML model is by defining a grid, which determines the range of values
a particular parameter can assume. This kind of behavior is achieved via the
ParamGridBuilder object. A grid based tuning works as follows. Imagine
your model M has depends on two different hyperparameters: p&lt;sub&gt;1&lt;/sub&gt; and p&lt;sub&gt;2&lt;/sub&gt;. You define a
range of values for each one of them. For instance you may specify that p&lt;sub&gt;1&lt;/sub&gt; varies within
the range [0,10] with step 2, while p&lt;sub&gt;2&lt;/sub&gt; varies within the range [-10, 10] with step.
This means that you would have 5 different possible configuration for p&lt;sub&gt;1&lt;/sub&gt; and 3 different
configuration for p&lt;sub&gt;2&lt;/sub&gt;. In this case your model will be trained 5x3=15 different times, assessing
every possible configuration with respect to both parameters.&lt;/p&gt;
&lt;h2 id=&#34;cross-validation&#34;&gt;Cross Validation&lt;/h2&gt;
&lt;p&gt;Another important technique for selecting the best model is the Cross-Validation.
In order to train your model with the cross-validation there is an object
called &lt;code&gt;CrossValidator&lt;/code&gt;.
A &lt;code&gt;CrossValidator&lt;/code&gt; is actually an Estimator, so you can call &lt;code&gt;fit&lt;/code&gt; and then you get
a model.
The following snippet shows how to use the a CrossValidator.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;...
val tokenizer = new Tokenizer()
  .setInputCol(&amp;quot;text&amp;quot;)
  .setOutputCol(&amp;quot;words&amp;quot;)
val hashingTF = new HashingTF()
  .setInputCol(tokenizer.getOutputCol)
  .setOutputCol(&amp;quot;features&amp;quot;)
val lr = new LogisticRegression()
  .setMaxIter(10)
val pipeline = new Pipeline()
  .setStages(Array(tokenizer, hashingTF, lr))

// We use a ParamGridBuilder to construct a grid of parameters to search over.
// With 3 values for hashingTF.numFeatures and 2 values for lr.regParam,
// this grid will have 3 x 2 = 6 parameter settings for CrossValidator to choose from.
val paramGrid = new ParamGridBuilder()
  .addGrid(hashingTF.numFeatures, Array(10, 100, 1000))
  .addGrid(lr.regParam, Array(0.1, 0.01))
  .build()

val cv = new CrossValidator()
  .setEstimator(pipeline)
  .setEvaluator(new BinaryClassificationEvaluator)
  .setEstimatorParamMaps(paramGrid)
  .setNumFolds(2)  // Use 3+ in practice
  .setParallelism(2)  // Evaluate up to 2 parameter settings in parallel

val model = cv.fit(data)
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;anatomy-of-a-ml-application&#34;&gt;Anatomy of a ML application&lt;/h1&gt;
&lt;p&gt;Any ML application should adopt the following development steps:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Think about the problem you are interested in. You should ask yourself the following
questions:
&lt;ul&gt;
&lt;li&gt;How would I frame it?&lt;/li&gt;
&lt;li&gt;Is it a supervised problem? Is it better to address the problem as a
classification problem rather then a regression one?&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Get the data. In this stage you get the data required to solve the problem you
are interested in.&lt;/li&gt;
&lt;li&gt;Analyze your data, compute summary statistics, plot the data, gain insights.&lt;/li&gt;
&lt;li&gt;Prepare your data to apply ML algorithms. In this stage you apply transformer to clean
your data – for instance you can apply the Imputer to replace missing value –, or
you can apply transformer to obtain a different representation of your data – for
instance Word2Vec or OneHotEncoding.&lt;/li&gt;
&lt;li&gt;Split your dataset. You &lt;strong&gt;&lt;strong&gt;must&lt;/strong&gt;&lt;/strong&gt; always remember to reserve a fraction of your data
to validate your model. It means that you must split the data (at least) into training and test
set. The training set is used to train your model, while the test is used to understand how godd
is your model in terms of generalization&lt;/li&gt;
&lt;li&gt;Select the most promising models and apply parameter tuning on them&lt;/li&gt;
&lt;li&gt;Present the results&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Spark (More) Advanced Examples</title>
      <link>/courses/bdanalytics/advancedspark/</link>
      <pubDate>Sun, 05 May 2019 00:00:00 +0100</pubDate>
      <guid>/courses/bdanalytics/advancedspark/</guid>
      <description>&lt;p&gt;What you will learn:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;How to use Spark to compute aggregate statistics on a distributed dataset&lt;/li&gt;
&lt;li&gt;How to do collaborative filtering in spark&lt;/li&gt;
&lt;li&gt;How to represent a graph data structure &amp;ndash; without third party libraries &amp;ndash; and to execute a BFS algorithm&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The material for this lesson is available  &lt;a href=&#34;https://github.com/acalio/BDAnalytics/blob/main/03-Spark-advanced-examples/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Structured Streaming</title>
      <link>/courses/bdanalytics/sparkstreaming/</link>
      <pubDate>Sun, 05 May 2019 00:00:00 +0100</pubDate>
      <guid>/courses/bdanalytics/sparkstreaming/</guid>
      <description>&lt;p&gt;Structured Streaming is a very efficient stream processing engine built on top of the Spark SQL.&lt;/p&gt;
&lt;p&gt;There is no significance difference in the way you define your computations between an input stream and
a batch of static data.&lt;/p&gt;
&lt;p&gt;The SQL engine takes care of   running your queries or transformations incrementally, updating the results
as the streaming data continue to arrive. It should be also noted that Spark process each data &lt;em&gt;exactly once&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Spark Streaming can work in two different modes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;micro-batch&lt;/em&gt;, it the one used by default. The streams of data is processed as a series of small batches of data.
The latency in this case can decrease as low as 100 milliseconds.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;continuous processing&lt;/em&gt;, the latency is decreased further, so that it can be achieved a 1 millisecond latency.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The code for this lesson is available &lt;a href=&#34;https://github.com/acalio/BDAnalytics/tree/main/07-Streaming&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;warming-up&#34;&gt;Warming Up&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s a very simple example. We want to count the number of words from a text stream coming  from
a tcp connection. More specifically, we want to compute the number of occurrences for each different word
read from the stream.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;import org.apache.spark.sql.functions._
import org.apache.spark.sql.SparkSession

val spark = SparkSession
  .builder
  .appName(&amp;quot;StructuredNetworkWordCount&amp;quot;)
  .getOrCreate()

import spark.implicits._

// Create DataFrame representing the stream of input lines from connection to localhost:9999
val lines = spark.readStream
  .format(&amp;quot;socket&amp;quot;)
  .option(&amp;quot;host&amp;quot;, &amp;quot;localhost&amp;quot;)
  .option(&amp;quot;port&amp;quot;, 9999)
  .load()

// Split the lines into words
val words = lines.as[String].flatMap(_.split(&amp;quot; &amp;quot;))

// Generate running word count
val wordCounts = words.groupBy(&amp;quot;value&amp;quot;).count()

//run the above query and print the result to theh console
val query = wordCounts.writeStream
  .outputMode(&amp;quot;complete&amp;quot;)
  .format(&amp;quot;console&amp;quot;)
  .start()

query.awaitTermination()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the above code we first create a &lt;code&gt;SparSession&lt;/code&gt; which is responsible for the initialization of both the
SQL engine and the stream processing engine.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;lines&lt;/code&gt; &lt;code&gt;DataFrame&lt;/code&gt; represents the unbounded table containing the streaming data – the unbounded table is
the table that receive the input data from the stream, it is constantly updated – which has only one
string-valued column named value. Each line of the streaming text becomes a new entry.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;lines&lt;/code&gt; variable is then converted into a &lt;code&gt;DataSet&lt;/code&gt; of &lt;code&gt;String&lt;/code&gt; values by splitting each line.
After this operation you will have a new &lt;code&gt;DataSet&lt;/code&gt; where each entry contains exactly a word.&lt;/p&gt;
&lt;p&gt;In order to count the number of occurrence for each word, we need to first aggregate by the column
&lt;code&gt;value&lt;/code&gt;  and then count.&lt;/p&gt;
&lt;p&gt;So far we have just defined the operations, they are not actually run by the engine. In order to start
the entire processing we need to write the stream – in this case we print on the system console – and then
call &lt;code&gt;start&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Usually, when your run the query, you use &lt;code&gt;awaitTermination&lt;/code&gt; so that process will continue to listen for
new data until you ask it to stop.&lt;/p&gt;
&lt;h1 id=&#34;programming-model&#34;&gt;Programming Model&lt;/h1&gt;
&lt;p&gt;The main abstraction used in structured streaming is the Unbounded Table.
It is a table that  continuously grows, as new are fed into the stream and eventually into
the table – new records are appended at the end of the table.&lt;/p&gt;
&lt;p&gt;Figure &lt;a id=&#34;orgb3c2ebd&#34;&gt;&lt;/a&gt;  shows a high level representation of an unbounded img.








  











&lt;figure id=&#34;figure-data--stream&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;/media/img/utable.png&#34; data-caption=&#34;Data  Stream&#34;&gt;


  &lt;img src=&#34;/media/img/utable.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Data  Stream
  &lt;/figcaption&gt;


&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;In the previous example, the unbounded table was the &lt;code&gt;lines&lt;/code&gt; &lt;code&gt;DataFrame&lt;/code&gt;.
When a you run a query on the input, you generate a result table. This result table is updated
every trigger interval – for instance 1 sec –  in order to account for newly generated data.&lt;/p&gt;
&lt;p&gt;The following figure shows how the result table is updated as time goes and new data is available from the
stream.








  











&lt;figure id=&#34;figure-programming-model&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;/media/img/pmodel.png&#34; data-caption=&#34;Programming Model&#34;&gt;


  &lt;img src=&#34;/media/img/pmodel.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Programming Model
  &lt;/figcaption&gt;


&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;The bottom layer of the above image represents how Spark shows the results related to the query.
You have three different options:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;complete mode&lt;/em&gt;, the entire Result Table will be written to the external storage&lt;/li&gt;
&lt;li&gt;&lt;em&gt;append mode&lt;/em&gt;, only the new rows appended in the Result Table since the last trigger will be written
to the external storage.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;update mode&lt;/em&gt;, only the rows in the Result Table that are updated since the last trigger will be written to the
external storage.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Let&amp;rsquo;s see how this model applies to the above example.








  











&lt;figure id=&#34;figure-model-of-the-example&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;/media/img/Screenshot%20from%202020-12-21%2016-30-09.png&#34; data-caption=&#34;Model of the example&#34;&gt;


  &lt;img src=&#34;/media/img/Screenshot%20from%202020-12-21%2016-30-09.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Model of the example
  &lt;/figcaption&gt;


&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;lines&lt;/code&gt; DataFrame is the input table while the &lt;code&gt;wordCounts&lt;/code&gt; is the result table.&lt;br&gt;
Any time new data comes in, the counter all the counters are updated.&lt;/p&gt;
&lt;p&gt;One main advantage of Spark Streaming as opposed to other solutions for stream processing is that
it keeps the minimum amount of data in order to run the query.&lt;/p&gt;
&lt;p&gt;In this case, it means that it does not actually materialize the entire input table, but
it keeps the minimal amount of &lt;em&gt;intermediate&lt;/em&gt; data needed to update the result.&lt;/p&gt;
&lt;p&gt;This model prevents the programmer from having to reason about fault-tolerance and data consistency, as
they are automatically managed by the Spark engine.&lt;/p&gt;
&lt;h2 id=&#34;how-to-use-datasets-and-dataframes&#34;&gt;How to use Datasets and DataFrames&lt;/h2&gt;
&lt;p&gt;You can create streaming Datasets or DataFrames from a variety of different sources.
For instance you can have:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;File Source. Supported formats are: CSV, JSON, text, ORC and Parquet files&lt;/li&gt;
&lt;li&gt;Kafka Source. For integrating Spark with Kafka&lt;/li&gt;
&lt;li&gt;Socket Source. For reading text from a socket connection&lt;/li&gt;
&lt;li&gt;Rate Source. For generating data at the specified number of rows per second. This source is used only for
testing and benchmarking&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In order to use any of the above source you must the &lt;code&gt;readStream&lt;/code&gt; method on the Spark session and then
specify a set of source-specific options, which are summarized in the following (not exhaustive) table:&lt;/p&gt;
&lt;table border=&#34;2&#34; cellspacing=&#34;0&#34; cellpadding=&#34;6&#34; rules=&#34;groups&#34; frame=&#34;hsides&#34;&gt;
&lt;colgroup&gt;
&lt;col  class=&#34;org-left&#34; /&gt;
&lt;col  class=&#34;org-left&#34; /&gt;
&lt;col  class=&#34;org-left&#34; /&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th scope=&#34;col&#34; class=&#34;org-left&#34;&gt;Source&lt;/th&gt;
&lt;th scope=&#34;col&#34; class=&#34;org-left&#34;&gt;Option&lt;/th&gt;
&lt;th scope=&#34;col&#34; class=&#34;org-left&#34;&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td class=&#34;org-left&#34;&gt;File&lt;/td&gt;
&lt;td class=&#34;org-left&#34;&gt;`path`&lt;/td&gt;
&lt;td class=&#34;org-left&#34;&gt;Path to the input directory&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;org-left&#34;&gt;&amp;#xa0;&lt;/td&gt;
&lt;td class=&#34;org-left&#34;&gt;`maxFilesPerTrigger`&lt;/td&gt;
&lt;td class=&#34;org-left&#34;&gt;maximum number of new files to be considered in every trigger (by default no max is set)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;org-left&#34;&gt;&amp;#xa0;&lt;/td&gt;
&lt;td class=&#34;org-left&#34;&gt;`latestFirst`&lt;/td&gt;
&lt;td class=&#34;org-left&#34;&gt;True if the latest files have to be processed first&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;org-left&#34;&gt;&amp;#xa0;&lt;/td&gt;
&lt;td class=&#34;org-left&#34;&gt;`fileNameOnly`&lt;/td&gt;
&lt;td class=&#34;org-left&#34;&gt;True if you want to ignore file extension&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;org-left&#34;&gt;&amp;#xa0;&lt;/td&gt;
&lt;td class=&#34;org-left&#34;&gt;`maxFileAge`&lt;/td&gt;
&lt;td class=&#34;org-left&#34;&gt;Maximum age of a file before it is ignored&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td class=&#34;org-left&#34;&gt;Socket&lt;/td&gt;
&lt;td class=&#34;org-left&#34;&gt;`host`&lt;/td&gt;
&lt;td class=&#34;org-left&#34;&gt;the host to connect to&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;org-left&#34;&gt;&amp;#xa0;&lt;/td&gt;
&lt;td class=&#34;org-left&#34;&gt;`port`&lt;/td&gt;
&lt;td class=&#34;org-left&#34;&gt;the port to connect to&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td class=&#34;org-left&#34;&gt;Rate&lt;/td&gt;
&lt;td class=&#34;org-left&#34;&gt;`rowsPersecond`&lt;/td&gt;
&lt;td class=&#34;org-left&#34;&gt;how many rows should be generate every second&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;org-left&#34;&gt;&amp;#xa0;&lt;/td&gt;
&lt;td class=&#34;org-left&#34;&gt;`numPartitions`&lt;/td&gt;
&lt;td class=&#34;org-left&#34;&gt;the partition number for the generated rows&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td class=&#34;org-left&#34;&gt;Kafka&lt;/td&gt;
&lt;td class=&#34;org-left&#34;&gt;`subscribe`&lt;/td&gt;
&lt;td class=&#34;org-left&#34;&gt;Topic to subscribe to&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;org-left&#34;&gt;&amp;#xa0;&lt;/td&gt;
&lt;td class=&#34;org-left&#34;&gt;`kafka.bootstrap.servers`&lt;/td&gt;
&lt;td class=&#34;org-left&#34;&gt;URL of a Kafka broker (you can specify more than one address)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Here are some examples:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;val spark: SparkSession = ...

// Read text from socket
val socketDF = spark
  .readStream
  .format(&amp;quot;socket&amp;quot;)
  .option(&amp;quot;host&amp;quot;, &amp;quot;localhost&amp;quot;)
  .option(&amp;quot;port&amp;quot;, 9999)
  .load()

socketDF.isStreaming    // Returns True for DataFrames that have streaming sources

socketDF.printSchema

// Read all the csv files written atomically in a directory
val userSchema = new StructType().add(&amp;quot;name&amp;quot;, &amp;quot;string&amp;quot;).add(&amp;quot;age&amp;quot;, &amp;quot;integer&amp;quot;)
val csvDF = spark
  .readStream
  .option(&amp;quot;sep&amp;quot;, &amp;quot;;&amp;quot;)
  .schema(userSchema)      // Specify schema of the csv files
  .csv(&amp;quot;/path/to/directory&amp;quot;)    // Equivalent to format(&amp;quot;csv&amp;quot;).load(&amp;quot;/path/to/directory&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It should be noted that in this examples Spark is not able to infer the schema at compile time.
However, some operations like &lt;code&gt;map&lt;/code&gt; or &lt;code&gt;flatMap&lt;/code&gt; need to work with a compile time known type. In these case
you must first convert the &lt;code&gt;DataFrame&lt;/code&gt;  into the corresponding, typed, &lt;code&gt;Datasets&lt;/code&gt; and then run the
map operations.&lt;/p&gt;
&lt;p&gt;Also, interestingly, when you read data from a source file, Spark forces you to explicitly define the schema of your data.
In this way, avoiding runtime inference, Spark ensures consistency between the input and the output data.&lt;/p&gt;
&lt;h1 id=&#34;stream-operations&#34;&gt;Stream Operations&lt;/h1&gt;
&lt;h2 id=&#34;basic-operations&#34;&gt;Basic Operations&lt;/h2&gt;
&lt;p&gt;Streaming datasets/dataframe are no different from the their batch counterparts. It means that they support any
operation that is also supported by a regular DataFrame/Dataset, &lt;strong&gt;&lt;strong&gt;although there are some exceptions&lt;/strong&gt;&lt;/strong&gt; which
are listed below:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Multiple streaming aggregation – you cannot define chain of aggregations&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;You cannot take the first N rows&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Distinct operations are not supported&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Sorting operations are only supported after you apply some aggregation on the input dataset, and only if
you use &lt;em&gt;complete&lt;/em&gt; output mode&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Some types of outer joins are not supported&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Example&lt;/p&gt;
&lt;p&gt;Here is an  example on how to execute queries on a streaming dataframe.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;case class DeviceData(device: String, deviceType: String, signal: Double, time: DateTime)
    
val df: DataFrame = ... // streaming DataFrame with IOT device data with schema { device: string, deviceType: string, signal: double, time: string }
val ds: Dataset[DeviceData] = df.as[DeviceData]    // streaming Dataset with IOT device data
    
// Select the devices which have signal more than 10
df.select(&amp;quot;device&amp;quot;).where(&amp;quot;signal &amp;gt; 10&amp;quot;)      // using untyped APIs   
ds.filter(_.signal &amp;gt; 10).map(_.device)         // using typed APIs
    
// Running count of the number of updates for each device type
df.groupBy(&amp;quot;deviceType&amp;quot;).count()                          // using untyped API
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;window-operations-on-event-time&#34;&gt;Window Operations on Event Time&lt;/h2&gt;
&lt;p&gt;One interesting feature provided by Spark streaming is the ability to aggregate data based on event-time windows.&lt;/p&gt;
&lt;p&gt;For instance, let&amp;rsquo;s change the first example a little bit. More specifically, here we want to count the number
of occurrences for each word and for each specific time-frame. Therefore, you can imagine there is a sliding
time window, which groups the input records accordingly to the timestamp the have been generated.
You can think ad the log data coming from a web servers, each log is associated with a timestamp and it
has a text data associated with it. Therefore each entry has the following schema:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;{timestamp: Timestamp, word: String} 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The following image shows the result table if we set a time-window that has  a 10 minutes width and it moves
every 5 minutes:








  











&lt;figure id=&#34;figure-time-window-aggregation&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;/media/img/window.png&#34; data-caption=&#34;Time-window aggregation&#34;&gt;


  &lt;img src=&#34;/media/img/window.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Time-window aggregation
  &lt;/figcaption&gt;


&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;To achieve the above result with must modify the above example as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;val windowedCounts = words.groupBy(
  window($&amp;quot;timestamp&amp;quot;, &amp;quot;10 minutes&amp;quot;, &amp;quot;5 minutes&amp;quot;),
  $&amp;quot;word&amp;quot;
).count()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The above example means that we want to aggregate data accordingly to a 10 minutes time window which moves
forward, i.e., it is updated, every 5 minutes.&lt;/p&gt;
&lt;p&gt;Aggregating by a time window increases the complexity of the entire picture. In fact, it might happen that
the time a record is generated is significantly different from the time the same data is received by Spark.&lt;/p&gt;
&lt;p&gt;Here is an example:








  











&lt;figure id=&#34;figure-late-data&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;/media/img/late.png&#34; data-caption=&#34;Late data&#34;&gt;


  &lt;img src=&#34;/media/img/late.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Late data
  &lt;/figcaption&gt;


&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Imagine you have to run the above query for days? In this case, if you allow the aggregation function to account
for data received with a, let&amp;rsquo;s say, two days of delay, you will increase the footprint of your application
in a very dramatic way.
Therefore, we need a mechanism for letting Spark to understand when the intermediate result of a time window
can be dropped, therefore any other data referring to that window can be ignored.&lt;/p&gt;
&lt;p&gt;For instance, in the above example, we want to ignore the update due to the arrival of the entry
&lt;code&gt;\{12:04, dog\}&lt;/code&gt; received two times windows after the one it refers to.&lt;/p&gt;
&lt;p&gt;We can implement this mechanism with &lt;em&gt;watermarking&lt;/em&gt;. With this behavior we can set a threshold to the
maximum delay of any record. For instance if we want maximum 10 minutes delay for each time-window we can use
the following snippet.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;import spark.implicits._
val words = ... // streaming DataFrame of schema { timestamp: Timestamp, word: String }
// Group the data by window and word and compute the count of each group
val windowedCounts = words
    .withWatermark(&amp;quot;timestamp&amp;quot;, &amp;quot;10 minutes&amp;quot;)
    .groupBy(
    window($&amp;quot;timestamp&amp;quot;, &amp;quot;10 minutes&amp;quot;, &amp;quot;5 minutes&amp;quot;),
    $&amp;quot;word&amp;quot;)
    .count()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Having a 10 minutes watermark means that any time windows cannot account for data arrived with more than
a 10 minutes delay.  More formally, for a specific window ending at time T, the engine will maintain the
intermediate state and allow late data to update the state until
&lt;code&gt;max event time seen - time threshold &amp;lt; T&lt;/code&gt;








  











&lt;figure id=&#34;figure-watermarking-with-update-mode&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;/media/img/water.png&#34; data-caption=&#34;Watermarking with update mode&#34;&gt;


  &lt;img src=&#34;/media/img/water.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Watermarking with update mode
  &lt;/figcaption&gt;


&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;We have the following time windows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;w1 -&amp;gt; 12:00-12:10&lt;/li&gt;
&lt;li&gt;w2 -&amp;gt; 12:05-12:15&lt;/li&gt;
&lt;li&gt;w3 -&amp;gt; 12:10-12:20&lt;/li&gt;
&lt;li&gt;w4 -&amp;gt; 12:15-12:25&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Also, we have the following triggers:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;t1 -&amp;gt; 12:05&lt;/li&gt;
&lt;li&gt;t2 -&amp;gt; 12:10&lt;/li&gt;
&lt;li&gt;t3 -&amp;gt; 12:15&lt;/li&gt;
&lt;li&gt;t4 -&amp;gt; 12:20&lt;/li&gt;
&lt;li&gt;t5 -&amp;gt; 12:25&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The watermark is updated at every trigger.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;t1 -&amp;gt; no data is received, the watermark is not set&lt;/li&gt;
&lt;li&gt;t2 -&amp;gt; the max event time is 12:08. The watermark is not set as 12:08 - 10 is lower than the starting time&lt;/li&gt;
&lt;li&gt;t3 -&amp;gt; the max event time is 12:14. The watermark is set to 12:04. It means that any data with event time
after 12:04 should be taken into account in order to update the time window w1 and w2.&lt;/li&gt;
&lt;li&gt;t4 -&amp;gt; before the trigger is reached, two watermark updates happen.
&lt;ul&gt;
&lt;li&gt;The first   update happens when &lt;code&gt;{12:15, cat}&lt;/code&gt; is received. The watermark is set to 12:05. It means that
the watermark is set to 12:05, therefore the &lt;code&gt;{12:08, dog}&lt;/code&gt;  will be taken into account to update
w1 and  &lt;code&gt;{12:13, own}&lt;/code&gt; is taken into account to update w2.&lt;/li&gt;
&lt;li&gt;The second watermark happens when  &lt;code&gt;{12:21, owl}&lt;/code&gt; is received. The watermark thus becomes 12:11.
It means that any data with event time previous to this value will be ignored.
For this reason, &lt;code&gt;{12:04, donkey}&lt;/code&gt; is ignored while &lt;code&gt;{12:17, monkey}&lt;/code&gt; is considered to update
the time window w4&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In the above example, the update output model has been enabled. It means that the result table is actually
update after each trigger.&lt;/p&gt;
&lt;p&gt;If we enable…. all the intermediate steps are kept in memory and only the final result is written to the update table.








  











&lt;figure id=&#34;figure-watermarking-with-append-mode&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;/media/img/lateappend.png&#34; data-caption=&#34;Watermarking with append mode&#34;&gt;


  &lt;img src=&#34;/media/img/lateappend.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Watermarking with append mode
  &lt;/figcaption&gt;


&lt;/figure&gt;
&lt;/p&gt;
&lt;h1 id=&#34;running-queries&#34;&gt;Running Queries&lt;/h1&gt;
&lt;p&gt;Once you have defined the final result DataFrame/Dataset all that is left is for you
is to start the steaming  computation. In order to do it, you need to obtain
a &lt;code&gt;DataSteamWriter&lt;/code&gt; by calling &lt;code&gt;writeStream&lt;/code&gt; on the queried dataset.
When you use a &lt;code&gt;DataSteamWriter&lt;/code&gt; you need to specify the following information:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;details of the output sink&lt;/em&gt;, Data format, location, etc.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;output mode&lt;/em&gt;, specify what gets written to the output sink&lt;/li&gt;
&lt;li&gt;&lt;em&gt;query name&lt;/em&gt;, optionally, specify a unique name to the query&lt;/li&gt;
&lt;li&gt;&lt;em&gt;trigger interval&lt;/em&gt;. optionally, specify the trigger interval. It it is not specified
the system will check for availability of new data as soon as the previous batch has been processed.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;checkpoint location&lt;/em&gt;, for some output sinks, in order to obtain the fault tolerance you
need to specify where to store the information for a potential recovery.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;output-sinks&#34;&gt;Output Sinks&lt;/h2&gt;
&lt;p&gt;Spark steaming provides a variety of built-in output sinks.
Here are the most common ones.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;strong&gt;File Sink&lt;/strong&gt;&lt;/strong&gt; - Supported Modes: Append&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;writeStream
.format(&amp;quot;parquet&amp;quot;)        // can be &amp;quot;orc&amp;quot;, &amp;quot;json&amp;quot;, &amp;quot;csv&amp;quot;, etc.
.option(&amp;quot;path&amp;quot;, &amp;quot;path/to/destination/dir&amp;quot;)
.start()
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;strong&gt;Kafka Sink&lt;/strong&gt;&lt;/strong&gt; - Supported Modes: Append, Update, Complete&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;writeStream
.format(&amp;quot;kafka&amp;quot;)
.option(&amp;quot;kafka.bootstrap.servers&amp;quot;, &amp;quot;host1:port1,host2:port2&amp;quot;)
.option(&amp;quot;topic&amp;quot;, &amp;quot;updates&amp;quot;)
.start()
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;strong&gt;Foreach Sink&lt;/strong&gt;&lt;/strong&gt; - Supported Modes: Append, Update, Complete&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;writeStream
.foreach(...)
.start()
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;strong&gt;Console Sink&lt;/strong&gt;&lt;/strong&gt; (debugging) - Supported Modes: Append, Update, Complete&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;writeStream
.format(&amp;quot;console&amp;quot;)
.start()
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;strong&gt;Memory sink&lt;/strong&gt;&lt;/strong&gt; (debugging) Supported Modes: Append, Complete&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;writeStream
.format(&amp;quot;memory&amp;quot;)
.queryName(&amp;quot;tableName&amp;quot;)
.start()
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>The Spark Ecosystem</title>
      <link>/courses/bdanalytics/sparkeco/</link>
      <pubDate>Sun, 05 May 2019 00:00:00 +0100</pubDate>
      <guid>/courses/bdanalytics/sparkeco/</guid>
      <description>&lt;p&gt;This lesson provides a quick introduction to the Scala ecosystem.&lt;/p&gt;
&lt;p&gt;What you will learn:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The key Spark components&lt;/li&gt;
&lt;li&gt;What is an RDD&lt;/li&gt;
&lt;li&gt;How to handle RDDs in Spark&lt;/li&gt;
&lt;li&gt;How to write some basic Spark driver programs with Scala&lt;/li&gt;
&lt;li&gt;How to dockerize your environment and your appllication&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The material for this lesson is available  &lt;a href=&#34;https://github.com/acalio/BDAnalytics/blob/main/02-Spark-Ecosystem/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Working with Graphs</title>
      <link>/courses/bdanalytics/sparkgraphx/</link>
      <pubDate>Sun, 05 May 2019 00:00:00 +0100</pubDate>
      <guid>/courses/bdanalytics/sparkgraphx/</guid>
      <description>&lt;p&gt;GraphX is a new component in Spark
for graphs and graph-parallel computation.&lt;/p&gt;
&lt;p&gt;At a high level, GraphX extends the Spark RDD by
introducing a new Graph abstraction: a directed multigraph
with properties attached to each vertex and edge.&lt;/p&gt;
&lt;p&gt;To support graph computation, GraphX exposes a set of fundamental operators
(e.g., subgraph, joinVertices, and aggregateMessages) as well
as an optimized variant of the Pregel API.
In addition, GraphX includes a growing collection of graph algorithms
and builders to simplify graph analytics tasks.&lt;/p&gt;
&lt;p&gt;In order to use this library in your driver program you need to add the following imports:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;import org.apache.spark._
import org.apache.spark.graphx._
import org.apache.spark.rdd.RDD
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The code for this lesson is available &lt;a href=&#34;https://github.com/acalio/BDAnalytics/tree/main/06-Spark-GraphX&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h1 id=&#34;the-property-graph&#34;&gt;The Property Graph&lt;/h1&gt;
&lt;p&gt;It is a directed multigraph with user defined objects attached to each vertex and edge.&lt;/p&gt;
&lt;p&gt;A directed multigraph is simply a directed graph that can have multiple edges connecting the same
pair of vertices. This ability enable the possibility to represent multiple relationships between
the nodes in the graph.&lt;/p&gt;
&lt;p&gt;Each vertex is associated with a unique 64-bit long key identifier, aka &lt;code&gt;VertexId&lt;/code&gt;.
Similarly edges have source and destination identifiers.&lt;/p&gt;
&lt;p&gt;The property graph is parameterized over the vertex and edge types, denoted by VD and ED, respectively.
VD and ED are therefore the types associated with objects stored in the graph.&lt;/p&gt;
&lt;p&gt;You can also exploit inheritance to have specialized nodes within the graph as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;class VertexProperty()
case class UserProperty(val name: String) extends VertexProperty
case class ProductProperty(val name: String, val price: Double) extends VertexProperty
// The graph might then have the type:
var graph: Graph[VertexProperty, String] = null
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In this case the &lt;code&gt;Graph&lt;/code&gt; can store two different types of objects.&lt;/p&gt;
&lt;p&gt;Graphs inherit all the good things from the RDD.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Graphs are immutable, therefore any change to the values contained in a graph instance produce a new instance –
graphs cannot be changed in place!&lt;/li&gt;
&lt;li&gt;Graphs are distributed. The Graph is partitioned along the worker nodes using a range of heuristic methods.&lt;/li&gt;
&lt;li&gt;Graphs are fault tolerant. Therefore, in case of failure of a worker node, the partition can be easily recreated&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;the-property-graph-under-the-hood&#34;&gt;The property Graph under the hood&lt;/h2&gt;
&lt;p&gt;A property Graph is simply a compound type having two typed collections (RDDs).&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;class Graph[VD, ED] {
  val vertices: VertexRDD[VD]
  val edges: EdgesRDD[ED]
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;VertexRDD&lt;/code&gt; and &lt;code&gt;EdgesRDD&lt;/code&gt; are two specialized and optimized versions of &lt;code&gt;RDD[(VertexId, VD)]&lt;/code&gt; and
&lt;code&gt;RDD[Edge[ED]]&lt;/code&gt;, respectively. As opposed to a classic RDD, both VertexRDD and EdgesRDD provide specialized
operations tailored for Graph computation.&lt;/p&gt;
&lt;h2 id=&#34;example-how-to-use-the-property-graph&#34;&gt;Example: How to use the Property Graph&lt;/h2&gt;
&lt;p&gt;Suppose you want to construct the following graph.








  











&lt;figure id=&#34;figure-property-graph&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;/media/img/propGraph.png&#34; data-caption=&#34;Property Graph&#34;&gt;


  &lt;img src=&#34;/media/img/propGraph.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Property Graph
  &lt;/figcaption&gt;


&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;It has the following signature:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;val userGraph: Graph[(String, String), String]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can construct a Graph object in multiple ways.
The following method is probably the most general and useful.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;// Assume the SparkContext has already been constructed
val sc: SparkContext
// Create an RDD for the vertices
val users: RDD[(VertexId, (String, String))] =
  sc.parallelize(Seq((3L, (&amp;quot;rxin&amp;quot;, &amp;quot;student&amp;quot;)), (7L, (&amp;quot;jgonzal&amp;quot;, &amp;quot;postdoc&amp;quot;)),
               (5L, (&amp;quot;franklin&amp;quot;, &amp;quot;prof&amp;quot;)), (2L, (&amp;quot;istoica&amp;quot;, &amp;quot;prof&amp;quot;))))
// Create an RDD for edges
val relationships: RDD[Edge[String]] =
  sc.parallelize(Seq(Edge(3L, 7L, &amp;quot;collab&amp;quot;),    Edge(5L, 3L, &amp;quot;advisor&amp;quot;),
               Edge(2L, 5L, &amp;quot;colleague&amp;quot;), Edge(5L, 7L, &amp;quot;pi&amp;quot;)))
// Define a default user in case there are relationship with missing user
val defaultUser = (&amp;quot;John Doe&amp;quot;, &amp;quot;Missing&amp;quot;)
// Build the initial Graph
val graph = Graph(users, relationships, defaultUser)
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;Edge&lt;/code&gt; is a built in class for storing edges information. It is defined as:&lt;/p&gt;
&lt;p&gt;Edge[ED](
srcId: VertexId = 0,
dstId: VertexId = 0,
attr: ED = null.asInstanceOf[ED]) extends Serializable with Product&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The Graph constructor takes as input two RDDs and a default user, which is needed to handle situations when a vertex
is defined in the EdgeRDD but not in the VertexRDD.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Given a Graph instance, you can query both the RDD containing the vertices and the edges directly from that
instance.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;val graph: Graph[(String, String), String] // Constructed from above
// Count all users which are postdocs
graph.vertices.filter { case (id, (name, pos)) =&amp;gt; pos == &amp;quot;postdoc&amp;quot; }.count
// Count all the edges where src &amp;gt; dst
graph.edges.filter(e =&amp;gt; e.srcId &amp;gt; e.dstId).count
// or equivalently
graph.edges.filter{case Edge(src, dst, prop) =&amp;gt; src &amp;gt;  dst}.count
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It should be noted that &lt;code&gt;graph.vertices&lt;/code&gt; returns an &lt;code&gt;VertexRDD[(String, String)]&lt;/code&gt; in this case, which
it is interpreted as an &lt;code&gt;RDD[(VertexID, (String, String)]&lt;/code&gt; – this is why we can use the &lt;code&gt;case&lt;/code&gt; construct
(&lt;a href=&#34;https://docs.scala-lang.org/tour/pattern-matching.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;take a look here&lt;/a&gt;). The same holds when you access the edges with  &lt;code&gt;graph.edges&lt;/code&gt;. This returns an instance
of &lt;code&gt;EdgeRDD&lt;/code&gt; which contains objects of type &lt;code&gt;Edge[String]&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;In addition to these two mechanism there is a third one, which exposes a triplet view.&lt;/p&gt;
&lt;p&gt;A triplet view consists of a series of &lt;code&gt;EdgeTriplet&lt;/code&gt; objects. An &lt;code&gt;EdgeTriplet&lt;/code&gt; object merges the information
about the two vertices – endpoints of the edges – and the relationship between them.
Conceptually, you can think at the figure:&lt;/p&gt;








  











&lt;figure id=&#34;figure-edge-triplet&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;/media/img/triplet.png&#34; data-caption=&#34;Edge Triplet&#34;&gt;


  &lt;img src=&#34;/media/img/triplet.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Edge Triplet
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;The &lt;code&gt;EdgeTriplet&lt;/code&gt; class extends &lt;code&gt;Edge&lt;/code&gt; by adding the attributes of both the source node and the target node.&lt;/p&gt;
&lt;p&gt;You can use the triplet view as in the following example:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;val graph: Graph[(String, String), String] // Constructed from above
// Use the triplets view to create an RDD of facts.
val facts: RDD[String] =
  graph.triplets.map(triplet =&amp;gt;
    triplet.srcAttr._1 + &amp;quot; is the &amp;quot; + triplet.attr + &amp;quot; of &amp;quot; + triplet.dstAttr._1)
facts.collect.foreach(println(_))
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;graph-operations&#34;&gt;Graph Operations&lt;/h1&gt;
&lt;p&gt;As any other RDD, a graph supports basic operations such as &lt;code&gt;map&lt;/code&gt;, &lt;code&gt;filter&lt;/code&gt;, &lt;code&gt;reduce&lt;/code&gt; and so on.
In addition to that, graphs support a number of transformations specifically meaningful when working with graph.&lt;/p&gt;
&lt;p&gt;This kind of operations are defined as &lt;code&gt;GraphOps&lt;/code&gt; and most of the time they are accessible as
members of a Graph object.&lt;/p&gt;
&lt;p&gt;The following list provide a summary of the major GraphOps directly accessible from a
graph instance.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;/** Summary of the functionality in the property graph */
class Graph[VD, ED] {
  // Information about the Graph ===================================================================
  val numEdges: Long
  val numVertices: Long
  val inDegrees: VertexRDD[Int]
  val outDegrees: VertexRDD[Int]
  val degrees: VertexRDD[Int]
  // Views of the graph as collections =============================================================
  val vertices: VertexRDD[VD]
  val edges: EdgeRDD[ED]
  val triplets: RDD[EdgeTriplet[VD, ED]]
  // Functions for caching graphs ==================================================================
  def persist(newLevel: StorageLevel = StorageLevel.MEMORY_ONLY): Graph[VD, ED]
  def cache(): Graph[VD, ED]
  def unpersistVertices(blocking: Boolean = false): Graph[VD, ED]
  // Change the partitioning heuristic  ============================================================
  def partitionBy(partitionStrategy: PartitionStrategy): Graph[VD, ED]
  // Transform vertex and edge attributes ==========================================================
  def mapVertices[VD2](map: (VertexId, VD) =&amp;gt; VD2): Graph[VD2, ED]
  def mapEdges[ED2](map: Edge[ED] =&amp;gt; ED2): Graph[VD, ED2]
  def mapEdges[ED2](map: (PartitionID, Iterator[Edge[ED]]) =&amp;gt; Iterator[ED2]): Graph[VD, ED2]
  def mapTriplets[ED2](map: EdgeTriplet[VD, ED] =&amp;gt; ED2): Graph[VD, ED2]
  def mapTriplets[ED2](map: (PartitionID, Iterator[EdgeTriplet[VD, ED]]) =&amp;gt; Iterator[ED2])
    : Graph[VD, ED2]
  // Modify the graph structure ====================================================================
  def reverse: Graph[VD, ED]
  def subgraph(
      epred: EdgeTriplet[VD,ED] =&amp;gt; Boolean = (x =&amp;gt; true),
      vpred: (VertexId, VD) =&amp;gt; Boolean = ((v, d) =&amp;gt; true))
    : Graph[VD, ED]
  def mask[VD2, ED2](other: Graph[VD2, ED2]): Graph[VD, ED]
  def groupEdges(merge: (ED, ED) =&amp;gt; ED): Graph[VD, ED]
  // Join RDDs with the graph ======================================================================
  def joinVertices[U](table: RDD[(VertexId, U)])(mapFunc: (VertexId, VD, U) =&amp;gt; VD): Graph[VD, ED]
  def outerJoinVertices[U, VD2](other: RDD[(VertexId, U)])
      (mapFunc: (VertexId, VD, Option[U]) =&amp;gt; VD2)
    : Graph[VD2, ED]
  // Aggregate information about adjacent triplets =================================================
  def collectNeighborIds(edgeDirection: EdgeDirection): VertexRDD[Array[VertexId]]
  def collectNeighbors(edgeDirection: EdgeDirection): VertexRDD[Array[(VertexId, VD)]]
  def aggregateMessages[Msg: ClassTag](
      sendMsg: EdgeContext[VD, ED, Msg] =&amp;gt; Unit,
      mergeMsg: (Msg, Msg) =&amp;gt; Msg,
      tripletFields: TripletFields = TripletFields.All)
    : VertexRDD[A]
  // Iterative graph-parallel computation ==========================================================
  def pregel[A](initialMsg: A, maxIterations: Int, activeDirection: EdgeDirection)(
      vprog: (VertexId, VD, A) =&amp;gt; VD,
      sendMsg: EdgeTriplet[VD, ED] =&amp;gt; Iterator[(VertexId, A)],
      mergeMsg: (A, A) =&amp;gt; A)
    : Graph[VD, ED]
  // Basic graph algorithms ========================================================================
  def pageRank(tol: Double, resetProb: Double = 0.15): Graph[Double, Double]
  def connectedComponents(): Graph[VertexId, ED]
  def triangleCount(): Graph[Int, ED]
  def stronglyConnectedComponents(numIter: Int): Graph[VertexId, ED]
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Operators can be classified into three different categories:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Property Operators&lt;/p&gt;
&lt;p&gt;They are very similar to the map operator of a regular RDD.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;class Graph[VD, ED] {
  def mapVertices[VD2](map: (VertexId, VD) =&amp;gt; VD2): Graph[VD2, ED]
  def mapEdges[ED2](map: Edge[ED] =&amp;gt; ED2): Graph[VD, ED2]
  def mapTriplets[ED2](map: EdgeTriplet[VD, ED] =&amp;gt; ED2): Graph[VD, ED2]
}
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Each of these operators yields a
new graph with the vertex
or edge properties modified
by the user defined map function.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The structure of the graph is not affected by these operations&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Note that in order to use GraphX optimization you must prefer the following snippet&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;val newGraph = graph.mapVertices((id, attr) =&amp;gt; mapUdf(id, attr))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;to the following one&lt;/p&gt;
&lt;pre&gt;&lt;code&gt; val newVertices = graph.vertices.map { case (id, attr) =&amp;gt; (id, mapUdf(id, attr)) }
val newGraph = Graph(newVertices, graph.edges)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Although they accomplish the same task, only the first one preserves the structural properties an it
exploits GraphX optimizations.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;When to use these oprators&lt;/p&gt;
&lt;p&gt;Usually, you want to use these transformations to prepare your graph before the application of some
algorithm.
For instance, if you want to prepare the graph in order to compute the page rank.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;// Given a graph where the vertex property is the out degree
val inputGraph: Graph[Int, String] =
  graph.outerJoinVertices(graph.outDegrees)((vid, _, degOpt) =&amp;gt; degOpt.getOrElse(0))
// Construct a graph where each edge contains the weight
// and each vertex is the initial PageRank
val outputGraph: Graph[Double, Double] =
  inputGraph.mapTriplets(triplet =&amp;gt; 1.0 / triplet.srcAttr).mapVertices((id, _) =&amp;gt; 1.0)
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Structural Operators&lt;/p&gt;
&lt;p&gt;Currently GraphX supports only a simple set of commonly used structural operators.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;class Graph[VD, ED] {
  def reverse: Graph[VD, ED]
  def subgraph(epred: EdgeTriplet[VD,ED] =&amp;gt; Boolean,
           vpred: (VertexId, VD) =&amp;gt; Boolean): Graph[VD, ED]
  def mask[VD2, ED2](other: Graph[VD2, ED2]): Graph[VD, ED]
  def groupEdges(merge: (ED, ED) =&amp;gt; ED): Graph[VD,ED]
}
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;reverse&lt;/code&gt; returns a new Graph where the direction of every edge is reversed&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;subgraph&lt;/code&gt; returns the subgraph induced by the conditions provided as input – it is like applying a filter.
This kind of operator is very useful when you need to restrict the graph to a group of vertices of interest,
or when you want to ignore a set of broken links.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;// Create an RDD for the vertices
val users: RDD[(VertexId, (String, String))] =
sc.parallelize(Seq((3L, (&amp;quot;rxin&amp;quot;, &amp;quot;student&amp;quot;)), (7L, (&amp;quot;jgonzal&amp;quot;, &amp;quot;postdoc&amp;quot;)),
    (5L, (&amp;quot;franklin&amp;quot;, &amp;quot;prof&amp;quot;)), (2L, (&amp;quot;istoica&amp;quot;, &amp;quot;prof&amp;quot;)),
    (4L, (&amp;quot;peter&amp;quot;, &amp;quot;student&amp;quot;))))
// Create an RDD for edges
val relationships: RDD[Edge[String]] =
sc.parallelize(Seq(Edge(3L, 7L, &amp;quot;collab&amp;quot;),    Edge(5L, 3L, &amp;quot;advisor&amp;quot;),
    Edge(2L, 5L, &amp;quot;colleague&amp;quot;), Edge(5L, 7L, &amp;quot;pi&amp;quot;),
    Edge(4L, 0L, &amp;quot;student&amp;quot;),   Edge(5L, 0L, &amp;quot;colleague&amp;quot;)))
// Define a default user in case there are relationship with missing user
val defaultUser = (&amp;quot;John Doe&amp;quot;, &amp;quot;Missing&amp;quot;)
// Build the initial Graph
val graph = Graph(users, relationships, defaultUser)
// Notice that there is a user 0 (for which we have no information) connected to users
// 4 (peter) and 5 (franklin).
graph.triplets.map(
    triplet =&amp;gt; triplet.srcAttr._1 + &amp;quot; is the &amp;quot; + triplet.attr + &amp;quot; of &amp;quot; + triplet.dstAttr._1
    ).collect.foreach(println(_))

// Remove missing vertices as well as the edges to connected to them
val validGraph = graph.subgraph(vpred = (id, attr) =&amp;gt; attr._2 != &amp;quot;Missing&amp;quot;)
// The valid subgraph will disconnect users 4 and 5 by removing user 0
validGraph.vertices.collect.foreach(println(_))
validGraph.triplets.map(
triplet =&amp;gt; triplet.srcAttr._1 + &amp;quot; is the &amp;quot; + triplet.attr + &amp;quot; of &amp;quot; + triplet.dstAttr._1
).collect.foreach(println(_))
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;It should be noted that you are not required to provide two different predicates. The one that you do not provided
is defaulted to a predicate that returns always true&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;mask&lt;/code&gt; returns a new graph containing only the vertices contained in the input graph.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;  // Run Connected Components
val ccGraph = graph.connectedComponents() // No longer contains missing field
// Remove missing vertices as well as the edges to connected to them
val validGraph = graph.subgraph(vpred = (id, attr) =&amp;gt; attr._2 != &amp;quot;Missing&amp;quot;)
// Restrict the answer to the valid subgraph
val validCCGraph = ccGraph.mask(validGrap
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;groupEdges&lt;/code&gt; merges parallel edges in the multigraph.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Join Operators&lt;/p&gt;
&lt;p&gt;In many cases it is necessary to join data from external collections (RDDs) with graphs.
For example, we might have extra user
properties that we want to merge
with an existing graph or we might want to pull
vertex properties from one graph
into another. These tasks can be
accomplished using the join operators.
Below we list the key join operators:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;class Graph[VD, ED] {
  def joinVertices[U](table: RDD[(VertexId, U)])(map: (VertexId, VD, U) =&amp;gt; VD)
    : Graph[VD, ED]
  def outerJoinVertices[U, VD2](table: RDD[(VertexId, U)])(map: (VertexId, VD, Option[U]) =&amp;gt; VD2)
    : Graph[VD2, ED]
}
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The &lt;code&gt;joinVertices&lt;/code&gt; returns a new Graph where the vertices are obtaining by merging the original ones with ones
of the input RDD. Then, the user defined map function is applied upon the joined set of vertices.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;val nonUniqueCosts: RDD[(VertexId, Double)]
val uniqueCosts: VertexRDD[Double] =
graph.vertices.aggregateUsingIndex(nonUnique, (a,b) =&amp;gt; a + b)
val joinedGraph = graph.joinVertices(uniqueCosts)(
(id, oldCost, extraCost) =&amp;gt; oldCost + extraCost)
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;It should be noted that if an RDD contains more that one value for a given vertex index, only the first value
is involved in the join operation.&lt;br&gt;
Also, nodes in the original graph that are not involved in the join process keep their original value.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The &lt;code&gt;outerJoinVertices&lt;/code&gt; is similar to the previous operation, the only exception is that the user defined
map function is applied to every node both in the graph and in the input RDD and it can also change the vertex
property type.
For instance, we can set the up a graph for PageRank by initializing the vertex properties with the out degree of each node.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;val outDegrees: VertexRDD[Int] = graph.outDegrees
val degreeGraph = graph.outerJoinVertices(outDegrees) { (id, oldAttr, outDegOpt) =&amp;gt;
outDegOpt match { //this ampping function change the type of the property
    case Some(outDeg) =&amp;gt; outDeg
    case None =&amp;gt; 0 // No outDegree means zero outDegree
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Note that, since it is not required that every node in the original graph has a counterpart in the input RDD,
the map function returns an Option type.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;graph-specific-operations&#34;&gt;Graph Specific Operations&lt;/h1&gt;
&lt;p&gt;A key step in many graph analytics tasks is aggregating information about the neighborhood of each vertex.&lt;/p&gt;
&lt;p&gt;For example, we might want to know the number of
followers each user has or the average age
of the followers of each user.&lt;/p&gt;
&lt;p&gt;Many iterative graph algorithms (e.g., PageRank, Shortest Path, and connected components)
repeatedly aggregate properties of neighboring vertices
(e.g., current PageRank Value, shortest path to the source, and smallest reachable vertex id).&lt;/p&gt;
&lt;p&gt;The core of this aggregation mechanism in GraphX is represented by the &lt;code&gt;aggregateMessages&lt;/code&gt; operation.
This operator applies a user defined &lt;code&gt;sendMsg&lt;/code&gt; function to each edge triplet in the graph and then uses the &lt;code&gt;mergeMsg&lt;/code&gt;
function to aggregate those messages as their destination vertex.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;class Graph[VD, ED] {
  def aggregateMessages[Msg: ClassTag](
      sendMsg: EdgeContext[VD, ED, Msg] =&amp;gt; Unit,
      mergeMsg: (Msg, Msg) =&amp;gt; Msg,
      tripletFields: TripletFields = TripletFields.All)
    : VertexRDD[Msg]
}
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;sendMsg&lt;/code&gt; takes an &lt;code&gt;EdgeContext&lt;/code&gt;, which exposes the source and destination attributes along with the edge
attribute and function (&lt;code&gt;sendToSrc&lt;/code&gt; and &lt;code&gt;sendToDst&lt;/code&gt;) to send messages to the source and destination attributes.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;mergeMsg&lt;/code&gt; takes two messages destined to the same vertex and returns a single message.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;tripletFields&lt;/code&gt; it is an optional argument that indicates what data is accessed in the EdgeContext (i.e., the source vertex attribute).
The default option is:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;TripletFields.All&lt;/code&gt;, which indicates that the user defined &lt;code&gt;sendMsg&lt;/code&gt; function may access any fields in the &lt;code&gt;EdgeContext&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;TripletFields&lt;/code&gt; are  a convenient way to specify which part of the &lt;code&gt;EdgeContext&lt;/code&gt; is involved in the transformation, allowing
GraphX to apply some optimization.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;You can think at a these two function &lt;code&gt;send/mergeMsg&lt;/code&gt; as a  &lt;code&gt;map/reduce&lt;/code&gt; transformation.
The aggregateMessages returns a VertexRDD[Msg] containing the aggregate messages (of type Msg) destined to
each vertex. All the vertices that did not receive any message are not included in the final result.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Example&lt;/p&gt;
&lt;p&gt;In the following example we use the aggregateMessages operator to compute the average age of the more senior followers of each user.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;import org.apache.spark.graphx.{Graph, VertexRDD}
import org.apache.spark.graphx.util.GraphGenerators
    
// Create a graph with &amp;quot;age&amp;quot; as the vertex property.
// Here we use a random graph for simplicity.
val graph: Graph[Double, Int] =
  GraphGenerators.logNormalGraph(sc, numVertices = 100).mapVertices( (id, _) =&amp;gt; id.toDouble )
// Compute the number of older followers and their total age
val olderFollowers: VertexRDD[(Int, Double)] = graph.aggregateMessages[(Int, Double)](
  triplet =&amp;gt; { // Map Function
    if (triplet.srcAttr &amp;gt; triplet.dstAttr) {
      // Send message to destination vertex containing counter and age
      triplet.sendToDst((1, triplet.srcAttr))
    }
  },
  // Add counter and age
  (a, b) =&amp;gt; (a._1 + b._1, a._2 + b._2) // Reduce Function
)
// Divide total age by number of older followers to get average age of older followers
val avgAgeOfOlderFollowers: VertexRDD[Double] =
  olderFollowers.mapValues( (id, value) =&amp;gt;
    value match { case (count, totalAge) =&amp;gt; totalAge / count } )
// Display the results
avgAgeOfOlderFollowers.collect.foreach(println(_))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt; AggregateMessages works best when the messages are constant sized – so no list, no concatenation.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;degree-information&#34;&gt;Degree Information&lt;/h2&gt;
&lt;p&gt;The following  example shows how to compute the maximum degree of any vertex in the graph.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;/ Define a reduce operation to compute the highest degree vertex
def max(a: (VertexId, Int), b: (VertexId, Int)): (VertexId, Int) = {
  if (a._2 &amp;gt; b._2) a else b
}
// Compute the max degrees
val maxInDegree: (VertexId, Int)  = graph.inDegrees.reduce(max)
val maxOutDegree: (VertexId, Int) = graph.outDegrees.reduce(max)
val maxDegrees: (VertexId, Int)   = graph.degrees.reduce(max)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;caching-and-uncaching&#34;&gt;Caching and Uncaching&lt;/h2&gt;
&lt;p&gt;As any other RDD,if you need to use a graph multiple time you should cache it first – call &lt;code&gt;graph.cache()&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Unlike regular RDDs, graphs are often involved in iterative computations, therefore it would be great to have
a mechanism to &lt;em&gt;uncaching&lt;/em&gt; the intermediate graphs created within an algorithm iteration.
Even though these intermediate results are eventually evicted by the system, it still a waste of memory,
thus performance.&lt;/p&gt;
&lt;p&gt;There is no trivial way to uncache a Graph, or in general an RDD, for this reason, if you need to design an iterative
algorithm over a graph, it is better to use the &lt;strong&gt;Pregel  API&lt;/strong&gt;&lt;/p&gt;
&lt;h1 id=&#34;pregel-api&#34;&gt;Pregel API&lt;/h1&gt;
&lt;h2 id=&#34;what-is-pregel&#34;&gt;What is Pregel&lt;/h2&gt;
&lt;p&gt;Pregel is a data flow paradigm and system for large-scale graph processing created at Google,
to solve problems that were not easily solvable with an approach based on map-reduce.&lt;/p&gt;
&lt;p&gt;If the system remains proprietary at Google, the computational paradigm was adopted by many graph-processing systems,
including GraphX. In order to adopt the Pregel paradigm, most algorithms need to be redesigned
to embrace this approach.
Pregel is essentially a
message-passing interface constrained to the edges of a graph.&lt;br&gt;
To re-design an algorithm in a Pregel fashion, ones should &amp;ldquo;Think like a vertex&amp;rdquo;.
Also, the state of a node is defined by the state of its neighborhood.&lt;/p&gt;








  











&lt;figure id=&#34;figure-pregel-paradigm&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;/media/img/Screenshot%20from%202020-12-10%2021-47-22.png&#34; data-caption=&#34;Pregel Paradigm&#34;&gt;


  &lt;img src=&#34;/media/img/Screenshot%20from%202020-12-10%2021-47-22.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Pregel Paradigm
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;The above figure shows the Pregel data flow model.
A Pregel computation takes as input:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;a graph&lt;/li&gt;
&lt;li&gt;a set of vertex states&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;At each iteration, referred to as a superstep, each vertex can:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;send a message to its neighbors,&lt;/li&gt;
&lt;li&gt;process the messages received in a previous superstep and update its state, accordingly.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Thus, each superstep consists of a round of messages being passed between neighbors and an
update of the global vertex state.&lt;/p&gt;
&lt;p&gt;A few examples of Pregel implementations of graph algorithmswill help clarify how the paradigm works.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Example&lt;/p&gt;
&lt;p&gt;Imagine you need to compute the maximum value among all the nodes in the network.
The following sequence of figures show how we can accomplish this task with the Pregel paradigm.
The following figure represents the input graph.








  











&lt;figure id=&#34;figure-initial-graph&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;/media/img/Screenshot%20from%202020-12-10%2021-57-04.png&#34; data-caption=&#34;Initial Graph&#34;&gt;


  &lt;img src=&#34;/media/img/Screenshot%20from%202020-12-10%2021-57-04.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Initial Graph
  &lt;/figcaption&gt;


&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;In each step a vertex reads the messages received bu its incoming neighbors and
set its state to the maximum value between its own vale and  all the received messages.








  











&lt;figure id=&#34;figure-step-1&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;/media/img/Screenshot%20from%202020-12-10%2021-57-16.png&#34; data-caption=&#34;Step 1&#34;&gt;


  &lt;img src=&#34;/media/img/Screenshot%20from%202020-12-10%2021-57-16.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Step 1
  &lt;/figcaption&gt;


&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;If during an iteration a node does not update its state, then it becomes halted.
This means that it will not send any message in the following iteration.








  











&lt;figure id=&#34;figure-step-2&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;/media/img/Screenshot%20from%202020-12-10%2021-57-28.png&#34; data-caption=&#34;Step 2&#34;&gt;


  &lt;img src=&#34;/media/img/Screenshot%20from%202020-12-10%2021-57-28.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Step 2
  &lt;/figcaption&gt;


&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;The algorithm proceeds until every node becomes halted.








  











&lt;figure id=&#34;figure-step-3&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;/media/img/Screenshot%20from%202020-12-10%2021-57-37.png&#34; data-caption=&#34;Step 3&#34;&gt;


  &lt;img src=&#34;/media/img/Screenshot%20from%202020-12-10%2021-57-37.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Step 3
  &lt;/figcaption&gt;


&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;In general, sever optimization can be applied to the above example.
For instance one may use &lt;em&gt;combiners&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;A &lt;em&gt;combiner&lt;/em&gt; is a user provided function that can combine multiple messages
intended for the same vertex. This mechanism leads to a reduction in the number
of messages transmitted between the vertices.
In the above example a &lt;em&gt;combiner&lt;/em&gt; could collapse multiple messages into a single
one containing the maximum.&lt;/p&gt;
&lt;p&gt;Another useful mechanism is offered by the &lt;em&gt;aggregators&lt;/em&gt;.
An &lt;em&gt;aggregator&lt;/em&gt; enables global information exchange. Each vertex can provide
a value to an aggregator during a superste S, the Pregel framework combines
those values using a reduction operator, and the resulting value is made available to all
the vertices in the subsequent superstep S+1.
Another common way to use aggregators is to elect a node to play a distinguished role in
an algorithm.&lt;/p&gt;
&lt;p&gt;There is also a mechanism that allows the removal or the addition of a new vertex
or edge. This is very useful for those algorithms that need to change the graph&amp;rsquo;s
topology – for instance a clustering algorithm might collapse every node belonging
to the same cluster into a single node.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;pregel-in-graphx&#34;&gt;Pregel in GraphX&lt;/h2&gt;
&lt;p&gt;The Pregel implementation of GraphX has some key difference from the original
definition.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Messages are compute in parallel as a function of the edgetriplet&lt;/li&gt;
&lt;li&gt;The message computation is not only available to recipient of the message, but it is also
available to the sender node&lt;/li&gt;
&lt;li&gt;Nodes can only send messages to their direct neighbors – no hops are allowed as in many other
pregel implementations.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As in the original Pregel definition, a node that becomes inactive – because it has not
any message to send or to process – is ignored during the superstep.
Also as in the original Pregel, the algorithm terminates when there are no remaining messages.&lt;/p&gt;
&lt;p&gt;The signature of the &lt;code&gt;pregel&lt;/code&gt; function – which is a member function of &lt;code&gt;Graph&lt;/code&gt; –
is defined as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;def pregel[A]
      (initialMsg: A,
       maxIter: Int = Int.MaxValue,
       activeDir: EdgeDirection = EdgeDirection.Out)
      (vprog: (VertexId, VD, A) =&amp;gt; VD,
       sendMsg: EdgeTriplet[VD, ED] =&amp;gt; Iterator[(VertexId, A)],
       mergeMsg: (A, A) =&amp;gt; A)
    : Graph[VD, ED] 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It takes two arguments lists:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The first list contains configuration parameters including:
&lt;ul&gt;
&lt;li&gt;The initial message&lt;/li&gt;
&lt;li&gt;The maximum number of iterations&lt;/li&gt;
&lt;li&gt;The edge direction in which to send messages (by default messages are sent via outgoing links)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;The second argument list contains:
&lt;ul&gt;
&lt;li&gt;The user defined function for receiving messages - &lt;code&gt;vprog&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;The user defined function for computing messages - &lt;code&gt;sendMsg&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;The user defined function for combining messages - &lt;code&gt;mergeMsg&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The previous example in GraphX can be solved as follows.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;import org.apache.spark.graphx.{Graph, VertexId}
import org.apache.spark.graphx.util.GraphGenerators
val r = scala.util.Random
// A graph with edge attributes containing distances
val graph: Graph[Long, Double] =
  GraphGenerators.logNormalGraph(sc, numVertices = 100).mapEdges(e =&amp;gt; e.attr.toDouble)
val sourceId: VertexId = 42 // The ultimate source
// Initialize the graph such that all vertices except the root have distance infinity.
val initialGraph = graph.mapVertices((id, _) =&amp;gt; r.nextInt)

//pregel
val sssp = initialGraph.pregel(
    Int.MinValue //inital messages 
)(
  (id, currentValue, receivedValue) =&amp;gt; math.max(currentValue, receivedValue), // Vertex Program
  triplet =&amp;gt; {  // Send Message 
    val sourceVertex = triplet.srcAttr //get the property associated with the src vertex
    if (sourceVertex._1 == sourceVertex._2_) //new value match the current one - the node is halted
      Iterator.empty // no messages
    else
      Iterator((triplet.dstId, sourceVertex._1)) //send out the message
    }
  },
  (a, b) =&amp;gt; math.max(a, b) // Merge Message if multiple messages are received from the same vertex
)
println(sssp.vertices.collect.mkString(&amp;quot;\n&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;
An edge triplet has five properties:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;srcId&lt;/code&gt;. The source vertex id&lt;/li&gt;
&lt;li&gt;&lt;code&gt;srcAttr&lt;/code&gt;. The source vertex property&lt;/li&gt;
&lt;li&gt;&lt;code&gt;stdId&lt;/code&gt;. The destination vertex id&lt;/li&gt;
&lt;li&gt;&lt;code&gt;dstAttr&lt;/code&gt;. The destination vertex property&lt;/li&gt;
&lt;li&gt;&lt;code&gt;attr&lt;/code&gt;. The edge property.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;p&gt;The material for this lesson is available  &lt;a href=&#34;https://github.com/acalio/BDAnalytics/blob/main/05-Spark-GraphX/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Attribute based Diversification of Seeds for Targeted Influence Maximization</title>
      <link>/publication/calio-attribute-2020/</link>
      <pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate>
      <guid>/publication/calio-attribute-2020/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Complex Influence Propagation based on Trust-aware Dynamic Linear Threshold Models</title>
      <link>/project/f2dlt/</link>
      <pubDate>Wed, 14 Oct 2020 09:44:20 +0200</pubDate>
      <guid>/project/f2dlt/</guid>
      <description>&lt;h1 id=&#34;overview&#34;&gt;Overview&lt;/h1&gt;
&lt;p&gt;This project is a web-based simulation environment for Friend-Foe-Dynamic-Linear-Threshold (F2DLT) Models
presented for the first time at &lt;a href=&#34;/publication/calio-trust-based-2018/&#34; title=&#34;publications&#34;&gt;TrustCom 2018&lt;/a&gt; and
further discussed in &lt;a href=&#34;/publication/calio-complex-2019/&#34; title=&#34;publications&#34;&gt;Applied Network Science 2019&lt;/a&gt;
and at &lt;a href=&#34;/publication/calio-framework-nodate/&#34; title=&#34;publications&#34;&gt;SEBD 2020&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;If you follow the above link you will be presented with a user-friendly interface, that allows you
to create/upload a diffusion graph and to run a diffusion process.&lt;/p&gt;
&lt;p&gt;As the diffusion unfolds, the platform will show you real-time statistics about the current diffusion phenomena.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>(Di)Graph Decompositions</title>
      <link>/project/cores/</link>
      <pubDate>Wed, 14 Oct 2020 09:30:27 +0200</pubDate>
      <guid>/project/cores/</guid>
      <description>&lt;h1 id=&#34;overview&#34;&gt;Overview&lt;/h1&gt;
&lt;p&gt;This project is developed as part of the following research paper:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://doi.org/10.1145/3394231.3397908&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;A. Caliò, A. Tagarelli, F. Bonchi (2020). Cores matter? An analysis of graph decomposition effects on influence maximization problem. In Procs. of the 12th ACM Web Science Conference (WebSci-2020), July 6th - July 10th, 2020,  Southampton, UK&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;It contains a variety of graph-decomposition methods, specially adapted to be applied in an influence
propagation problem &amp;ndash; i.e., upon directed and weighted graphs.&lt;/p&gt;
&lt;p&gt;Please follow the above link for detailed instructions on how to install and use this project.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>ADITUM - Attribute DIversity-sensitive Targeted inflUence Maximization</title>
      <link>/project/aditum/</link>
      <pubDate>Wed, 14 Oct 2020 09:30:27 +0200</pubDate>
      <guid>/project/aditum/</guid>
      <description>&lt;h1 id=&#34;overview&#34;&gt;Overview&lt;/h1&gt;
&lt;p&gt;This project is developed as part of the following research paper:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://doi.org/10.1016/j.ins.2020.08.093&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;A. Caliò, A. Tagarelli.Attribute based Diversification of Seeds for Targeted Influence Maximization problem.
Information Sciences, 2020&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;It contains the implementation of every influence maximization algorithm discussed in above paper.
Pleas follow the link for detailed instructions on how to install and use this project.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title> Cores matter? An analysis of graph decomposition effects on influence maximization</title>
      <link>/talk/websci/</link>
      <pubDate>Tue, 14 Jul 2020 00:00:00 +0000</pubDate>
      <guid>/talk/websci/</guid>
      <description></description>
    </item>
    
    <item>
      <title>A Framework for Complex Inﬂuence Propagation based on the F 2DLT Class of Diﬀusion Models</title>
      <link>/publication/calio-framework-2020/</link>
      <pubDate>Wed, 01 Jul 2020 00:00:00 +0000</pubDate>
      <guid>/publication/calio-framework-2020/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Cores matter? An analysis of graph decomposition effects on influence maximization problems</title>
      <link>/publication/calio-cores-2020/</link>
      <pubDate>Wed, 01 Jul 2020 00:00:00 +0000</pubDate>
      <guid>/publication/calio-cores-2020/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Complex influence propagation based on trust-aware dynamic linear threshold models</title>
      <link>/publication/calio-complex-2019/</link>
      <pubDate>Sun, 01 Dec 2019 00:00:00 +0000</pubDate>
      <guid>/publication/calio-complex-2019/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Topology-Driven Diversity for Targeted Influence Maximization with Application to User Engagement in Social Networks</title>
      <link>/publication/calio-topology-driven-2018/</link>
      <pubDate>Sat, 01 Dec 2018 00:00:00 +0000</pubDate>
      <guid>/publication/calio-topology-driven-2018/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Classification</title>
      <link>/courses/datamining/classproblem/</link>
      <pubDate>Sun, 09 Sep 2018 00:00:00 +0000</pubDate>
      <guid>/courses/datamining/classproblem/</guid>
      <description>&lt;p&gt;In this lesson you are going to solve a classification problem.&lt;/p&gt;
&lt;p&gt;The notebok for this lesson is available &lt;a href=&#34;https://github.com/acalio/DM/tree/main/03-classification&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Data Handling &amp; Visualization</title>
      <link>/courses/datamining/pandasintro/</link>
      <pubDate>Sun, 09 Sep 2018 00:00:00 +0000</pubDate>
      <guid>/courses/datamining/pandasintro/</guid>
      <description>&lt;p&gt;This lesson introduces you to Pandas.
Your swiss-knife  for handling data in Python.&lt;/p&gt;
&lt;p&gt;The notebok for this lesson is available &lt;a href=&#34;https://github.com/acalio/DM&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Introduction to scikit-learn</title>
      <link>/courses/datamining/introscikit/</link>
      <pubDate>Sun, 09 Sep 2018 00:00:00 +0000</pubDate>
      <guid>/courses/datamining/introscikit/</guid>
      <description>&lt;p&gt;This lesson introduces you to scikit-learn.
You will go through a real-world data mining problem.&lt;/p&gt;
&lt;p&gt;The notebok for this lesson is available &lt;a href=&#34;https://github.com/acalio/DM/blob/main/02-Scikit_intro/scikit_intro.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Trust-based dynamic linear threshold models for non-competitive and competitive influence propagation</title>
      <link>/talk/trustcom/</link>
      <pubDate>Wed, 01 Aug 2018 15:29:00 +0200</pubDate>
      <guid>/talk/trustcom/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Trust-Based Dynamic Linear Threshold Models for Non-competitive and Competitive Influence Propagation</title>
      <link>/publication/calio-trust-based-2018/</link>
      <pubDate>Wed, 01 Aug 2018 00:00:00 +0000</pubDate>
      <guid>/publication/calio-trust-based-2018/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Machine Learning Crash Course with TensorFlow APIs</title>
      <link>/talk/google-jam/</link>
      <pubDate>Sat, 09 Jun 2018 15:29:36 +0200</pubDate>
      <guid>/talk/google-jam/</guid>
      <description></description>
    </item>
    
    <item>
      <title></title>
      <link>/courses/datamining/clustering/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/courses/datamining/clustering/</guid>
      <description></description>
    </item>
    
    <item>
      <title></title>
      <link>/courses/datamining/enslearn/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/courses/datamining/enslearn/</guid>
      <description></description>
    </item>
    
    <item>
      <title></title>
      <link>/courses/datamining/finallesson/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/courses/datamining/finallesson/</guid>
      <description></description>
    </item>
    
    <item>
      <title></title>
      <link>/courses/datamining/numpyintro/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/courses/datamining/numpyintro/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
