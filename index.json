[{"authors":["admin"],"categories":null,"content":"Antonio Caliò is a PhD student in the field of Data Science, with a particular emphasis on Social Network Analysis.\nDuring his PhD, he developed deep knowledge on many relevant problems in the Network Science landscape.\nMore specifically, he studied problems related to diffusion models, influence propagation/maximization and graph decomposition algorithms.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/author/antonio-calio/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/antonio-calio/","section":"authors","summary":"Antonio Caliò is a PhD student in the field of Data Science, with a particular emphasis on Social Network Analysis.\nDuring his PhD, he developed deep knowledge on many relevant problems in the Network Science landscape.","tags":null,"title":"Antonio Caliò","type":"authors"},{"authors":null,"categories":null,"content":"Course Description This course provides a broad introduction about techniques used to analyze Big-Data.\nThe course will also present a variety of tools that are leading the industry when it comes to handling and working with Big-Data.\nMost part of tech stack adopted throughout the course is based on the Apache Spark framework.\nTeachers  Andrea Tagarelli Antonio Caliò (Teaching Assistant)  Roadmap  Introduction to Scala and its Build System  Scala Crash Course Overview of the main Scala build system: sbt   The Spark ecosystem  Introduction to the Spark ecosystem Example projects in Scala   Useful Big-Data Libraries  Spark-SQL Spark-Streaming Spark-MLLib SPark-GraphX Kafka   Integration with other system and deploy  Apache Kafka Deploy on a real cluster    ","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536451200,"objectID":"d66894a2a524ecc36636439554372fe2","permalink":"/courses/bdanalytics/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/courses/bdanalytics/","section":"courses","summary":"Learn how to use technology for real-time analysis of your big data application.","tags":null,"title":"Overview","type":"docs"},{"authors":null,"categories":null,"content":"Course Description This course provides an introduction on Data Mining and Machine Learning.\nMore precisely, this course will present you the main Python libraries that any Data Scientist should know.\nYou will learn how to gain insights from your analysis, how to visualize your data and finally to use your data in order to train a number of different machine learning algorithm with the goal of solving some business problem.\nTeachers  Sergio Greco Antonio Caliò  Roadmap  Introduction to the course NumPy Basics Data handling with Pandas Data visualization with matplotlib and seaborn Introduction to scikit-learn Solving a classification problem Ensamble learning Clustering Project Example  ","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536451200,"objectID":"a2bfa9b112a7d646ed2447b2a4dfcfc1","permalink":"/courses/datamining/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/courses/datamining/","section":"courses","summary":"Learn data mining with Python.","tags":null,"title":"Overview","type":"docs"},{"authors":null,"categories":null,"content":"This lesson provides a quick introduction to the Scala ecosystem.\nWhat you will learn:\n How to write a Scala program How to compile Scala source code via the SBT shell Ho to work with loops and data structures How to use the the SBT build system How to structure a real-world project How to manage dependencies in your project  The material for this lesson is available here\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"4d78a7b1ddcdbf07bff0290ddf6f5c2f","permalink":"/courses/bdanalytics/scalaintro/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/bdanalytics/scalaintro/","section":"courses","summary":"This lesson provides a quick introduction to the Scala ecosystem.\nWhat you will learn:\n How to write a Scala program How to compile Scala source code via the SBT shell Ho to work with loops and data structures How to use the the SBT build system How to structure a real-world project How to manage dependencies in your project  The material for this lesson is available here","tags":null,"title":"Introduction to Scala and its Build System","type":"docs"},{"authors":null,"categories":null,"content":"SparkSQL is a library for structured data processing. It provides an abstraction mechanism – the main one is called DataFrame – which can serve as a distributed SQL query engine.\nSpark SQL offers the following features:\n  Integrated.\n Seamlessly mix SQL queries with Spark programs. Spark SQL lets you query structured data as a distributed dataset (RDD) in Spark This tight integration makes it easy to run SQL queries alongside complex analytic algorithms    Unified Data Access\n Load and query data from a variety of different sources like: Apache Hive tables, parquet files, JSON files, etc.    Scalability\n Use the same engine for both interactive and long queries SparkSQL leverages on the RDD model to provide fault tolerance an scalability    The code for this lesson is available here.\nArchitecture   SparkSQL architecture   The architecture contains three layers namely:\n Language API − Spark is compatible with different languages and Spark SQL. It is also, supported by these languages- API (python, scala, java, HiveQL). Schema RDD − Spark Core is designed with special data structure called RDD. Generally, Spark SQL works on schemas, tables, and records. Therefore, we can use the Schema RDD as temporary table. We can call this Schema RDD as Data Frame. Data Sources − Usually the Data source for spark-core is a text file, Avro file, etc. However, the Data Sources for Spark SQL is different. Those are Parquet file, JSON document, HIVE tables, and Cassandra database.  What is a DataFrame A DataFrame is a distributed collection of data, which is organized into named columns. You can think of a DataFrame as a relational table.\nDataFrames can be constructed from a variety of different sources such as Hive tables, Structured Data files, external database, or also an existing RDD.\nFeatures of a DataFrame   Ability to process the data in the size of Kilobytes to Petabytes on a single node cluster to large cluster\n  Supports different data formats (Avro, csv, elastic search, and Cassandra) and storage systems (HDFS, HIVE tables, mysql, etc).\n  State of art optimization and code generation through the Spark SQL Catalyst optimizer (tree transformation framework).\n  Can be easily integrated with all Big Data tools and frameworks via Spark-Core.\n  Optimized application of udf function over the entire dataframe. The following instructions will create a new column whose values are given by the column value to the power of two\nimport org.apache.spark.sql.functions.udf val square = (x=\u0026gt; x*x) val squaredDF = df.withColumn(\u0026ldquo;square\u0026rdquo;, square(\u0026ldquo;value\u0026rdquo;))\n  Example\nImagine you have the following data, formatted as a JSON file:\n{ {\u0026quot;id\u0026quot; : \u0026quot;1201\u0026quot;, \u0026quot;name\u0026quot; : \u0026quot;satish\u0026quot;, \u0026quot;age\u0026quot; : \u0026quot;25\u0026quot;} {\u0026quot;id\u0026quot; : \u0026quot;1202\u0026quot;, \u0026quot;name\u0026quot; : \u0026quot;krishna\u0026quot;, \u0026quot;age\u0026quot; : \u0026quot;28\u0026quot;} {\u0026quot;id\u0026quot; : \u0026quot;1203\u0026quot;, \u0026quot;name\u0026quot; : \u0026quot;amith\u0026quot;, \u0026quot;age\u0026quot; : \u0026quot;39\u0026quot;} {\u0026quot;id\u0026quot; : \u0026quot;1204\u0026quot;, \u0026quot;name\u0026quot; : \u0026quot;javed\u0026quot;, \u0026quot;age\u0026quot; : \u0026quot;23\u0026quot;} {\u0026quot;id\u0026quot; : \u0026quot;1205\u0026quot;, \u0026quot;name\u0026quot; : \u0026quot;prudvi\u0026quot;, \u0026quot;age\u0026quot; : \u0026quot;23\u0026quot;} }  You can read this file and create a dataframe as follows:\n... val spark = SparkSession .builder .appName(\u0026quot;SparkSQL\u0026quot;) .master(\u0026quot;local[*]\u0026quot;) .getOrCreate() val df = spark.sqlContext.read.json(\u0026quot;example.json\u0026quot;) df.show()  The last instruction returns the following result:\n+----+------+--------+ |age | id | name | +----+------+--------+ | 25 | 1201 | satish | | 28 | 1202 | krishna| | 39 | 1203 | amith | | 23 | 1204 | javed | | 23 | 1205 | prudvi | +----+------+--------+  You can access to the structure underlying a dataframe as follows:\ndf.printSchema  In this case it returns the following:\nroot |-- age: string (nullable = true) |-- id: string (nullable = true) |-- name: string (nullable = true)  You handle a DataFrame in a very similar fashion to a Pandas dataframe.\nFor instance:\ndf.select(\u0026quot;name\u0026quot;).show  Or also\n// This import is needed to use the $-notation import spark.sqlContext.implicts._ df.select($\u0026quot;name\u0026quot;).show  It returns:\n+--------+ | name | +--------+ | satish | | krishna| | amith | | javed | | prudvi | +--------+  You can use filter:\ndfs.filter(dfs(\u0026quot;age\u0026quot;) \u0026gt; 23).show()  It returns:\n+----+------+--------+ |age | id | name | +----+------+--------+ | 25 | 1201 | satish | | 28 | 1202 | krishna| | 39 | 1203 | amith | +----+------+--------+  You can group and apply aggregate functions to your data as follows:\ndfs.groupBy(\u0026quot;age\u0026quot;).count().show()  It returns:\n+----+-----+ |age |count| +----+-----+ | 23 | 2 | | 25 | 1 | | 28 | 1 | | 39 | 1 | +----+-----+    Running SQL Queries An SQLContext enables applications to run SQL queries programmatically while running SQL functions and returns the result as a DataFrame.\nGenerally, in the background, SparkSQL supports two different methods for converting existing RDDs into DataFrames.\n  Inferring the schema via reflection\nThis method uses reflection to generate the schema of an RDD that contains specific types of objects. The Scala interface for Spark SQL supports automatically converting an RDD containing case classes to a DataFrame. The case class defines the schema of the table. The names of the arguments to the case class are read using reflection and they become the names of the columns.\nCase classes can also be nested or contain complex types such as Sequences or Arrays. This RDD can be implicitly be converted to a DataFrame and then registered as a table. Tables can be used in subsequent SQL statements.\n  Example\nImagine you are given with the following data:\n1201, satish, 25 1202, krishna, 28 1203, amith, 39 1204, javed, 23 1205, prudvi, 23  First you need to define a case class – which is class that only define its contructor – to provide your data with a fixed structure:\ncase class Employee(id: Int, name: String, age: Int)  Next, you create an RDD mapping each line to the above case class and then convert it to a DataFrame\nval spark = SparkSession .builder .appName(\u0026quot;SparkSQL\u0026quot;) .master(\u0026quot;local[*]\u0026quot;) .getOrCreate() val df = spark.sparkContext.textFile(\u0026quot;employee.txt\u0026quot;) .map(_.split(\u0026quot;,\u0026quot;)).map(e=\u0026gt;Employee(e(0).trim.toInt, e(1), e(2).trim.toInt)).toDF df.show()  Now you have a fully functional data frame. If you want to use the SQL engine your first need to register the dataframe as a table:\nempl.registerTempTable(\u0026quot;employee\u0026quot;) //set the name of the table associated with the dataset  Then your can perform regular SQL query as follows:\nspark.sqlContext.sql(\u0026quot;Select * from employee*\u0026quot;).show  It returns:\n+------+---------+----+ | id | name |age | +------+---------+----+ | 1201 | satish | 25 | | 1202 | krishna | 28 | | 1203 | amith | 39 | | 1204 | javed | 23 | | 1205 | prudvi | 23 | +------+---------+----+  SparkSQL understands any sql query – so if you know SQL you are good to go.\n    Specify the schema programmatically\nThe second method for creating DataFrame is through programmatic interface that allows you to construct a schema and then apply it to an existing RDD. We can create a DataFrame programmatically using the following three steps.\n  Create an RDD of Rows from an Original RDD.\n  Create the schema represented by a StructType matching the structure of Rows in the RDD created in Step 1.\n  Apply the schema to the RDD of Rows via createDataFrame method provided by SQLContext.\n  Example\nImagine you are given with the following data:\n1201, satish, 25 1202, krishna, 28 1203, amith, 39 1204, javed, 23 1205, prudvi, 23  Next, you create an RDD from the text file.\n//remember to include these imports import org.apache.spark.sql.Row import org.apache.spark.sql.types.{StructType, StructField, StringType} val spark = SparkSession .builder .appName(\u0026quot;SparkSQL\u0026quot;) .master(\u0026quot;local[*]\u0026quot;) .getOrCreate() val df = spark.sparkContext.textFile(\u0026quot;employee.txt\u0026quot;)  Now, instead of defining a case class as we did earlier, we define schema with a String.\nval schemaString = \u0026quot;id name age\u0026quot;  The above string is then used to generate a schema as follows:\nval schema = StructType(schemaString.split(\u0026quot; \u0026quot;).map(fieldName =\u0026gt; =StructField(fieldName, StringType, true)))  Use the following command to convert an RDD (employee) to Rows. It means, here we are specifying the logic for reading the RDD data and store it into rowRDD. Here we are using two map functions: one is a delimiter for splitting the record string (.map(.split(\u0026quot;,\u0026quot;))) and the second map function for defining a Row with the field index value (.map(e ⇒ Row(e(0).trim.toInt, e(1), e(2).trim.toInt))).\nval rowRDD = employee.map(_.split(\u0026quot;,\u0026quot;)).map(e ⇒ Row(e(0).trim.toInt, e(1), e(2).trim.toInt)) val employeeDF = sqlContext.createDataFrame(rowRDD, schema)      Comparative evaluation DataFrame vs DataSet They are basically the same object, namely a collection of structured data – for instance DataSet[Person], DataSet[(String, Double)].\nActually, a DataFrame is an alias for Dataset[Row].\nThe major difference between the two is that the structure of the data contained within a DataFrame is inferred at runtime, while for a DataSet object Scala is able to infer the actual type of the objects at compile time. Clearly, this second mechanism is beneficial in terms of performance and it is also less prone to potential errors.\nDataSet vs RDD An RDD can be converted to a DataSet object with the method toDS.\nDataSets are more convenient than RDD for the following reasons:\n better efficiency better interoperability with other libraries:  MLlib relies on Datasets Spark streaming is moving towards structured streaming Everything that use apache Avro     The material for this lesson is available here\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"6e4971e8a090843ec5b715dc502ecbb8","permalink":"/courses/bdanalytics/sparksql/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/bdanalytics/sparksql/","section":"courses","summary":"SparkSQL is a library for structured data processing. It provides an abstraction mechanism – the main one is called DataFrame – which can serve as a distributed SQL query engine.","tags":null,"title":"Introduction to Spark SQL","type":"docs"},{"authors":null,"categories":null,"content":"MLlib is the Spark is the primary library for addressing machine learning problem within the Spark ecosystem.\nMLlib provides the following features:\n A collection of ML Algorithms for solving a variety of ML problem, such as classification, regression, clustering and collaborative filtering A collection of techniques for featurization, i.e., extracting and transforming the features of your dataset The ability to design e construct Pipeline of execution Persistence: models can be saved on file so that you will not be required to train a model every time you need to use it Utility tools, mostly for linear algebra e statistics  Spark MLlib is available in the version 3.0. It now uses dataset instead of regular RDDs.\nIn the following we will discuss some of the main tools provided by MLlib.\nThe code for this lesson is available here.\nBasic Statistics Correlation Correlation measures the strength of a linear relationship between two variables. More specifically, given two variables X e Y, they are considered:\n positively correlated – if when X increases Y increases and vice versa negatively correlated – if when X increases Y increases and vice versa no correlated – when their trends are not dependent from one another  The quantity compute by MLLlib is the Pearson\u0026rsquo;s coefficient, which is defined in [-1,1]. A coefficient equal to 1 (-1) defines a strong positive (negative) correlation.\nThe following snippet compute the correlation matrix between each pair of variables.\nimport org.apache.spark.ml.linalg.{Matrix, Vectors} import org.apache.spark.ml.stat.Correlation import org.apache.spark.sql.Row val data = Seq( Vectors.sparse(4, Seq((0, 1.0), (3, -2.0))), Vectors.dense(4.0, 5.0, 0.0, 3.0), Vectors.dense(6.0, 7.0, 0.0, 8.0), Vectors.sparse(4, Seq((0, 9.0), (3, 1.0))) ) val df = data.map(Tuple1.apply).toDF(\u0026quot;features\u0026quot;) val Row(coeff1: Matrix) = Correlation.corr(df, \u0026quot;features\u0026quot;).head println(s\u0026quot;Pearson correlation matrix:\\n $coeff1\u0026quot;) val Row(coeff2: Matrix) = Correlation.corr(df, \u0026quot;features\u0026quot;, \u0026quot;spearman\u0026quot;).head println(s\u0026quot;Spearman correlation matrix:\\n $coeff2\u0026quot;)  Summarizer It provides a summary of the main statistics for each column of a given DataFrame. The metrics computed by this object are the column-wise:\n  max\n  min\n  average\n  sum\n  variance/std\n  number of non-zero elements\n  total count\nimport org.apache.spark.ml.linalg.{Vector, Vectors} import org.apache.spark.ml.stat.Summarizer val data = Seq( (Vectors.dense(2.0, 3.0, 5.0), 1.0), (Vectors.dense(4.0, 6.0, 7.0), 2.0) ) val df = data.toDF(\u0026quot;features\u0026quot;, \u0026quot;weight\u0026quot;) val (meanVal, varianceVal) = df.select(metrics(\u0026quot;mean\u0026quot;, \u0026quot;variance\u0026quot;) .summary($\u0026quot;features\u0026quot;, $\u0026quot;weight\u0026quot;).as(\u0026quot;summary\u0026quot;)) .select(\u0026quot;summary.mean\u0026quot;, \u0026quot;summary.variance\u0026quot;) n.as[(Vector, Vector)].first() println(s\u0026quot;with weight: mean = ${meanVal}, variance = ${varianceVal}\u0026quot;) val (meanVal2, varianceVal2) = df.select(mean($\u0026quot;features\u0026quot;), variance($\u0026quot;features\u0026quot;)) .as[(Vector, Vector)].first() println(s\u0026quot;without weight: mean = ${meanVal2}, sum = ${varianceVal2}\u0026quot;)    Data Sources MLlib is able to work with a variety of \u0026ldquo;standard\u0026rdquo; formats as Parquet, CSV, JSON. In addition to that the library provides mechanisms to read some other special data format.\nImage data source This data is used to load image files directly from a folder. Images can be in any format (e.g., jpg, png, …). When you load the DataFrame, it has a column of type StryctType, which represents the actual image. Each image has an image schema which comprises the following information:\n origin: StringType, the path of the file containing the image height: IntegerType, the height of the image width: IntegerType, the width of the image nChannels: Integertype, number of image channels mode: IntegerType, OpenCV compatible type data: BinaryType, the image bytes in OpenCV-compatible format  In order to read the images you need to use the \u0026ldquo;image\u0026rdquo; format. You also need to provide the path of the folder containing the images you want to read.\nval df = spark.read.format(\u0026quot;image\u0026quot;).option(\u0026quot;dropInvalid\u0026quot;, true).load(\u0026quot;{your path}\u0026quot;)  LIBSVM data source This source is used to read data in the libsvm format from a given directory. When you load the file, MLlib creates a data frame with two columns:\n label: DoubleType, represents the labels of the dataset features: VectorUDT, represents the feature-vectors of each data point  In order to read this type of data you need to specify the \u0026ldquo;libsvm\u0026rdquo; format.\nval df = spark.read.format(\u0026quot;libsvm\u0026quot;).option(\u0026quot;numFeatures\u0026quot;, \u0026quot;780\u0026quot;).load(\u0026quot;{your path}\u0026quot;)  Pipelines Pipelines are a very convenient to define e organize all the single procedures and step that contribute to you entire machine learning approach.\nThey are very similar to scikit-learn pipelines. The Pipelines API are designed around the following concepts:\n DataFrame Transformer. It is an abstraction that includes both feature transformers and ML models. Any transformer has a method transform() which takes a DataFrame as input and returns another DataFrame obtained by applying some function, whose results are retrieved within a new column appended to the original structure. Estimators. It is an abstraction for denoting any algorithm that has the method fit in its own interface. The fit() method takes as input a DataFrame and returns an object of type Model. Pipelines. Usually, when you deal with ML problems, the entire algorithm representing your approach for solving the problem can be narrowed down to a set of subsequent different steps.  How Pipelines Work Let\u0026rsquo;s imagine you are dealing with some classification problems over text documents.\nChances are that your workflow will include the following stages:\n Split each document into words Convert each word int a vector – you may want to apply some representation model such as Word2Vec Train and compute the prediction of your model  Step 1 requires a Transformer, while Step 2 and Step 3 require to use some Estimators. Through a Pipeline you will be able to aggregate all these steps and to represent them as a pipeline.\nA Pipeline is specified as a sequence of stages, and each stage is either a Transformer or an Estimator. These stages are run in order, and the input DataFrame is transformed as it passes through each stage.\nFor Transformer stages, the transform() method is called on the DataFrame. For Estimator stages, the fit() method is called to produce a Transformer (which becomes part of the PipelineModel, or fitted Pipeline), and that Transformer’s transform() method is called on the DataFrame.\nWe illustrate this for the simple text document workflow. The figure below is for the training time usage of a Pipeline.\nThe following image represents the pipeline described above.   Pipeline   A Pipeline works as an Estimator, it means that it has the method fit(). When you call this method on the pipeline, all the stages are executed. More specifically, for any stage Si we have:\n if Si is a transformer, the pipeline will call the method transform. if Si is an estimator, the pipeline will first call the fit method and then the transform method  The method fit called on a Pipeline yields a Pipeline model, which is also a Transformer. This means that you can use the this model to call the transform method. As a result, the pipeline will go through all the stages by calling the transform method over each on of them. Be careful that, as opposed to when you called the fit method over the entire pipeline, when you call transform on the Pipeline, all the estimators work as Transformers.\nParameters You can specify the parameters for either a Transformer of an Estimators in one of the following two ways:\n  Both Estimators and Transformers share a common API to specify their parameters. A parameter is specified via a Param object, which is a named parameter with a self-contained documentation. A ParamMap is instead a set of parameter-value pairs. You can define a ParamMap and pass it to the fit() or the transform() method. A ParamMap is created with the following notation:\nParamMap(l1.maxIter -\u0026gt; 10, l2.maxIter -\u0026gt; 20)  Where l1 and l2 are two instance included in the Pipeline.\n  Set the parameter directly on the instance that you include within the your Pipeline. Let algo be an instance of any ML algorithm available in MLlib. You can use this instance to set directly the parameters for that particular algoirthm Usually, you have method like set{NameOfTheParameter}.\n  Examples   Estimator, Transformer and Param\nimport org.apache.spark.ml.classification.LogisticRegression import org.apache.spark.ml.linalg.{Vector, Vectors} import org.apache.spark.ml.param.ParamMap import org.apache.spark.sql.Row // Prepare training data from a list of (label, features) tuples. val training = spark.createDataFrame(Seq( (1.0, Vectors.dense(0.0, 1.1, 0.1)), (0.0, Vectors.dense(2.0, 1.0, -1.0)), (0.0, Vectors.dense(2.0, 1.3, 1.0)), (1.0, Vectors.dense(0.0, 1.2, -0.5)) )).toDF(\u0026quot;label\u0026quot;, \u0026quot;features\u0026quot;) // Create a LogisticRegression instance. This instance is an Estimator. val lr = new LogisticRegression() // Print out the parameters, documentation, and any default values. println(s\u0026quot;LogisticRegression parameters:\\n ${lr.explainParams()}\\n\u0026quot;) // We may set parameters using setter methods. lr.setMaxIter(10) .setRegParam(0.01) // Learn a LogisticRegression model. This uses the parameters stored in lr. val model1 = lr.fit(training) // Since model1 is a Model (i.e., a Transformer produced by an Estimator), // we can view the parameters it used during fit(). // This prints the parameter (name: value) pairs, where names are unique IDs for this // LogisticRegression instance. println(s\u0026quot;Model 1 was fit using parameters: ${model1.parent.extractParamMap}\u0026quot;) // We may alternatively specify parameters using a ParamMap, // which supports several methods for specifying parameters. val paramMap = ParamMap(lr.maxIter -\u0026gt; 20) .put(lr.maxIter, 30) // Specify 1 Param. This overwrites the original maxIter. .put(lr.regParam -\u0026gt; 0.1, lr.threshold -\u0026gt; 0.55) // Specify multiple Params. // One can also combine ParamMaps. val paramMap2 = ParamMap(lr.probabilityCol -\u0026gt; \u0026quot;myProbability\u0026quot;) // Change output column name. val paramMapCombined = paramMap ++ paramMap2 // Now learn a new model using the paramMapCombined parameters. // paramMapCombined overrides all parameters set earlier via lr.set* methods. val model2 = lr.fit(training, paramMapCombined) println(s\u0026quot;Model 2 was fit using parameters: ${model2.parent.extractParamMap}\u0026quot;) // Prepare test data. val test = spark.createDataFrame(Seq( (1.0, Vectors.dense(-1.0, 1.5, 1.3)), (0.0, Vectors.dense(3.0, 2.0, -0.1)), (1.0, Vectors.dense(0.0, 2.2, -1.5)) )).toDF(\u0026quot;label\u0026quot;, \u0026quot;features\u0026quot;) // Make predictions on test data using the Transformer.transform() method. // LogisticRegression.transform will only use the 'features' column. // Note that model2.transform() outputs a 'myProbability' column instead of the usual // 'probability' column since we renamed the lr.probabilityCol parameter previously. model2.transform(test) .select(\u0026quot;features\u0026quot;, \u0026quot;label\u0026quot;, \u0026quot;myProbability\u0026quot;, \u0026quot;prediction\u0026quot;) .collect() .foreach { case Row(features: Vector, label: Double, prob: Vector, prediction: Double) =\u0026gt; println(s\u0026quot;($features, $label) -\u0026gt; prob=$prob, prediction=$prediction\u0026quot;) }    Pipeline\nimport org.apache.spark.ml.{Pipeline, PipelineModel} import org.apache.spark.ml.classification.LogisticRegression import org.apache.spark.ml.feature.{HashingTF, Tokenizer} import org.apache.spark.ml.linalg.Vector import org.apache.spark.sql.Row // Prepare training documents from a list of (id, text, label) tuples. val training = spark.createDataFrame(Seq( (0L, \u0026quot;a b c d e spark\u0026quot;, 1.0), (1L, \u0026quot;b d\u0026quot;, 0.0), (2L, \u0026quot;spark f g h\u0026quot;, 1.0), (3L, \u0026quot;hadoop mapreduce\u0026quot;, 0.0) )).toDF(\u0026quot;id\u0026quot;, \u0026quot;text\u0026quot;, \u0026quot;label\u0026quot;) // Configure an ML pipeline, which consists of three stages: tokenizer, hashingTF, and lr. val tokenizer = new Tokenizer() .setInputCol(\u0026quot;text\u0026quot;) .setOutputCol(\u0026quot;words\u0026quot;) val hashingTF = new HashingTF() .setNumFeatures(1000) .setInputCol(tokenizer.getOutputCol) .setOutputCol(\u0026quot;features\u0026quot;) val lr = new LogisticRegression() .setMaxIter(10) .setRegParam(0.001) val pipeline = new Pipeline() .setStages(Array(tokenizer, hashingTF, lr)) // Fit the pipeline to training documents. val model = pipeline.fit(training) // Now we can optionally save the fitted pipeline to disk model.write.overwrite().save(\u0026quot;/tmp/spark-logistic-regression-model\u0026quot;) // We can also save this unfit pipeline to disk pipeline.write.overwrite().save(\u0026quot;/tmp/unfit-lr-model\u0026quot;) // And load it back in during production val sameModel = PipelineModel.load(\u0026quot;/tmp/spark-logistic-regression-model\u0026quot;) // Prepare test documents, which are unlabeled (id, text) tuples. val test = spark.createDataFrame(Seq( (4L, \u0026quot;spark i j k\u0026quot;), (5L, \u0026quot;l m n\u0026quot;), (6L, \u0026quot;spark hadoop spark\u0026quot;), (7L, \u0026quot;apache hadoop\u0026quot;) )).toDF(\u0026quot;id\u0026quot;, \u0026quot;text\u0026quot;) // Make predictions on test documents. model.transform(test) .select(\u0026quot;id\u0026quot;, \u0026quot;text\u0026quot;, \u0026quot;probability\u0026quot;, \u0026quot;prediction\u0026quot;) .collect() .foreach { case Row(id: Long, text: String, prob: Vector, prediction: Double) =\u0026gt; println(s\u0026quot;($id, $text) --\u0026gt; prob=$prob, prediction=$prediction\u0026quot;) }    Extracting, transforming and Selecting This section is organized into three different subsections:\n Feature Extractors. This section introduces some algorithm for extracting features from your dataset Feature Transformers. This section introduces some of the algorithm that execute commonly used transformations the input dataset  Feature Extractors   Word2Vec\nWord2Vec is an Estimator which takes as input a sequence of words representing a document and then it trains a Word2VecModel over these words. The model maps each word to a unique fixed-size vector. A document is then represented by simply taking the average of all the vectors associated with any word contained within the original text. Once you have a vector, you can apply all sorts of ML algorithms.\nThe following snippet shows how to use this model.\nimport org.apache.spark.ml.feature.Word2Vec import org.apache.spark.ml.linalg.Vector import org.apache.spark.sql.Row // Input data: Each row is a bag of words from a sentence or document. val documentDF = spark.createDataFrame(Seq( \u0026quot;Hi I heard about Spark\u0026quot;.split(\u0026quot; \u0026quot;), \u0026quot;I wish Java could use case classes\u0026quot;.split(\u0026quot; \u0026quot;), \u0026quot;Logistic regression models are neat\u0026quot;.split(\u0026quot; \u0026quot;) ).map(Tuple1.apply)).toDF(\u0026quot;text\u0026quot;) // Learn a mapping from words to Vectors. val word2Vec = new Word2Vec() .setInputCol(\u0026quot;text\u0026quot;) .setOutputCol(\u0026quot;result\u0026quot;) .setVectorSize(3) .setMinCount(0) val model = word2Vec.fit(documentDF) val result = model.transform(documentDF) result.collect().foreach { case Row(text: Seq[_], features: Vector) =\u0026gt; println(s\u0026quot;Text: [${text.mkString(\u0026quot;, \u0026quot;)}] =\u0026gt; \\nVector: $features\\n\u0026quot;) }    Feature Transformers   Binarizer\nBinarization is the process of thresholding numerical features to binary (0/1) features.\nBinarizer takes the common parameters inputCol and outputCol, as well as the threshold for binarization. Feature values greater than the threshold are binarized to 1.0; values equal to or less than the threshold are binarized to 0.0. Both Vector and Double types are supported for inputCol.\nimport org.apache.spark.ml.feature.Binarizer val data = Array((0, 0.1), (1, 0.8), (2, 0.2)) val dataFrame = spark.createDataFrame(data).toDF(\u0026quot;id\u0026quot;, \u0026quot;feature\u0026quot;) val binarizer: Binarizer = new Binarizer() .setInputCol(\u0026quot;feature\u0026quot;) .setOutputCol(\u0026quot;binarized_feature\u0026quot;) .setThreshold(0.5) val binarizedDataFrame = binarizer.transform(dataFrame) println(s\u0026quot;Binarizer output with Threshold = ${binarizer.getThreshold}\u0026quot;) binarizedDataFrame.show()    PCA\nPCA is a statistical procedure that uses an orthogonal transformation to convert a set of observations of possibly correlated variables into a set of values of linearly uncorrelated variables called principal components. A PCA class trains a model to project vectors to a low-dimensional space using PCA. The example below shows how to project 5-dimensional feature vectors into 3-dimensional principal components.\nimport org.apache.spark.ml.feature.PCA import org.apache.spark.ml.linalg.Vectors val data = Array( Vectors.sparse(5, Seq((1, 1.0), (3, 7.0))), Vectors.dense(2.0, 0.0, 3.0, 4.0, 5.0), Vectors.dense(4.0, 0.0, 0.0, 6.0, 7.0) ) val df = spark.createDataFrame(data.map(Tuple1.apply)).toDF(\u0026quot;features\u0026quot;) val pca = new PCA() .setInputCol(\u0026quot;features\u0026quot;) .setOutputCol(\u0026quot;pcaFeatures\u0026quot;) .setK(3) .fit(df) val result = pca.transform(df).select(\u0026quot;pcaFeatures\u0026quot;) result.show(false)    OneHotEncoder\nOne-hot-encoding maps a categorical feature to a binary vector with at most a single one-value. For instance, imagine you have a categorical feature that allows 5 possible categorical values: A,B,C,D,E. This feature is then represented as a 5-sized vector where each value is mapped to a particular position. Therefore the value A - becomes -\u0026gt; [1,0,0,0,0], the value B - becomes -\u0026gt; [0,1,0,0,0] ans so on.\nThe following snippet shows how to use this transformer:\nimport org.apache.spark.ml.feature.OneHotEncoder val df = spark.createDataFrame(Seq( (0.0, 1.0), (1.0, 0.0), (2.0, 1.0), (0.0, 2.0), (0.0, 1.0), (2.0, 0.0) )).toDF(\u0026quot;categoryIndex1\u0026quot;, \u0026quot;categoryIndex2\u0026quot;) val encoder = new OneHotEncoder() .setInputCols(Array(\u0026quot;categoryIndex1\u0026quot;, \u0026quot;categoryIndex2\u0026quot;)) .setOutputCols(Array(\u0026quot;categoryVec1\u0026quot;, \u0026quot;categoryVec2\u0026quot;)) val model = encoder.fit(df) val encoded = model.transform(df) encoded.show()    StandardScaler Many ML algorithm are very sensitive to the scale of the input dataset. These algorithms work best when all the feature have the same scale. For this reason it is often required to normalize your data. This scaler normalize the data so that each numerical feature has unit standard deviation and zero mean.\nThe StandardScaler is actually an Estimator, thus it has the method fit which returns a StandardScalerModel object, which is a Transformer. Therefore, by calling transform on the StandardScaler model you can scale the input features as desired.\nThe following snippet shows how to use this scaler.\nimport org.apache.spark.ml.feature.StandardScaler val dataFrame = spark.read.format(\u0026quot;libsvm\u0026quot;).load(\u0026quot;data/mllib/sample_libsvm_data.txt\u0026quot;) val scaler = new StandardScaler() .setInputCol(\u0026quot;features\u0026quot;) .setOutputCol(\u0026quot;scaledFeatures\u0026quot;) .setWithStd(true) .setWithMean(false) // Compute summary statistics by fitting the StandardScaler. val scalerModel = scaler.fit(dataFrame) // Normalize each feature to have unit standard deviation. val scaledData = scalerModel.transform(dataFrame) scaledData.show()  This is not the only option when it comes to scaling your data. There are other techniques that work the same way as this scaler, the only thing that change of course is the algorithm used for scale the data.\nBucketizer A Bucketizer transforms a real-valued feature into a column where values are divided into buckets.\nWhen you use this type of transformer you need to specify the buckets into which you wish to divided your feature. Therefore you must specify an ordered vector containing the ranges according to which buckets have to been defined.\nThe following snippet shows how to use this transformer.\nimport org.apache.spark.ml.feature.Bucketizer val splits = Array(Double.NegativeInfinity, -0.5, 0.0, 0.5, Double.PositiveInfinity) val data = Array(-999.9, -0.5, -0.3, 0.0, 0.2, 999.9) val dataFrame = spark.createDataFrame(data.map(Tuple1.apply)).toDF(\u0026quot;features\u0026quot;) val bucketizer = new Bucketizer() .setInputCol(\u0026quot;features\u0026quot;) .setOutputCol(\u0026quot;bucketedFeatures\u0026quot;) .setSplits(splits) // Transform original data into its bucket index. val bucketedData = bucketizer.transform(dataFrame) println(s\u0026quot;Bucketizer output with ${bucketizer.getSplits.length-1} buckets\u0026quot;) bucketedData.show() val splitsArray = Array( Array(Double.NegativeInfinity, -0.5, 0.0, 0.5, Double.PositiveInfinity), Array(Double.NegativeInfinity, -0.3, 0.0, 0.3, Double.PositiveInfinity)) val data2 = Array( (-999.9, -999.9), (-0.5, -0.2), (-0.3, -0.1), (0.0, 0.0), (0.2, 0.4), (999.9, 999.9)) val dataFrame2 = spark.createDataFrame(data2).toDF(\u0026quot;features1\u0026quot;, \u0026quot;features2\u0026quot;) val bucketizer2 = new Bucketizer() .setInputCols(Array(\u0026quot;features1\u0026quot;, \u0026quot;features2\u0026quot;)) .setOutputCols(Array(\u0026quot;bucketedFeatures1\u0026quot;, \u0026quot;bucketedFeatures2\u0026quot;)) .setSplitsArray(splitsArray) // Transform original data into its bucket index. val bucketedData2 = bucketizer2.transform(dataFrame2) println(s\u0026quot;Bucketizer output with [\u0026quot; + s\u0026quot;${bucketizer2.getSplitsArray(0).length-1}, \u0026quot; + s\u0026quot;${bucketizer2.getSplitsArray(1).length-1}] buckets for each input column\u0026quot;) bucketedData2.show()  Imputer The Imputer is an estimator that is useful for replacing missing/null/NaN values accordingly to a predefined strategy. For instance, you might use an Imputer to replace the missing values in a feature of the dataset by replacing them with the average value computed with respect to that feature. An Imputer can only work with numerical features – it is not able to understand categorical values. It also provides the ability to configure which value has to be considered as \u0026ldquo;missing\u0026rdquo;. For instance, if we set .setMissingValue(0) then all the occurrences of 0 will be replaced by the Imputer.\nThe following snippet shows how to use the Imputer.\nimport org.apache.spark.ml.feature.Imputer val df = spark.createDataFrame(Seq( (1.0, Double.NaN), (2.0, Double.NaN), (Double.NaN, 3.0), (4.0, 4.0), (5.0, 5.0) )).toDF(\u0026quot;a\u0026quot;, \u0026quot;b\u0026quot;) val imputer = new Imputer() .setInputCols(Array(\u0026quot;a\u0026quot;, \u0026quot;b\u0026quot;)) .setOutputCols(Array(\u0026quot;out_a\u0026quot;, \u0026quot;out_b\u0026quot;)) val model = imputer.fit(df) model.transform(df).show()  Classification \u0026amp; Regression These are two different types of supervised learning problem.\nIn any supervised learning problem you are provided with a set of items, each item has a set of features, and more importantly it is assigned with a label.\nThe type of the label tells you if you are dealing with a regression problem rather than a classification one. More specifically, if the label is a real value then you have a regression problem, while if it is a categorical value then you are dealing with a classification problem.\nMLlib offers a lot of algorithms for addressing these kinds of supervised problems. This is the list of algorithms provided by the library:\n Classification  Logistic regression  Binomial logistic regression Multinomial logistic regression   Decision tree classifier Random forest classifier Gradient-boosted tree classifier Multilayer perceptron classifier Linear Support Vector Machine One-vs-Rest classifier (a.k.a. One-vs-All) Naive Bayes Factorization machines classifier   Regression  Linear regression Generalized linear regression  Available families   Decision tree regression Random forest regression Gradient-boosted tree regression Survival regression Isotonic regression Factorization machines regressor   Linear methods Factorization Machines Decision trees  Inputs and Outputs  Input Columns Output Columns     Tree Ensembles  Random Forests  Inputs and Outputs  Input Columns Output Columns (Predictions)     Gradient-Boosted Trees (GBTs)  Inputs and Outputs  Input Columns Output Columns (Predictions)        We will not discuss any of the previous algorithms. Since this is beyond the scope of this lesson. A useful resource to learn (quickly) is the scikit-learn documentation, available here.\nModel Selection and Tuning This section describes how to use MLlib’s tooling for tuning ML algorithms and Pipelines. Built-in Cross-Validation and other tooling allow users to optimize hyperparameters in algorithms and Pipelines.\nParameter Tuning Parameter Tuning is the task of trying different configuration in order to improve performance.\nYou can tune a single estimator as well as an entire pipeline. When you want to improve the performance of your model you need an object of type Evaluator. An Evaluator allows you to evaluate how well your model is able to fit the data. There is a number of different evaluators, the right one depends on the type of problem you are dealing with. For instance if you are dealing with a regression problem you should use the RegressionEvaluator, while for classification problems you should use the BinaryClassificationEvaluator or the MulticlassificationEvaluator.\nA common way tune an ML model is by defining a grid, which determines the range of values a particular parameter can assume. This kind of behavior is achieved via the ParamGridBuilder object. A grid based tuning works as follows. Imagine your model M has depends on two different hyperparameters: p1 and p2. You define a range of values for each one of them. For instance you may specify that p1 varies within the range [0,10] with step 2, while p2 varies within the range [-10, 10] with step. This means that you would have 5 different possible configuration for p1 and 3 different configuration for p2. In this case your model will be trained 5x3=15 different times, assessing every possible configuration with respect to both parameters.\nCross Validation Another important technique for selecting the best model is the Cross-Validation. In order to train your model with the cross-validation there is an object called CrossValidator. A CrossValidator is actually an Estimator, so you can call fit and then you get a model. The following snippet shows how to use the a CrossValidator.\n... val tokenizer = new Tokenizer() .setInputCol(\u0026quot;text\u0026quot;) .setOutputCol(\u0026quot;words\u0026quot;) val hashingTF = new HashingTF() .setInputCol(tokenizer.getOutputCol) .setOutputCol(\u0026quot;features\u0026quot;) val lr = new LogisticRegression() .setMaxIter(10) val pipeline = new Pipeline() .setStages(Array(tokenizer, hashingTF, lr)) // We use a ParamGridBuilder to construct a grid of parameters to search over. // With 3 values for hashingTF.numFeatures and 2 values for lr.regParam, // this grid will have 3 x 2 = 6 parameter settings for CrossValidator to choose from. val paramGrid = new ParamGridBuilder() .addGrid(hashingTF.numFeatures, Array(10, 100, 1000)) .addGrid(lr.regParam, Array(0.1, 0.01)) .build() val cv = new CrossValidator() .setEstimator(pipeline) .setEvaluator(new BinaryClassificationEvaluator) .setEstimatorParamMaps(paramGrid) .setNumFolds(2) // Use 3+ in practice .setParallelism(2) // Evaluate up to 2 parameter settings in parallel val model = cv.fit(data)  Anatomy of a ML application Any ML application should adopt the following development steps:\n Think about the problem you are interested in. You should ask yourself the following questions:  How would I frame it? Is it a supervised problem? Is it better to address the problem as a classification problem rather then a regression one?   Get the data. In this stage you get the data required to solve the problem you are interested in. Analyze your data, compute summary statistics, plot the data, gain insights. Prepare your data to apply ML algorithms. In this stage you apply transformer to clean your data – for instance you can apply the Imputer to replace missing value –, or you can apply transformer to obtain a different representation of your data – for instance Word2Vec or OneHotEncoding. Split your dataset. You must always remember to reserve a fraction of your data to validate your model. It means that you must split the data (at least) into training and test set. The training set is used to train your model, while the test is used to understand how godd is your model in terms of generalization Select the most promising models and apply parameter tuning on them Present the results  ","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"b179d089a2f2cf729444272ec5fdcb8a","permalink":"/courses/bdanalytics/sparkmllib/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/bdanalytics/sparkmllib/","section":"courses","summary":"MLlib is the Spark is the primary library for addressing machine learning problem within the Spark ecosystem.\nMLlib provides the following features:\n A collection of ML Algorithms for solving a variety of ML problem, such as classification, regression, clustering and collaborative filtering A collection of techniques for featurization, i.","tags":null,"title":"Machine Learning with Spark","type":"docs"},{"authors":null,"categories":null,"content":"What you will learn:\n How to use Spark to compute aggregate statistics on a distributed dataset How to do collaborative filtering in spark How to represent a graph data structure \u0026ndash; without third party libraries \u0026ndash; and to execute a BFS algorithm  The material for this lesson is available here\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"7bdaf5d02cfab4735019b5049c3ffa19","permalink":"/courses/bdanalytics/advancedspark/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/bdanalytics/advancedspark/","section":"courses","summary":"What you will learn:\n How to use Spark to compute aggregate statistics on a distributed dataset How to do collaborative filtering in spark How to represent a graph data structure \u0026ndash; without third party libraries \u0026ndash; and to execute a BFS algorithm  The material for this lesson is available here","tags":null,"title":"Spark (More) Advanced Examples","type":"docs"},{"authors":null,"categories":null,"content":"This lesson provides a quick introduction to the Scala ecosystem.\nWhat you will learn:\n The key Spark components What is an RDD How to handle RDDs in Spark How to write some basic Spark driver programs with Scala How to dockerize your environment and your appllication  The material for this lesson is available here\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"6bf0e92266964f3834bad96c5e3971d0","permalink":"/courses/bdanalytics/sparkeco/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/bdanalytics/sparkeco/","section":"courses","summary":"This lesson provides a quick introduction to the Scala ecosystem.\nWhat you will learn:\n The key Spark components What is an RDD How to handle RDDs in Spark How to write some basic Spark driver programs with Scala How to dockerize your environment and your appllication  The material for this lesson is available here","tags":null,"title":"The Spark Ecosystem","type":"docs"},{"authors":null,"categories":null,"content":"GraphX is a new component in Spark for graphs and graph-parallel computation.\nAt a high level, GraphX extends the Spark RDD by introducing a new Graph abstraction: a directed multigraph with properties attached to each vertex and edge.\nTo support graph computation, GraphX exposes a set of fundamental operators (e.g., subgraph, joinVertices, and aggregateMessages) as well as an optimized variant of the Pregel API. In addition, GraphX includes a growing collection of graph algorithms and builders to simplify graph analytics tasks.\nIn order to use this library in your driver program you need to add the following imports:\nimport org.apache.spark._ import org.apache.spark.graphx._ import org.apache.spark.rdd.RDD  The code for this lesson is available here.\nThe Property Graph It is a directed multigraph with user defined objects attached to each vertex and edge.\nA directed multigraph is simply a directed graph that can have multiple edges connecting the same pair of vertices. This ability enable the possibility to represent multiple relationships between the nodes in the graph.\nEach vertex is associated with a unique 64-bit long key identifier, aka VertexId. Similarly edges have source and destination identifiers.\nThe property graph is parameterized over the vertex and edge types, denoted by VD and ED, respectively. VD and ED are therefore the types associated with objects stored in the graph.\nYou can also exploit inheritance to have specialized nodes within the graph as follows:\nclass VertexProperty() case class UserProperty(val name: String) extends VertexProperty case class ProductProperty(val name: String, val price: Double) extends VertexProperty // The graph might then have the type: var graph: Graph[VertexProperty, String] = null  In this case the Graph can store two different types of objects.\nGraphs inherit all the good things from the RDD.\n Graphs are immutable, therefore any change to the values contained in a graph instance produce a new instance – graphs cannot be changed in place! Graphs are distributed. The Graph is partitioned along the worker nodes using a range of heuristic methods. Graphs are fault tolerant. Therefore, in case of failure of a worker node, the partition can be easily recreated  The property Graph under the hood A property Graph is simply a compound type having two typed collections (RDDs).\nclass Graph[VD, ED] { val vertices: VertexRDD[VD] val edges: EdgesRDD[ED] }  VertexRDD and EdgesRDD are two specialized and optimized versions of RDD[(VertexId, VD)] and RDD[Edge[ED]], respectively. As opposed to a classic RDD, both VertexRDD and EdgesRDD provide specialized operations tailored for Graph computation.\nExample: How to use the Property Graph Suppose you want to construct the following graph.   Property Graph   It has the following signature:\nval userGraph: Graph[(String, String), String]  You can construct a Graph object in multiple ways. The following method is probably the most general and useful.\n// Assume the SparkContext has already been constructed val sc: SparkContext // Create an RDD for the vertices val users: RDD[(VertexId, (String, String))] = sc.parallelize(Seq((3L, (\u0026quot;rxin\u0026quot;, \u0026quot;student\u0026quot;)), (7L, (\u0026quot;jgonzal\u0026quot;, \u0026quot;postdoc\u0026quot;)), (5L, (\u0026quot;franklin\u0026quot;, \u0026quot;prof\u0026quot;)), (2L, (\u0026quot;istoica\u0026quot;, \u0026quot;prof\u0026quot;)))) // Create an RDD for edges val relationships: RDD[Edge[String]] = sc.parallelize(Seq(Edge(3L, 7L, \u0026quot;collab\u0026quot;), Edge(5L, 3L, \u0026quot;advisor\u0026quot;), Edge(2L, 5L, \u0026quot;colleague\u0026quot;), Edge(5L, 7L, \u0026quot;pi\u0026quot;))) // Define a default user in case there are relationship with missing user val defaultUser = (\u0026quot;John Doe\u0026quot;, \u0026quot;Missing\u0026quot;) // Build the initial Graph val graph = Graph(users, relationships, defaultUser)    Edge is a built in class for storing edges information. It is defined as:\nEdge[ED]( srcId: VertexId = 0, dstId: VertexId = 0, attr: ED = null.asInstanceOf[ED]) extends Serializable with Product\n  The Graph constructor takes as input two RDDs and a default user, which is needed to handle situations when a vertex is defined in the EdgeRDD but not in the VertexRDD.\n  Given a Graph instance, you can query both the RDD containing the vertices and the edges directly from that instance.\nval graph: Graph[(String, String), String] // Constructed from above // Count all users which are postdocs graph.vertices.filter { case (id, (name, pos)) =\u0026gt; pos == \u0026quot;postdoc\u0026quot; }.count // Count all the edges where src \u0026gt; dst graph.edges.filter(e =\u0026gt; e.srcId \u0026gt; e.dstId).count // or equivalently graph.edges.filter{case Edge(src, dst, prop) =\u0026gt; src \u0026gt; dst}.count  It should be noted that graph.vertices returns an VertexRDD[(String, String)] in this case, which it is interpreted as an RDD[(VertexID, (String, String)] – this is why we can use the case construct (take a look here). The same holds when you access the edges with graph.edges. This returns an instance of EdgeRDD which contains objects of type Edge[String].\nIn addition to these two mechanism there is a third one, which exposes a triplet view.\nA triplet view consists of a series of EdgeTriplet objects. An EdgeTriplet object merges the information about the two vertices – endpoints of the edges – and the relationship between them. Conceptually, you can think at the figure:\n  Edge Triplet   The EdgeTriplet class extends Edge by adding the attributes of both the source node and the target node.\nYou can use the triplet view as in the following example:\nval graph: Graph[(String, String), String] // Constructed from above // Use the triplets view to create an RDD of facts. val facts: RDD[String] = graph.triplets.map(triplet =\u0026gt; triplet.srcAttr._1 + \u0026quot; is the \u0026quot; + triplet.attr + \u0026quot; of \u0026quot; + triplet.dstAttr._1) facts.collect.foreach(println(_))  Graph Operations As any other RDD, a graph supports basic operations such as map, filter, reduce and so on. In addition to that, graphs support a number of transformations specifically meaningful when working with graph.\nThis kind of operations are defined as GraphOps and most of the time they are accessible as members of a Graph object.\nThe following list provide a summary of the major GraphOps directly accessible from a graph instance.\n/** Summary of the functionality in the property graph */ class Graph[VD, ED] { // Information about the Graph =================================================================== val numEdges: Long val numVertices: Long val inDegrees: VertexRDD[Int] val outDegrees: VertexRDD[Int] val degrees: VertexRDD[Int] // Views of the graph as collections ============================================================= val vertices: VertexRDD[VD] val edges: EdgeRDD[ED] val triplets: RDD[EdgeTriplet[VD, ED]] // Functions for caching graphs ================================================================== def persist(newLevel: StorageLevel = StorageLevel.MEMORY_ONLY): Graph[VD, ED] def cache(): Graph[VD, ED] def unpersistVertices(blocking: Boolean = false): Graph[VD, ED] // Change the partitioning heuristic ============================================================ def partitionBy(partitionStrategy: PartitionStrategy): Graph[VD, ED] // Transform vertex and edge attributes ========================================================== def mapVertices[VD2](map: (VertexId, VD) =\u0026gt; VD2): Graph[VD2, ED] def mapEdges[ED2](map: Edge[ED] =\u0026gt; ED2): Graph[VD, ED2] def mapEdges[ED2](map: (PartitionID, Iterator[Edge[ED]]) =\u0026gt; Iterator[ED2]): Graph[VD, ED2] def mapTriplets[ED2](map: EdgeTriplet[VD, ED] =\u0026gt; ED2): Graph[VD, ED2] def mapTriplets[ED2](map: (PartitionID, Iterator[EdgeTriplet[VD, ED]]) =\u0026gt; Iterator[ED2]) : Graph[VD, ED2] // Modify the graph structure ==================================================================== def reverse: Graph[VD, ED] def subgraph( epred: EdgeTriplet[VD,ED] =\u0026gt; Boolean = (x =\u0026gt; true), vpred: (VertexId, VD) =\u0026gt; Boolean = ((v, d) =\u0026gt; true)) : Graph[VD, ED] def mask[VD2, ED2](other: Graph[VD2, ED2]): Graph[VD, ED] def groupEdges(merge: (ED, ED) =\u0026gt; ED): Graph[VD, ED] // Join RDDs with the graph ====================================================================== def joinVertices[U](table: RDD[(VertexId, U)])(mapFunc: (VertexId, VD, U) =\u0026gt; VD): Graph[VD, ED] def outerJoinVertices[U, VD2](other: RDD[(VertexId, U)]) (mapFunc: (VertexId, VD, Option[U]) =\u0026gt; VD2) : Graph[VD2, ED] // Aggregate information about adjacent triplets ================================================= def collectNeighborIds(edgeDirection: EdgeDirection): VertexRDD[Array[VertexId]] def collectNeighbors(edgeDirection: EdgeDirection): VertexRDD[Array[(VertexId, VD)]] def aggregateMessages[Msg: ClassTag]( sendMsg: EdgeContext[VD, ED, Msg] =\u0026gt; Unit, mergeMsg: (Msg, Msg) =\u0026gt; Msg, tripletFields: TripletFields = TripletFields.All) : VertexRDD[A] // Iterative graph-parallel computation ========================================================== def pregel[A](initialMsg: A, maxIterations: Int, activeDirection: EdgeDirection)( vprog: (VertexId, VD, A) =\u0026gt; VD, sendMsg: EdgeTriplet[VD, ED] =\u0026gt; Iterator[(VertexId, A)], mergeMsg: (A, A) =\u0026gt; A) : Graph[VD, ED] // Basic graph algorithms ======================================================================== def pageRank(tol: Double, resetProb: Double = 0.15): Graph[Double, Double] def connectedComponents(): Graph[VertexId, ED] def triangleCount(): Graph[Int, ED] def stronglyConnectedComponents(numIter: Int): Graph[VertexId, ED] }  Operators can be classified into three different categories:\n  Property Operators\nThey are very similar to the map operator of a regular RDD.\nclass Graph[VD, ED] { def mapVertices[VD2](map: (VertexId, VD) =\u0026gt; VD2): Graph[VD2, ED] def mapEdges[ED2](map: Edge[ED] =\u0026gt; ED2): Graph[VD, ED2] def mapTriplets[ED2](map: EdgeTriplet[VD, ED] =\u0026gt; ED2): Graph[VD, ED2] }    Each of these operators yields a new graph with the vertex or edge properties modified by the user defined map function.\n  The structure of the graph is not affected by these operations\n  Note that in order to use GraphX optimization you must prefer the following snippet\nval newGraph = graph.mapVertices((id, attr) =\u0026gt; mapUdf(id, attr))  to the following one\n val newVertices = graph.vertices.map { case (id, attr) =\u0026gt; (id, mapUdf(id, attr)) } val newGraph = Graph(newVertices, graph.edges)  Although they accomplish the same task, only the first one preserves the structural properties an it exploits GraphX optimizations.\n  When to use these oprators\nUsually, you want to use these transformations to prepare your graph before the application of some algorithm. For instance, if you want to prepare the graph in order to compute the page rank.\n// Given a graph where the vertex property is the out degree val inputGraph: Graph[Int, String] = graph.outerJoinVertices(graph.outDegrees)((vid, _, degOpt) =\u0026gt; degOpt.getOrElse(0)) // Construct a graph where each edge contains the weight // and each vertex is the initial PageRank val outputGraph: Graph[Double, Double] = inputGraph.mapTriplets(triplet =\u0026gt; 1.0 / triplet.srcAttr).mapVertices((id, _) =\u0026gt; 1.0)      Structural Operators\nCurrently GraphX supports only a simple set of commonly used structural operators.\nclass Graph[VD, ED] { def reverse: Graph[VD, ED] def subgraph(epred: EdgeTriplet[VD,ED] =\u0026gt; Boolean, vpred: (VertexId, VD) =\u0026gt; Boolean): Graph[VD, ED] def mask[VD2, ED2](other: Graph[VD2, ED2]): Graph[VD, ED] def groupEdges(merge: (ED, ED) =\u0026gt; ED): Graph[VD,ED] }    reverse returns a new Graph where the direction of every edge is reversed\n  subgraph returns the subgraph induced by the conditions provided as input – it is like applying a filter. This kind of operator is very useful when you need to restrict the graph to a group of vertices of interest, or when you want to ignore a set of broken links.\n// Create an RDD for the vertices val users: RDD[(VertexId, (String, String))] = sc.parallelize(Seq((3L, (\u0026quot;rxin\u0026quot;, \u0026quot;student\u0026quot;)), (7L, (\u0026quot;jgonzal\u0026quot;, \u0026quot;postdoc\u0026quot;)), (5L, (\u0026quot;franklin\u0026quot;, \u0026quot;prof\u0026quot;)), (2L, (\u0026quot;istoica\u0026quot;, \u0026quot;prof\u0026quot;)), (4L, (\u0026quot;peter\u0026quot;, \u0026quot;student\u0026quot;)))) // Create an RDD for edges val relationships: RDD[Edge[String]] = sc.parallelize(Seq(Edge(3L, 7L, \u0026quot;collab\u0026quot;), Edge(5L, 3L, \u0026quot;advisor\u0026quot;), Edge(2L, 5L, \u0026quot;colleague\u0026quot;), Edge(5L, 7L, \u0026quot;pi\u0026quot;), Edge(4L, 0L, \u0026quot;student\u0026quot;), Edge(5L, 0L, \u0026quot;colleague\u0026quot;))) // Define a default user in case there are relationship with missing user val defaultUser = (\u0026quot;John Doe\u0026quot;, \u0026quot;Missing\u0026quot;) // Build the initial Graph val graph = Graph(users, relationships, defaultUser) // Notice that there is a user 0 (for which we have no information) connected to users // 4 (peter) and 5 (franklin). graph.triplets.map( triplet =\u0026gt; triplet.srcAttr._1 + \u0026quot; is the \u0026quot; + triplet.attr + \u0026quot; of \u0026quot; + triplet.dstAttr._1 ).collect.foreach(println(_)) // Remove missing vertices as well as the edges to connected to them val validGraph = graph.subgraph(vpred = (id, attr) =\u0026gt; attr._2 != \u0026quot;Missing\u0026quot;) // The valid subgraph will disconnect users 4 and 5 by removing user 0 validGraph.vertices.collect.foreach(println(_)) validGraph.triplets.map( triplet =\u0026gt; triplet.srcAttr._1 + \u0026quot; is the \u0026quot; + triplet.attr + \u0026quot; of \u0026quot; + triplet.dstAttr._1 ).collect.foreach(println(_))    It should be noted that you are not required to provide two different predicates. The one that you do not provided is defaulted to a predicate that returns always true\n  mask returns a new graph containing only the vertices contained in the input graph.\n // Run Connected Components val ccGraph = graph.connectedComponents() // No longer contains missing field // Remove missing vertices as well as the edges to connected to them val validGraph = graph.subgraph(vpred = (id, attr) =\u0026gt; attr._2 != \u0026quot;Missing\u0026quot;) // Restrict the answer to the valid subgraph val validCCGraph = ccGraph.mask(validGrap    groupEdges merges parallel edges in the multigraph.\n    Join Operators\nIn many cases it is necessary to join data from external collections (RDDs) with graphs. For example, we might have extra user properties that we want to merge with an existing graph or we might want to pull vertex properties from one graph into another. These tasks can be accomplished using the join operators. Below we list the key join operators:\nclass Graph[VD, ED] { def joinVertices[U](table: RDD[(VertexId, U)])(map: (VertexId, VD, U) =\u0026gt; VD) : Graph[VD, ED] def outerJoinVertices[U, VD2](table: RDD[(VertexId, U)])(map: (VertexId, VD, Option[U]) =\u0026gt; VD2) : Graph[VD2, ED] }    The joinVertices returns a new Graph where the vertices are obtaining by merging the original ones with ones of the input RDD. Then, the user defined map function is applied upon the joined set of vertices.\nval nonUniqueCosts: RDD[(VertexId, Double)] val uniqueCosts: VertexRDD[Double] = graph.vertices.aggregateUsingIndex(nonUnique, (a,b) =\u0026gt; a + b) val joinedGraph = graph.joinVertices(uniqueCosts)( (id, oldCost, extraCost) =\u0026gt; oldCost + extraCost)    It should be noted that if an RDD contains more that one value for a given vertex index, only the first value is involved in the join operation.\nAlso, nodes in the original graph that are not involved in the join process keep their original value.\n  The outerJoinVertices is similar to the previous operation, the only exception is that the user defined map function is applied to every node both in the graph and in the input RDD and it can also change the vertex property type. For instance, we can set the up a graph for PageRank by initializing the vertex properties with the out degree of each node.\nval outDegrees: VertexRDD[Int] = graph.outDegrees val degreeGraph = graph.outerJoinVertices(outDegrees) { (id, oldAttr, outDegOpt) =\u0026gt; outDegOpt match { //this ampping function change the type of the property case Some(outDeg) =\u0026gt; outDeg case None =\u0026gt; 0 // No outDegree means zero outDegree } }    Note that, since it is not required that every node in the original graph has a counterpart in the input RDD, the map function returns an Option type.\n  Graph Specific Operations A key step in many graph analytics tasks is aggregating information about the neighborhood of each vertex.\nFor example, we might want to know the number of followers each user has or the average age of the followers of each user.\nMany iterative graph algorithms (e.g., PageRank, Shortest Path, and connected components) repeatedly aggregate properties of neighboring vertices (e.g., current PageRank Value, shortest path to the source, and smallest reachable vertex id).\nThe core of this aggregation mechanism in GraphX is represented by the aggregateMessages operation. This operator applies a user defined sendMsg function to each edge triplet in the graph and then uses the mergeMsg function to aggregate those messages as their destination vertex.\nclass Graph[VD, ED] { def aggregateMessages[Msg: ClassTag]( sendMsg: EdgeContext[VD, ED, Msg] =\u0026gt; Unit, mergeMsg: (Msg, Msg) =\u0026gt; Msg, tripletFields: TripletFields = TripletFields.All) : VertexRDD[Msg] }    sendMsg takes an EdgeContext, which exposes the source and destination attributes along with the edge attribute and function (sendToSrc and sendToDst) to send messages to the source and destination attributes.\n  mergeMsg takes two messages destined to the same vertex and returns a single message.\n  tripletFields it is an optional argument that indicates what data is accessed in the EdgeContext (i.e., the source vertex attribute). The default option is:\n TripletFields.All, which indicates that the user defined sendMsg function may access any fields in the EdgeContext  TripletFields are a convenient way to specify which part of the EdgeContext is involved in the transformation, allowing GraphX to apply some optimization.\n  You can think at a these two function send/mergeMsg as a map/reduce transformation. The aggregateMessages returns a VertexRDD[Msg] containing the aggregate messages (of type Msg) destined to each vertex. All the vertices that did not receive any message are not included in the final result.\n  Example\nIn the following example we use the aggregateMessages operator to compute the average age of the more senior followers of each user.\nimport org.apache.spark.graphx.{Graph, VertexRDD} import org.apache.spark.graphx.util.GraphGenerators // Create a graph with \u0026quot;age\u0026quot; as the vertex property. // Here we use a random graph for simplicity. val graph: Graph[Double, Int] = GraphGenerators.logNormalGraph(sc, numVertices = 100).mapVertices( (id, _) =\u0026gt; id.toDouble ) // Compute the number of older followers and their total age val olderFollowers: VertexRDD[(Int, Double)] = graph.aggregateMessages[(Int, Double)]( triplet =\u0026gt; { // Map Function if (triplet.srcAttr \u0026gt; triplet.dstAttr) { // Send message to destination vertex containing counter and age triplet.sendToDst((1, triplet.srcAttr)) } }, // Add counter and age (a, b) =\u0026gt; (a._1 + b._1, a._2 + b._2) // Reduce Function ) // Divide total age by number of older followers to get average age of older followers val avgAgeOfOlderFollowers: VertexRDD[Double] = olderFollowers.mapValues( (id, value) =\u0026gt; value match { case (count, totalAge) =\u0026gt; totalAge / count } ) // Display the results avgAgeOfOlderFollowers.collect.foreach(println(_))  Note AggregateMessages works best when the messages are constant sized – so no list, no concatenation.\n  Degree Information The following example shows how to compute the maximum degree of any vertex in the graph.\n/ Define a reduce operation to compute the highest degree vertex def max(a: (VertexId, Int), b: (VertexId, Int)): (VertexId, Int) = { if (a._2 \u0026gt; b._2) a else b } // Compute the max degrees val maxInDegree: (VertexId, Int) = graph.inDegrees.reduce(max) val maxOutDegree: (VertexId, Int) = graph.outDegrees.reduce(max) val maxDegrees: (VertexId, Int) = graph.degrees.reduce(max)  Caching and Uncaching As any other RDD,if you need to use a graph multiple time you should cache it first – call graph.cache().\nUnlike regular RDDs, graphs are often involved in iterative computations, therefore it would be great to have a mechanism to uncaching the intermediate graphs created within an algorithm iteration. Even though these intermediate results are eventually evicted by the system, it still a waste of memory, thus performance.\nThere is no trivial way to uncache a Graph, or in general an RDD, for this reason, if you need to design an iterative algorithm over a graph, it is better to use the Pregel API\nPregel API What is Pregel Pregel is a data flow paradigm and system for large-scale graph processing created at Google, to solve problems that were not easily solvable with an approach based on map-reduce.\nIf the system remains proprietary at Google, the computational paradigm was adopted by many graph-processing systems, including GraphX. In order to adopt the Pregel paradigm, most algorithms need to be redesigned to embrace this approach. Pregel is essentially a message-passing interface constrained to the edges of a graph.\nTo re-design an algorithm in a Pregel fashion, ones should \u0026ldquo;Think like a vertex\u0026rdquo;. Also, the state of a node is defined by the state of its neighborhood.\n  Pregel Paradigm   The above figure shows the Pregel data flow model. A Pregel computation takes as input:\n a graph a set of vertex states  At each iteration, referred to as a superstep, each vertex can:\n send a message to its neighbors, process the messages received in a previous superstep and update its state, accordingly.  Thus, each superstep consists of a round of messages being passed between neighbors and an update of the global vertex state.\nA few examples of Pregel implementations of graph algorithmswill help clarify how the paradigm works.\n  Example\nImagine you need to compute the maximum value among all the nodes in the network. The following sequence of figures show how we can accomplish this task with the Pregel paradigm. The following figure represents the input graph.   Initial Graph   In each step a vertex reads the messages received bu its incoming neighbors and set its state to the maximum value between its own vale and all the received messages.   Step 1   If during an iteration a node does not update its state, then it becomes halted. This means that it will not send any message in the following iteration.   Step 2   The algorithm proceeds until every node becomes halted.   Step 3   In general, sever optimization can be applied to the above example. For instance one may use combiners.\nA combiner is a user provided function that can combine multiple messages intended for the same vertex. This mechanism leads to a reduction in the number of messages transmitted between the vertices. In the above example a combiner could collapse multiple messages into a single one containing the maximum.\nAnother useful mechanism is offered by the aggregators. An aggregator enables global information exchange. Each vertex can provide a value to an aggregator during a superste S, the Pregel framework combines those values using a reduction operator, and the resulting value is made available to all the vertices in the subsequent superstep S+1. Another common way to use aggregators is to elect a node to play a distinguished role in an algorithm.\nThere is also a mechanism that allows the removal or the addition of a new vertex or edge. This is very useful for those algorithms that need to change the graph\u0026rsquo;s topology – for instance a clustering algorithm might collapse every node belonging to the same cluster into a single node.\n  Pregel in GraphX The Pregel implementation of GraphX has some key difference from the original definition.\n Messages are compute in parallel as a function of the edgetriplet The message computation is not only available to recipient of the message, but it is also available to the sender node Nodes can only send messages to their direct neighbors – no hops are allowed as in many other pregel implementations.  As in the original Pregel definition, a node that becomes inactive – because it has not any message to send or to process – is ignored during the superstep. Also as in the original Pregel, the algorithm terminates when there are no remaining messages.\nThe signature of the pregel function – which is a member function of Graph – is defined as follows:\ndef pregel[A] (initialMsg: A, maxIter: Int = Int.MaxValue, activeDir: EdgeDirection = EdgeDirection.Out) (vprog: (VertexId, VD, A) =\u0026gt; VD, sendMsg: EdgeTriplet[VD, ED] =\u0026gt; Iterator[(VertexId, A)], mergeMsg: (A, A) =\u0026gt; A) : Graph[VD, ED]  It takes two arguments lists:\n The first list contains configuration parameters including:  The initial message The maximum number of iterations The edge direction in which to send messages (by default messages are sent via outgoing links)   The second argument list contains:  The user defined function for receiving messages - vprog The user defined function for computing messages - sendMsg The user defined function for combining messages - mergeMsg    The previous example in GraphX can be solved as follows.\nimport org.apache.spark.graphx.{Graph, VertexId} import org.apache.spark.graphx.util.GraphGenerators val r = scala.util.Random // A graph with edge attributes containing distances val graph: Graph[Long, Double] = GraphGenerators.logNormalGraph(sc, numVertices = 100).mapEdges(e =\u0026gt; e.attr.toDouble) val sourceId: VertexId = 42 // The ultimate source // Initialize the graph such that all vertices except the root have distance infinity. val initialGraph = graph.mapVertices((id, _) =\u0026gt; r.nextInt) //pregel val sssp = initialGraph.pregel( Int.MinValue //inital messages )( (id, currentValue, receivedValue) =\u0026gt; math.max(currentValue, receivedValue), // Vertex Program triplet =\u0026gt; { // Send Message val sourceVertex = triplet.srcAttr //get the property associated with the src vertex if (sourceVertex._1 == sourceVertex._2_) //new value match the current one - the node is halted Iterator.empty // no messages else Iterator((triplet.dstId, sourceVertex._1)) //send out the message } }, (a, b) =\u0026gt; math.max(a, b) // Merge Message if multiple messages are received from the same vertex ) println(sssp.vertices.collect.mkString(\u0026quot;\\n\u0026quot;))  Note An edge triplet has five properties:\n srcId. The source vertex id srcAttr. The source vertex property stdId. The destination vertex id dstAttr. The destination vertex property attr. The edge property.   The material for this lesson is available here\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"5c8f0131e0480d8fb9a9404b7e32bba9","permalink":"/courses/bdanalytics/sparkgraphx/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/bdanalytics/sparkgraphx/","section":"courses","summary":"GraphX is a new component in Spark for graphs and graph-parallel computation.\nAt a high level, GraphX extends the Spark RDD by introducing a new Graph abstraction: a directed multigraph with properties attached to each vertex and edge.","tags":null,"title":"Working with Graphs","type":"docs"},{"authors":null,"categories":null,"content":"Learn how to deal with clustering problems with scikit-learn.\nThe notebook for this lesson is available here\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"4d2affa2fcde8b46db7a653788beb45f","permalink":"/courses/datamining/clustering/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/datamining/clustering/","section":"courses","summary":"Learn how to deal with clustering problems with scikit-learn.\nThe notebook for this lesson is available here","tags":null,"title":"Clustering","type":"docs"},{"authors":null,"categories":null,"content":"This lesson introduces you to a variety of Ensamble Learning techniques.\nThe notebook for this lesson is available here\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"2dc75ae5ab8dad7a8c6d3c5f78b2f544","permalink":"/courses/datamining/enslearn/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/datamining/enslearn/","section":"courses","summary":"This lesson introduces you to a variety of Ensamble Learning techniques.\nThe notebook for this lesson is available here","tags":null,"title":"Ensamble Learning","type":"docs"},{"authors":null,"categories":null,"content":"In this lesson we will discuss about the final project you are required to develop in order to pass the exam.\nThe notebook for this lesson is available here\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"4415f0dcfe404ec4e20a04c66b82fbb5","permalink":"/courses/datamining/finallesson/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/datamining/finallesson/","section":"courses","summary":"In this lesson we will discuss about the final project you are required to develop in order to pass the exam.\nThe notebook for this lesson is available here","tags":null,"title":"Final Lesson","type":"docs"},{"authors":null,"categories":null,"content":"This lesson introduces you to one of the building block Python as a scientific computing tool.\nThe notebook for this lesson is available here\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"cdf8457b325741b698bdde705b298b5c","permalink":"/courses/datamining/numpyintro/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/datamining/numpyintro/","section":"courses","summary":"This lesson introduces you to one of the building block Python as a scientific computing tool.\nThe notebook for this lesson is available here","tags":null,"title":"Introduction to NumPy","type":"docs"},{"authors":null,"categories":null,"content":"This lesson introduces you to Pandas. The main library for handling data in Python.\nThe notebook for this lesson is available here\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"f3b32515f053759a37275315270083b9","permalink":"/courses/datamining/pandasintro/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/datamining/pandasintro/","section":"courses","summary":"This lesson introduces you to Pandas. The main library for handling data in Python.\nThe notebook for this lesson is available here","tags":null,"title":"Data Handling \u0026 Visualization","type":"docs"},{"authors":null,"categories":null,"content":"In this lesson you will address a real machine learning project. This lesson provides also a primer into scikit-learn.\nThe notebook for this lesson is available here\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"1dc6eb16d6b9feb65c458e507404c5df","permalink":"/courses/datamining/introscikit/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/datamining/introscikit/","section":"courses","summary":"In this lesson you will address a real machine learning project. This lesson provides also a primer into scikit-learn.\nThe notebook for this lesson is available here","tags":null,"title":"Introduction to scikit-learn","type":"docs"},{"authors":null,"categories":null,"content":"In this lesson you will address a classifcation problem.\nThe notebook for this lesson is available here\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"6f47a4d9d31a1a55fd43deba2cb12f61","permalink":"/courses/datamining/classproblem/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/datamining/classproblem/","section":"courses","summary":"In this lesson you will address a classifcation problem.\nThe notebook for this lesson is available here","tags":null,"title":"Solving A Classification Problem","type":"docs"},{"authors":[],"categories":[],"content":"Overview This project is a web-based simulation environment for Friend-Foe-Dynamic-Linear-Threshold (F2DLT) Models presented for the first time at TrustCom 2018 and further discussed in Applied Network Science 2019 and at SEBD 2020\nIf you follow the above link you will be presented with a user-friendly interface, that allows you to create/upload a diffusion graph and to run a diffusion process.\nAs the diffusion unfolds, the platform will show you real-time statistics about the current diffusion phenomena.\n","date":1602661460,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1602661460,"objectID":"8f209e091569c8b8b268856515298894","permalink":"/project/f2dlt/","publishdate":"2020-10-14T09:44:20+02:00","relpermalink":"/project/f2dlt/","section":"project","summary":"Overview This project is a web-based simulation environment for Friend-Foe-Dynamic-Linear-Threshold (F2DLT) Models presented for the first time at TrustCom 2018 and further discussed in Applied Network Science 2019 and at SEBD 2020","tags":[],"title":"Complex Influence Propagation based on Trust-aware Dynamic Linear Threshold Models","type":"project"},{"authors":[],"categories":[],"content":"Overview This project is developed as part of the following research paper:\nA. Caliò, A. Tagarelli, F. Bonchi (2020). Cores matter? An analysis of graph decomposition effects on influence maximization problem. In Procs. of the 12th ACM Web Science Conference (WebSci-2020), July 6th - July 10th, 2020, Southampton, UK.\nIt contains a variety of graph-decomposition methods, specially adapted to be applied in an influence propagation problem \u0026ndash; i.e., upon directed and weighted graphs.\nPlease follow the above link for detailed instructions on how to install and use this project.\n","date":1602660627,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1602660627,"objectID":"98b3097bd506c834700943c82ef24076","permalink":"/project/cores/","publishdate":"2020-10-14T09:30:27+02:00","relpermalink":"/project/cores/","section":"project","summary":"Overview This project is developed as part of the following research paper:\nA. Caliò, A. Tagarelli, F. Bonchi (2020). Cores matter? An analysis of graph decomposition effects on influence maximization problem.","tags":[],"title":"(Di)Graph Decompositions","type":"project"},{"authors":[],"categories":[],"content":"Overview This project is developed as part of the following research paper:\nA. Caliò, A. Tagarelli.Attribute based Diversification of Seeds for Targeted Influence Maximization problem. Information Sciences, 2020.\nIt contains the implementation of every influence maximization algorithm discussed in above paper. Pleas follow the link for detailed instructions on how to install and use this project.\n","date":1602660627,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1602660627,"objectID":"c9e9a38ab687dcc7821bec68499f631c","permalink":"/project/aditum/","publishdate":"2020-10-14T09:30:27+02:00","relpermalink":"/project/aditum/","section":"project","summary":"Overview This project is developed as part of the following research paper:\nA. Caliò, A. Tagarelli.Attribute based Diversification of Seeds for Targeted Influence Maximization problem. Information Sciences, 2020.\nIt contains the implementation of every influence maximization algorithm discussed in above paper.","tags":[],"title":"ADITUM - Attribute DIversity-sensitive Targeted inflUence Maximization","type":"project"},{"authors":["Antonio Caliò","Andrea Tagarelli"],"categories":[],"content":"","date":1598918400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1601723265,"objectID":"f234b5d3f48c3d4e4cfb907e4dac52d5","permalink":"/publication/calio-attribute-2020/","publishdate":"2020-10-03T11:07:45.532809Z","relpermalink":"/publication/calio-attribute-2020/","section":"publication","summary":"Embedding diversity into knowledge discovery is important: the patterns mined will be more novel, more meaningful, and broader. Surprisingly, in the classic problem of influence maximization in social networks, relatively little study has been devoted to diversity and its integration into the objective function of an influence maximization method. In this work, we propose the integration of a categorical-based notion of seed diversity into the objective function of a targeted influence maximization problem. In this respect, we assume that the users of a social network are associated with a categorical dataset where each tuple expresses the profile of a user according to a predefined schema of categorical attributes. Upon this assumption, we design a class of monotone submodular functions specifically conceived for determining the diversity of the subset of categorical tuples associated with the seed users to be discovered. This allows us to develop an efficient approximate method, with a constant-factor guarantee of optimality. More precisely, we formulate the attribute-based diversity-sensitive targeted influence maximization problem under the state-of-the-art reverse influence sampling framework, and we develop a method, dubbed ADITUM, that ensures a (1-1/e-∊)-approximate solution under the general triggering diffusion model. Extensive experimental evaluation based on real-world networks as well as synthetically generated data has shown the meaningfulness and uniqueness of our proposed class of set diversity functions and of the ADITUM algorithm, also in comparison with methods that exploit numerical-attribute-based diversity and topology-driven diversity in influence maximization.","tags":["\"Diversity in influence maximization\"","\"Monotone submodular categorical set functions\"","\"Reverse influence sampling\"","\"Social recommendation\"","\"Viral marketing\""],"title":"Attribute based Diversification of Seeds for Targeted Influence Maximization","type":"publication"},{"authors":[],"categories":[],"content":"","date":1594684800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1601731749,"objectID":"ff96dc7b5145346cd1fcd8f583ad98bd","permalink":"/talk/websci/","publishdate":"2020-07-14T00:00:00Z","relpermalink":"/talk/websci/","section":"talk","summary":"The 12thACM Web Science Conference 2020 (WebSci'20)","tags":[],"title":" Cores matter? An analysis of graph decomposition effects on influence maximization","type":"talk"},{"authors":["Antonio Calio","Andrea Tagarelli"],"categories":[],"content":"","date":1593561600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1601723266,"objectID":"cd1b854acca8ff6f2f0fdaa3e61f4aa2","permalink":"/publication/calio-framework-2020/","publishdate":"2020-10-03T11:07:46.349304Z","relpermalink":"/publication/calio-framework-2020/","section":"publication","summary":"What are the key-features that enable an information diffusion model to explain the inherent complex dynamics of real-world propagation phenomena? To answer the above question, we discuss a novel class of stochastic Linear Threshold (LT) diﬀusion models, which are designed to capture the following aspects in inﬂuence propagation scenarios: trust/distrust in the user relationships, changes in adopting one or alternative information items, hesitation towards adopting an information item over time, latency in the propagation, time horizon for the unfolding of the diﬀusion process, and multiple cascades of information that might occur competitively. Around all such aspects, our deﬁned Friend-Foe Dynamic LT (F 2DLT ) class comprises a non-competitive model as well as two competitive models, which are able to represent semi-progressivity and non-progressivity, respectively, in the propagation process. The above key-constituents embedded in our models make them unique in the literature of diﬀusion models, including epidemic models. To validate our models through real-world networks, we also discuss diﬀerent strategies for the selection of the initial inﬂuencers to mimic non-competitive and competitive diﬀusion scenarios, inspired by the widely-known problem of limitation of misinformation spread. Finally, we describe a web-based simulation environment for testing the proposed diﬀusion models.","tags":[],"title":"A Framework for Complex Inﬂuence Propagation based on the F 2DLT Class of Diﬀusion Models","type":"publication"},{"authors":["Antonio Caliò","Andrea Tagarelli","Francesco Bonchi"],"categories":[],"content":"","date":1593561600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1601723265,"objectID":"11d414994b7bb03ee92f0c1cce85303f","permalink":"/publication/calio-cores-2020/","publishdate":"2020-10-03T11:07:45.669678Z","relpermalink":"/publication/calio-cores-2020/","section":"publication","summary":"Estimating the spreading potential of nodes in a social network is an important problem which finds application in a variety of different contexts, ranging from viral marketing to spread of viruses and rumor blocking. Several studies have exploited both mesoscale structures and local centrality measures in order to estimate the spreading potential of nodes. To this end, one known result in the literature establishes a correlation between the spreading potential of a node and its coreness: i.e., in a core-decompostion of a network, nodes in higher cores have a stronger influence potential on the rest of the network. In this paper we show that the above result does not hold in general under common settings of propagation models with submodular activation function on directed networks, as those ones used in the influence maximization (IM) problem. Motivated by this finding, we extensively explore where the set of influential nodes extracted by state-of-the-art IM methods are located in a network w.r.t. different notions of graph decomposition. Our analysis on real-world networks provides evidence that, regardless of the particular IM method, the best spreaders are not always located within the inner-most subgraphs defined according to commonly used graph-decomposition methods. We identify the main reasons that explain this behavior, which can be ascribed to the inability of classic decomposition methods in incorporating higher-order degree of nodes. By contrast, we find that a distance-based generalization of the core-decomposition for directed networks can profitably be exploited to actually restrict the location of candidate solutions for IM to a single, well-defined portion of a network graph.","tags":[],"title":"Cores matter? An analysis of graph decomposition effects on influence maximization problems","type":"publication"},{"authors":["Antonio Caliò","Andrea Tagarelli"],"categories":[],"content":"","date":1575158400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1601723266,"objectID":"9a19f4d2f1b4f3354a7a4418f328ce5b","permalink":"/publication/calio-complex-2019/","publishdate":"2020-10-03T11:07:45.854285Z","relpermalink":"/publication/calio-complex-2019/","section":"publication","summary":"To properly capture the complexity of influence propagation phenomena in real-world contexts, such as those related to viral marketing and misinformation spread, information diffusion models should fulfill a number of requirements. These include accounting for several dynamic aspects in the propagation (e.g., latency, time horizon), dealing with multiple cascades of information that might occur competitively, accounting for the contingencies that lead a user to change her/his adoption of one or alternative information items, and leveraging trust/distrust in the users’ relationships and its effect of influence on the users’ decisions. To the best of our knowledge, no diffusion model unifying all of the above requirements has been developed so far. In this work, we address such a challenge and propose a novel class of diffusion models, inspired by the classic linear threshold model, which are designed to deal with trust-aware, non-competitive as well as competitive time-varying propagation scenarios. Our theoretical inspection of the proposed models unveils important findings on the relations with existing linear threshold models for which properties are known about whether monotonicity and submodularity hold for the corresponding activation function. We also propose strategies for the selection of the initial spreaders of the propagation process, for both non-competitive and competitive influence propagation tasks, whose goal is to mimic contexts of misinformation spread. Our extensive experimental evaluation, which was conducted on publicly available networks and included comparison with competing methods, provides evidence on the meaningfulness and uniqueness of our models.","tags":[],"title":"Complex influence propagation based on trust-aware dynamic linear threshold models","type":"publication"},{"authors":["Antonio Caliò","Roberto Interdonato","Chiara Pulice","Andrea Tagarelli"],"categories":[],"content":"","date":1543622400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1601723266,"objectID":"825bf87dd76154103299829b59290c5a","permalink":"/publication/calio-topology-driven-2018/","publishdate":"2020-10-03T11:07:46.07962Z","relpermalink":"/publication/calio-topology-driven-2018/","section":"publication","summary":"Research on influence maximization ofter has to cope with marketing needs relating to the propagation of information towards specific users. However, little attention has been paid to the fact that the success of an information diffusion campaign might depend not only on the number of the initial influencers to be detected but also on their diversity w.r.t. the target of the campaign. Our main hypothesis is that if we learn seeds that are not only capable of influencing but also are linked to more diverse (groups of) users, then the influence triggers will be diversified as well, and hence the target users will get higher chance of being engaged. Upon this intuition, we define a novel problem, named Diversity-sensitive Targeted Influence Maximization (DTIM), which assumes to model user diversity by exploiting only topological information within a social graph. To the best of our knowledge, we are the first to bring the concept of topology-driven diversity into targeted IM problems, for which we define two alternative definitions. Accordingly, we propose approximate solutions of DTIM, which detect a size- $k$ set of users that maximizes the diversity-sensitive capital objective function, for a given selection of target users. We evaluate our DTIM methods on a special case of user engagement in online social networks, which concerns users who are not actively involved in the community life. Experimental evaluation on real networks has demonstrated the meaningfulness of our approach, also highlighting the opportunity of further development of solutions for DTIM applications.","tags":["\"Behavioral sciences\"","\"Cultural differences\"","\"Diversity-sensitive influence propagation\"","\"Linear programming\"","\"linear threshold diffusion model\"","\"lurking behavior analysis\"","\"social capital\"","\"Social network services\"","\"Task analysis\""],"title":"Topology-Driven Diversity for Targeted Influence Maximization with Application to User Engagement in Social Networks","type":"publication"},{"authors":[],"categories":[],"content":"","date":1533130140,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1601731740,"objectID":"4ffa978cb0d4e47799c1ffe8a73ec263","permalink":"/talk/trustcom/","publishdate":"2018-08-01T15:29:00+02:00","relpermalink":"/talk/trustcom/","section":"talk","summary":"The 17th IEEE Int. Conf. On Trust, Security And Privacy In Computing And Communications (TrustCom-18)","tags":[],"title":"Trust-based dynamic linear threshold models for non-competitive and competitive influence propagation","type":"talk"},{"authors":["Antonio Caliò","Andrea Tagarelli"],"categories":[],"content":"","date":1533081600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1601723266,"objectID":"78914e7ea63edcf22a77b96cc84d6832","permalink":"/publication/calio-trust-based-2018/","publishdate":"2020-10-03T11:07:46.21433Z","relpermalink":"/publication/calio-trust-based-2018/","section":"publication","summary":"What are the key-features that enable an information diffusion model to explain the inherent dynamic, and often competitive, nature of real-world propagation phenomena? In this paper we aim to answer this question by proposing a novel class of diffusion models, inspired by the classic Linear Threshold model, and built around the following aspects: trust/distrust in the user relationships, which is leveraged to model different effects of social influence on the decisions taken by an individual; changes in adopting one or alternative information items; hesitation towards adopting an information item over time; latency in the propagation; time horizon for the unfolding of the diffusion process; and multiple cascades of information that might occur competitively. To the best of our knowledge, the above aspects have never been unified into the same LT-based diffusion model. We also define different strategies for the selection of the initial influencers to simulate non-competitive and competitive diffusion scenarios, particularly related to the problem of limitation of misinformation spread. Results on publicly available networks have shown the meaningfulness and uniqueness of our models.","tags":["\"competitive diffusion scenarios\"","\"competitive influence propagation\"","\"Conferences\"","\"Data models\"","\"diffusion process\"","\"Diffusion processes\"","\"graph theory\"","\"influence propagation\"","\"information diffusion\"","\"information diffusion model\"","\"information item\"","\"Integrated circuit modeling\"","\"limitation of misinformation spread\"","\"LT based diffusion model\"","\"Optimization methods\"","\"real world propagation phenomena\"","\"Reliability\"","\"social influence\"","\"social networking (online)\"","\"time horizon\"","\"trust based dynamic linear threshold models\"","\"trust/distrust relationships\"","\"user relationships\""],"title":"Trust-Based Dynamic Linear Threshold Models for Non-competitive and Competitive Influence Propagation","type":"publication"},{"authors":[],"categories":[],"content":"","date":1528550976,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1601731776,"objectID":"8965175e0fad901c3e3157495d3dac4e","permalink":"/talk/google-jam/","publishdate":"2018-06-09T15:29:36+02:00","relpermalink":"/talk/google-jam/","section":"talk","summary":"GDG Cosenza: Machine Learning Study Jam @ University of Calabria","tags":[],"title":"Machine Learning Crash Course with TensorFlow APIs","type":"talk"}]